{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b2a482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os.path as osp\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f015d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_circular(h, w, max_numb=None, center=None, tolerance=1, min_val=0):\n",
    "    if center is None:  # use the middle of the image\n",
    "        center = (w / 2 - 0.5, h / 2 - 0.5)\n",
    "    if max_numb is None:\n",
    "        max_numb = max(h//2, w//2)\n",
    "\n",
    "    dist_from_center = np.zeros((h,w))\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            dist_from_center[i][j] = int(max(abs(i - center[0]), abs(j-center[1]))//tolerance + min_val)\n",
    "    return dist_from_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dbe2be5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4295/170061189.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "25f0e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [i for i in range(8*8)]\n",
    "data = np.array(data).reshape((8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "62c34f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randint(255, size=(32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d393e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "537ef042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fourier batches\n",
    "def split_tensor(imgs, split_coefs=None):\n",
    "    if split_coefs is None:\n",
    "        split_coefs = [1, 2, 4]\n",
    "    result = []\n",
    "\n",
    "    *_, h, w = imgs.shape\n",
    "    for coef in split_coefs:\n",
    "        step = h // coef\n",
    "        # add fft\n",
    "        for y in range(0, w, step):\n",
    "            for x in range(0, h, step):\n",
    "                value = torch.log(1 + torch.abs(torch.fft.fftshift(torch.fft.fft2(imgs[..., x:x + step, y:y + step]))))\n",
    "                value -= value.min()\n",
    "                value /= value.max()\n",
    "                result.append(value)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f9b233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_2d(array, border_size):\n",
    "    ptr_st, ptr_end = border_size, len(array) - border_size\n",
    "    data1 = list(array[:ptr_st])\n",
    "    data2 = list(array[ptr_end:])\n",
    "    itt_leng = 4\n",
    "    while ptr_st != ptr_end:\n",
    "        if itt_leng > 2:\n",
    "            data1.append(array[ptr_st])\n",
    "        else:\n",
    "            data2.append(array[ptr_st])\n",
    "        ptr_st += 1\n",
    "        itt_leng -= 1\n",
    "        if itt_leng == 0:\n",
    "            itt_leng = 4\n",
    "    return np.stack([data1, data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27cce2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76786206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, band_length, out_features:int, in_channels=1, hidden=16, kernel=2):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Conv2d(in_channels, hidden, kernel = kernel)\n",
    "        self.decoder = nn.Linear(hidden * (band_legnth - 1), out_features)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.decoder(nn.ReLU(self.encoder(X)))    \n",
    "                            \n",
    "class LinearAutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, features: int, hidden: int, out_features:int):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(features, hidden)\n",
    "        self.decoder = nn.Linear(hidden, features)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.decoder(nn.ReLU(self.encoder(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d136755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fourier batches \n",
    "def split_tensor(imgs, split_coefs=None):\n",
    "    if split_coefs is None:\n",
    "        split_coefs = [1, 2, 4]\n",
    "    result = []\n",
    "\n",
    "    *_, h, w = imgs.shape\n",
    "    last_val = 0\n",
    "    for coef in split_coefs:\n",
    "        step = h // coef\n",
    "        # add fft\n",
    "        row_data = []\n",
    "        for y in range(0, w, step):\n",
    "            step_result = []\n",
    "            for x in range(0, h, step):\n",
    "                #temperory unused\n",
    "                value = imgs[x:x + step, y:y + step]\n",
    "                mask = create_circular(step, step, min_val=last_val)\n",
    "                last_val = mask[-1][-1] + 1\n",
    "                                \n",
    "                \n",
    "                step_result.append(mask)\n",
    "            row_data.append(np.concatenate(step_result, axis=1))\n",
    "        result.append(np.concatenate(row_data, axis=0)) \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a69c93c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20291/681755919.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "result = split_tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67592203",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20291/1049141082.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "187135e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fecb0283e50>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, dpi=200)\n",
    "ax1.imshow(result[0])\n",
    "ax2.imshow(result[1])\n",
    "ax3.imshow(result[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a04dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_amount = int(result[2][-1][-1]+1)\n",
    "adj_matrix = np.zeros((vert_amount,vert_amount)).astype(int)\n",
    "gl_map, gen_map, loc_map = result[0].astype(int), result[1].astype(int), result[2].astype(int)\n",
    "for gl, gen, loc in np.nditer([gl_map, gen_map, loc_map]):\n",
    "    adj_matrix[gl, gen] = 1\n",
    "    adj_matrix[gl, loc] = 1\n",
    "    adj_matrix[gen, loc] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2ec150",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99a8dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcaae827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, band_length, out_features:int, in_channels=1, hidden=16, kernel=2):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Conv2d(in_channels, hidden, kernel_size = kernel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.decoder = nn.Linear(hidden * (band_length), out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.relu(x).flatten()\n",
    "        return self.decoder(x)\n",
    "                            \n",
    "class LinearAutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, features: int, hidden: int, out_features:int):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(features, hidden)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.decoder = nn.Linear(hidden, features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.relu(x)\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69a8fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create ModuleList\n",
    "def create_layers(img_shape, split_coefs=None, hidden=16, out=10, tp = 'linear'):\n",
    "    layers = nn.ModuleList()\n",
    "\n",
    "    if split_coefs is None:\n",
    "        split_coefs = [1, 2, 4]\n",
    "\n",
    "    for coef in split_coefs:\n",
    "        #Add simular pieces\n",
    "        for _ in range(coef**2):\n",
    "            input_layers = 4 if tp=='linear' else 1\n",
    "            for _ in range(img_shape//(2  * coef)):\n",
    "                #Linear\n",
    "                if tp == 'linear':\n",
    "                    layers.append(LinearAutoEncoder(input_layers, hidden, out))\n",
    "                    input_layers += 8\n",
    "                elif tp == 'conv':\n",
    "                    layers.append(ConvAutoEncoder(input_layers, out, hidden=hidden))\n",
    "                    input_layers += 4\n",
    "\n",
    "    return layers\n",
    "\n",
    "##Create ModuleList\n",
    "def create_border_leng(img_shape, split_coefs=None):\n",
    "    border_leng = []\n",
    "\n",
    "    if split_coefs is None:\n",
    "        split_coefs = [1, 2, 4]\n",
    "\n",
    "    for coef in split_coefs:\n",
    "        #Add simular pieces\n",
    "        for _ in range(coef**2):\n",
    "            inp = 2\n",
    "            for _ in range(img_shape//(2 * coef)):\n",
    "                border_leng.append(inp)\n",
    "                inp += 2\n",
    "                \n",
    "    return border_leng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4afaf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36598/2023808686.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmin_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mNEURAL_TYPE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'conv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNEURAL_TYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mNEURAL_TYPE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'conv'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mborder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_border_leng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_layers' is not defined"
     ]
    }
   ],
   "source": [
    "min_value = 0 \n",
    "NEURAL_TYPE = 'conv'\n",
    "layers = create_layers(data.shape[-1], tp=NEURAL_TYPE)\n",
    "if NEURAL_TYPE == 'conv':\n",
    "    border = create_border_leng(data.shape[-1])\n",
    "node_embeddings = []\n",
    "\n",
    "for value_map in result:\n",
    "    max_value = value_map[-1][-1].astype(int)\n",
    "    for rad in range(min_value, max_value+1):\n",
    "        values = data[value_map == rad]\n",
    "        if NEURAL_TYPE == 'linear':\n",
    "            values = torch.from_numpy(values).float()\n",
    "            node_embeddings.append(layers[rad](values))\n",
    "        elif NEURAL_TYPE == 'conv':\n",
    "            values2d = torch.from_numpy(reshape_2d(values, border[rad])).float()\n",
    "            values2d = values2d.unsqueeze(0)\n",
    "            node_embeddings.append(layers[rad](values2d))\n",
    "    min_value = max_value + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fe4b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e870bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fourier batches\n",
    "from PIL import Image\n",
    "\n",
    "def normalization(value):\n",
    "    value -= value.min()\n",
    "    value /= value.max()\n",
    "    return value\n",
    "\n",
    "def split_array(imgs, split_coefs=None):\n",
    "    if split_coefs is None:\n",
    "        split_coefs = [1, 2, 4]\n",
    "    fin_res_am = []\n",
    "    fin_res_ph = []\n",
    "    \n",
    "    *_, h, w = imgs.shape\n",
    "    for coef in split_coefs:\n",
    "        row_data_amp = []\n",
    "        row_data_ph = []\n",
    "        step = h // coef\n",
    "        # add fft\n",
    "        try:\n",
    "            for y in range(0, w, step):\n",
    "                result_amp = []\n",
    "                result_ph = []\n",
    "                for x in range(0, h, step):\n",
    "                    fft_data = np.fft.fftshift(np.fft.fft2(imgs[..., x:x + step, y:y + step]))\n",
    "                    amp_val, ph_val = np.abs(fft_data), np.angle(fft_data)\n",
    "                    amp_val = np.log(1 +  amp_val) #??\n",
    "                    amp_val, ph_val = normalization(amp_val), normalization(ph_val)\n",
    "                    result_amp.append(amp_val)\n",
    "                    result_ph.append(ph_val)\n",
    "\n",
    "                row_data_amp.append(np.concatenate(result_amp, axis=1))\n",
    "                row_data_ph.append(np.concatenate(result_ph, axis=1))\n",
    "        except:\n",
    "            print(x, y)\n",
    "        fin_res_am.append(np.concatenate(row_data_amp, axis=0)) \n",
    "        fin_res_ph.append(np.concatenate(row_data_ph, axis=0)) \n",
    "    return fin_res_am, fin_res_ph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff270385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import numpy as np\n",
    "bad_files = ['/raid/data/cats_dogs_dataset/PetImages/Cat/666.jpg', '/raid/data/cats_dogs_dataset/PetImages/Cat/Thumbs.db',\n",
    "            '/raid/data/cats_dogs_dataset/PetImages/Dog/Thumbs.db', '/raid/data/cats_dogs_dataset/PetImages/Dog/11702.jpg',]\n",
    "\n",
    "def dataset_preprocessing(directory, type_list=[('NORMAL', 0), ('PNEUMONIA', 1)],\n",
    "                          img_shape=(256, 256), target_dirname='preprocessed'):\n",
    "    result_data = []\n",
    "    # if not os.path.exists(f'/raid/data/cats_dogs_dataset/{target_dirname}'):\n",
    "        # os.mkdir(f'/raid/data/cats_dogs_dataset/{target_dirname}')\n",
    "    if not os.path.exists(f'/raid/data/chest_xray/{target_dirname}'):\n",
    "        os.mkdir(f'/raid/data/chest_xray/{target_dirname}')\n",
    "\n",
    "\n",
    "    for dir_name, target_type in type_list:\n",
    "        for file in tqdm(glob.glob(f'{directory}/{dir_name}/*')):\n",
    "            if file in bad_files:\n",
    "                continue\n",
    "            data = np.array(Image.open(file).convert('L').resize(img_shape, Image.ANTIALIAS))\n",
    "            try:\n",
    "                data_amp, data_phase = split_tensor(data)\n",
    "            except:\n",
    "                bad_files.append(file)\n",
    "                continue\n",
    "            if not os.path.exists(f'/raid/data/chest_xray/{target_dirname}/{dir_name}'):\n",
    "                os.mkdir(f'/raid/data/chest_xray/{target_dirname}/{dir_name}')\n",
    "            # np.save(f'/raid/data/cats_dogs_dataset/{target_dirname}/{dir_name}/{Path(file).stem}.npy',\n",
    "            #         [data_amp, data_phase, target_type])\n",
    "            np.save(f'/raid/data/chest_xray/{target_dirname}/{dir_name}/{Path(file).stem}.npy',\n",
    "                    [data_amp, data_phase, target_type])\n",
    "# def dataset_preprocessing(directory, type_list=[('Cat', 0), ('Dog', 1)],\n",
    "#                           img_shape=(256, 256), target_dirname='preprocessed'):\n",
    "#     result_data = []\n",
    "#     if not os.path.exists(f'/raid/data/cats_dogs_dataset/{target_dirname}'):\n",
    "#         os.mkdir(f'/raid/data/cats_dogs_dataset/{target_dirname}')\n",
    "\n",
    "#     for dir_name, target_type in type_list:\n",
    "#         for file in tqdm(glob.glob(f'{directory}/{dir_name}/*')):\n",
    "#             if file in bad_files:\n",
    "#                 continue\n",
    "#             data = np.array(Image.open(file).convert('L').resize(img_shape, Image.ANTIALIAS))\n",
    "# #             try:\n",
    "#             if file in bad_files:\n",
    "#                 continue\n",
    "#             data_amp, data_phase = split_array(data)\n",
    "# #             except:\n",
    "# #                 bad_files.append(file)\n",
    "# #                 continue\n",
    "#             if not os.path.exists(f'/raid/data/cats_dogs_dataset/{target_dirname}/{dir_name}'):\n",
    "#                 os.mkdir(f'/raid/data/cats_dogs_dataset/{target_dirname}/{dir_name}')\n",
    "#             np.save(f'/raid/data/cats_dogs_dataset/{target_dirname}/{dir_name}/{Path(file).stem}.npy',\n",
    "#                     [data_amp, data_phase, target_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f4eabf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raid/data/cats_dogs_dataset/PetImages/Cat/666.jpg',\n",
       " '/raid/data/cats_dogs_dataset/PetImages/Cat/Thumbs.db',\n",
       " '/raid/data/cats_dogs_dataset/PetImages/Dog/Thumbs.db',\n",
       " '/raid/data/cats_dogs_dataset/PetImages/Dog/11702.jpg']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07dd1f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_xray  processed  train\r\n"
     ]
    }
   ],
   "source": [
    "!ls /raid/data/chest_xray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d36ad9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                              | 0/1583 [00:00<?, ?it/s]/tmp/ipykernel_9384/3852367431.py:22: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  data = np.array(Image.open(file).convert('L').resize(img_shape, Image.ANTIALIAS))\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1583/1583 [00:47<00:00, 33.68it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4273/4273 [00:47<00:00, 89.06it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_preprocessing('/raid/data/chest_xray/train', target_dirname='preprocessed_xray_circ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb487bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAT - 0, DOG - 1\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "bad_files = ['/raid/data/cats_dogs_dataset/PetImages/Cat/666.jpg', '/raid/data/cats_dogs_dataset/PetImages/Cat/Thumbs.db',\n",
    "            '/raid/data/cats_dogs_dataset/PetImages/Dog/Thumbs.db', '/raid/data/cats_dogs_dataset/PetImages/Dog/11702.jpg',]\n",
    "def dataset_preprocessing(directory, type_list = [('Cat', 0) , ('Dog', 1)], img_shape=(256,256)):\n",
    "    result_data = []\n",
    "    if not os.path.exists(f'/raid/data/cats_dogs_dataset/{target_dirname}'):\n",
    "        os.mkdir(f'/raid/data/cats_dogs_dataset/{target_dirname}')\n",
    "\n",
    "    for dir_name, target_type in type_list:\n",
    "        for file in glob.glob(f'{directory}/{dir_name}/*'):\n",
    "            print(file)\n",
    "            if file in bad_files:\n",
    "                continue\n",
    "            data = np.array(Image.open(file).convert('L').resize(img_shape, Image.ANTIALIAS))\n",
    "            data_amp, data_phase = split_array(data)\n",
    "            np.save(f'/raid/data/cats_dogs_dataset/preprocessed_circ_basic/{dir_name}/{Path(file).stem}.npy', \n",
    "                    [data_amp, data_phase, target_type])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bb8cdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                             | 0/12500 [00:00<?, ?it/s]/tmp/ipykernel_9177/3591442012.py:14: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  data = np.array(Image.open(file).convert('L').resize(img_shape, Image.ANTIALIAS))\n",
      "  0%|                                                                                                                                                                             | 0/12500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'split_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/raid/data/cats_dogs_dataset/PetImages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 18\u001b[0m, in \u001b[0;36mdataset_preprocessing\u001b[0;34m(directory, type_list, img_shape, target_dirname)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m bad_files:\n\u001b[1;32m     17\u001b[0m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m             data_amp, data_phase \u001b[38;5;241m=\u001b[39m \u001b[43msplit_array\u001b[49m(data)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#             except:\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#                 bad_files.append(file)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#                 continue\u001b[39;00m\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/raid/data/cats_dogs_dataset/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_dirname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdir_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'split_array' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_preprocessing('/raid/data/cats_dogs_dataset/PetImages')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6013e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2cadd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN_img.ipynb  Untitled.ipynb  create_dataset_for_GNN.ipynb  main.py  wandb\r\n",
      "MADE.ipynb     conf.yml        final_projects\t\t     venv\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05a134c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tensor(size, split_coefs=None):\n",
    "    if split_coefs is None:\n",
    "        split_coefs = [1, 2, 4]\n",
    "    result = []\n",
    "\n",
    "    last_val = 0\n",
    "    for coef in split_coefs:\n",
    "        step = size // coef\n",
    "        # add fft\n",
    "        row_data = []\n",
    "        for y in range(0, size, step):\n",
    "            step_result = []\n",
    "            for x in range(0, size, step):\n",
    "                #temperory unused\n",
    "                mask = create_circular(step, step, min_val=last_val)\n",
    "                last_val = mask[-1][-1] + 1\n",
    "                step_result.append(mask)\n",
    "            row_data.append(np.concatenate(step_result, axis=1))\n",
    "        result.append(np.concatenate(row_data, axis=0)) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c91974f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 256\n",
    "global_map = split_tensor(SIZE)\n",
    "\n",
    "\n",
    "class ConvAutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, band_length, out_features:int, in_channels=1, hidden=16, kernel=2):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Conv2d(in_channels, hidden, kernel = kernel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.decoder = nn.Linear(hidden * (band_legnth - 1), out_features)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.decoder(self.relu(self.encoder(X)))    \n",
    "                            \n",
    "class LinearAutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, features: int, hidden: int, out_features:int):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(features, hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.decoder = nn.Linear(hidden, out_features)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.decoder(self.relu(self.encoder(X)))\n",
    "\n",
    "def create_layers(img_shape, split_coefs=None, hidden=16, out=10, tp = 'linear'):\n",
    "    layers = nn.ModuleList()\n",
    "\n",
    "    if split_coefs is None:\n",
    "        split_coefs = [1, 2, 4]\n",
    "\n",
    "    for coef in split_coefs:\n",
    "        #Add simular pieces\n",
    "        for _ in range(coef**2):\n",
    "            input_layers = 4 if tp=='linear' else 1\n",
    "            for _ in range(img_shape//(2  * coef)):\n",
    "                #Linear\n",
    "                if tp == 'linear':\n",
    "                    layers.append(LinearAutoEncoder(input_layers, hidden, out).to(device))\n",
    "                    input_layers += 8\n",
    "                elif tp == 'conv':\n",
    "                    layers.append(ConvAutoEncoder(input_layers, out, hidden=hidden).to(device))\n",
    "                    input_layers += 4\n",
    "\n",
    "    return layers\n",
    "\n",
    "def get_embedding(data, layers, border=None):\n",
    "    result = []\n",
    "    for batch in data:\n",
    "        min_value = 0\n",
    "        node_embeddings = []\n",
    "        for data_map, value_map in zip(batch, global_map):\n",
    "            value_map = value_map.astype(int)\n",
    "            max_value = value_map[-1][-1].astype(int)\n",
    "            \n",
    "            for rad in range(min_value, max_value+1):\n",
    "                values = torch.from_numpy(data_map[value_map == rad]).float().to(device)\n",
    "                if NEURAL_TYPE == 'linear':\n",
    "                    node_embeddings.append(layers[rad](values))\n",
    "                elif NEURAL_TYPE == 'conv':\n",
    "                    values2d = torch.from_numpy(reshape_2d(values, border[rad])).float()\n",
    "                    values2d = values2d.unsqueeze(0)\n",
    "                    node_embeddings.append(layers[rad](values2d))\n",
    "            min_value = max_value + 1\n",
    "        result.append(torch.stack(node_embeddings))\n",
    "    return torch.stack(result)\n",
    "\n",
    "def reshape_2d(array, border_size):\n",
    "    ptr_st, ptr_end = border_size, len(array) - border_size\n",
    "    data1 = list(array[:ptr_st])\n",
    "    data2 = list(array[ptr_end:])\n",
    "    itt_leng = 4\n",
    "    while ptr_st != ptr_end:\n",
    "        if itt_leng > 2:\n",
    "            data1.append(array[ptr_st])\n",
    "        else:\n",
    "            data2.append(array[ptr_st])\n",
    "        ptr_st += 1\n",
    "        itt_leng -= 1\n",
    "        if itt_leng == 0:\n",
    "            itt_leng = 4\n",
    "    return np.stack([data1, data2])\n",
    "\n",
    "def create_border_leng(img_shape, split_coefs=None):\n",
    "    border_leng = []\n",
    "\n",
    "    if split_coefs is None:\n",
    "        split_coefs = [1, 2, 4]\n",
    "    for coef in split_coefs:\n",
    "        #Add simular pieces\n",
    "        for _ in range(coef**2):\n",
    "            inp = 2\n",
    "            for _ in range(img_shape//(2 * coef)):\n",
    "                border_leng.append(inp)\n",
    "                inp += 2\n",
    "\n",
    "    return border_leng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20d99e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(data, layers, border=None):\n",
    "    min_value = 0 \n",
    "    node_embeddings = []\n",
    "    for data_map, value_map in zip(data, global_map):\n",
    "        value_map = value_map.astype(int)\n",
    "        max_value = value_map[-1][-1].astype(int)\n",
    "        \n",
    "        for rad in range(min_value, max_value+1):\n",
    "            values = torch.from_numpy(data_map[value_map == rad]).float().to(device)\n",
    "            if NEURAL_TYPE == 'linear':\n",
    "                node_embeddings.append(layers[rad](values))\n",
    "            elif NEURAL_TYPE == 'conv':\n",
    "                values2d = torch.from_numpy(reshape_2d(values, border[rad])).float()\n",
    "                values2d = values2d.unsqueeze(0)\n",
    "                node_embeddings.append(layers[rad](values2d))\n",
    "        min_value = max_value + 1\n",
    "    return torch.stack(node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d739d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ba5b4be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4295/170061189.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a1f4079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 256\n",
    "NEURAL_TYPE = 'linear'\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv,SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_classes, size=256, out=10, emb_type = 'linear'):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.neural_type = emb_type\n",
    "#         \n",
    "        self.layers = create_layers(size, tp=NEURAL_TYPE, out=out)\n",
    "        \n",
    "        if emb_type == 'conv':\n",
    "            self.border = create_border_leng(data.shape[-1])\n",
    "        else:\n",
    "            self.border = None\n",
    "        \n",
    "        \n",
    "        self.conv1 = SAGEConv(out, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.lin = nn.Linear(hidden_channels, num_classes)\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \n",
    "        x = get_embedding(x, self.layers, self.border) # current linear\n",
    "        x = x.view(2 * 896, -1)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        #x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return self.soft(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fbc5f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(num_classes=1, hidden_channels = 128, emb_type='linear')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001, weight_decay = 0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2198bd47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7981)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(torch.tensor([[0.1,0.3]]), torch.tensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7236f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_circular(h, w, max_numb=None, center=None, tolerance=1, min_val=0):\n",
    "    if center is None:  # use the middle of the image\n",
    "        center = (w / 2 - 0.5, h / 2 - 0.5)\n",
    "    if max_numb is None:\n",
    "        max_numb = max(h//2, w//2)\n",
    "\n",
    "    dist_from_center = np.zeros((h,w))\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            dist_from_center[i][j] = int(max(abs(i - center[0]), abs(j-center[1]))//tolerance + min_val)\n",
    "    return dist_from_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f4a1764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset, download_url\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class MyOwnDataset(Dataset):\n",
    "    def __init__(self, root, files_list, is_train, size=256, allow_loops=False, transform=None, pre_transform=None,\n",
    "                 pre_filter=None):\n",
    "        self.data = files_list\n",
    "        self.allow_loops = allow_loops\n",
    "        self.is_train = is_train\n",
    "        # TODO fix this shit\n",
    "        self.gl_count = 1750 if self.is_train else 750\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def num_nodes(self):\n",
    "        return SIZE // 2 + SIZE // 4 * 4 + SIZE // 8 * 16\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f'data_{idx}_is_train_{self.is_train}_loops={self.allow_loops}.pt' for idx in range(self.gl_count)]\n",
    "\n",
    "    def _create_cco_matrix(self):\n",
    "        result = split_tensor(SIZE)\n",
    "        vert_amount = int(result[2][-1][-1] + 1)\n",
    "        adj_matrix = np.zeros((vert_amount, vert_amount)).astype(int)\n",
    "\n",
    "        gl_map, gen_map, loc_map = result[0].astype(int), result[1].astype(int), result[2].astype(int)\n",
    "        for gl, gen, loc in np.nditer([gl_map, gen_map, loc_map]):\n",
    "            adj_matrix[gl, gen] = 1\n",
    "            adj_matrix[gl, loc] = 1\n",
    "            adj_matrix[gen, loc] = 1\n",
    "            if self.allow_loops:\n",
    "                adj_matrix[gl, gl] = 1\n",
    "                adj_matrix[loc, loc] = 1\n",
    "                adj_matrix[gen, gen] = 1\n",
    "\n",
    "        source_nodes = []\n",
    "        target_nodes = []\n",
    "        edge_list = []\n",
    "        for iy, ix in np.ndindex(adj_matrix.shape):\n",
    "            if adj_matrix[iy, ix] == 1:\n",
    "                source_nodes.append(ix)\n",
    "                target_nodes.append(iy)\n",
    "\n",
    "                # unweighted solution\n",
    "                edge_list.append(1)\n",
    "\n",
    "        return source_nodes, target_nodes, edge_list\n",
    "\n",
    "    def process(self):\n",
    "        idx = 0\n",
    "        source_vert, target_vert, edge_list = self._create_cco_matrix()\n",
    "        edge_idx = torch.tensor([source_vert, target_vert])\n",
    "\n",
    "        # DEBUG ROW\n",
    "        _, small_data = train_test_split(self.data, test_size=0.1, random_state=42)\n",
    "        train_data, test_data = train_test_split(small_data, test_size=0.3, random_state=42)\n",
    "        data = train_data if self.is_train else test_data\n",
    "\n",
    "        for file in data:\n",
    "            # Read data from `raw_path`.\n",
    "            amp, phase, target = np.load(file, allow_pickle=True)\n",
    "\n",
    "            # temporary only amp|\n",
    "            data = Data(x=amp,\n",
    "                        # edge_index=torch.tensor(edge_idx).clone().detach().float().requires_grad_(True),\n",
    "                        edge_index=edge_idx,\n",
    "                        edge_attrs=edge_list,\n",
    "                        y=torch.tensor([target]))\n",
    "\n",
    "            torch.save(data, osp.join(self.processed_dir, f'data_{idx}_is_train_{self.is_train}_loops={self.allow_loops}.pt'))\n",
    "            idx += 1\n",
    "        self.gl_count = idx\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}_is_train_{self.is_train}_loops={self.allow_loops}.pt'))\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01ced345",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20291/659088234.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msave_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/raid/data/cats_dogs_dataset/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/raid/data/cats_dogs_dataset/preprocessed/*/*.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyOwnDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyOwnDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "save_root = '/raid/data/cats_dogs_dataset/'\n",
    "files = glob.glob('/raid/data/cats_dogs_dataset/preprocessed/*/*.npy', recursive=True)\n",
    "train_dataset = MyOwnDataset(save_root, files, is_train = True)\n",
    "test_dataset = MyOwnDataset(save_root, files, is_train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ebdf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76531081",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36598/3343338999.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msave_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/raid/data/cats_dogs_dataset/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/raid/data/cats_dogs_dataset/preprocessed/*/*.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "device = torch.device('cpu')\n",
    "save_root = '/raid/data/cats_dogs_dataset/'\n",
    "files = glob.glob('/raid/data/cats_dogs_dataset/preprocessed/*/*.npy', recursive=True)\n",
    "train_dataset = MyOwnDataset(save_root, files, is_train = True)\n",
    "test_dataset = MyOwnDataset(save_root, files, is_train = False)\n",
    "\n",
    "model = GCN(num_classes=2, hidden_channels = 10).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001, weight_decay = 0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "batch_size = 2\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c0b2316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# wandb.init(project=\"GNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "55dbec0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6935, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7078, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7055, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6975, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6955, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6969, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6879, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6991, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7003, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6998, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6872, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6990, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6885, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6881, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6872, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6841, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6827, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6811, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7068, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7083, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6774, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6761, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6743, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7160, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7167, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7169, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7154, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6719, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6725, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7145, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6731, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7146, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7136, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7130, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7120, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6768, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6931, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6785, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6787, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7080, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6784, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7082, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7080, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7073, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7060, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6827, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7031, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4295/691641654.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mbest_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4295/691641654.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#         if itt % 100 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#             wandb.log({\"train_cross_entropy_loss\": loss.item()})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Derive gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update parameters based on gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# weights_stem = 'GNN_Linear_No_loops_bsize=1'\n",
    "# wandb.run.name = weights_stem\n",
    "\n",
    "\n",
    "#                 experiment.log_metric(\"train_dice_loss\", batch_loss.item(),\n",
    "#                                       epoch=epoch_idx, step=step_counter[action])\n",
    "def train():\n",
    "    model.train()\n",
    "    itt = 0\n",
    "    for itt, data in enumerate(train_loader):  # Iterate in batches over the training dataset.\n",
    "#         out = model(data.x, data.edge_index, data.batch) \n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        print(loss)\n",
    "\n",
    "#         if itt % 100 == 0:\n",
    "#             wandb.log({\"train_cross_entropy_loss\": loss.item()})\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for itt, data in enumerate(loader):  # %ђIterate in batches over the training/test dataset.\n",
    "        out = model(data.x[0], data.edge_index, data.batch) # Perform a single forward pass.\n",
    "        data = data.to(device)\n",
    "        pred = out.argmax(dim=1)# Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "#         correct += int((pred == data.y>.squeeze().unsqueeze(0).float()).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "best_train, cur_epoch, best_val = -1, -1, -1 \n",
    "for epoch in range(1, 500):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    best_train, cur_epoch, best_val = (train_acc, epoch, test_acc) if test_acc > best_val \\\n",
    "                                    else (best_train, cur_epoch, best_val)\n",
    "    wandb.log({'best train accuracy': best_train})\n",
    "    wandb.log({'best test accuracy': best_val})\n",
    "    \n",
    "    wandb.log({'current train accuracy': train_acc})\n",
    "    wandb.log({'current test accuracy': test_acc})\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "print(f'Best test accuracy - {best_val} on epoch {cur_epoch} with train accuracy - {best_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e242cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "amp = np.load('/raid/data/cats_dogs_dataset/preprocessed/Cat/0.npy', allow_pickle=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6539f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d7d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = 0 \n",
    "NEURAL_TYPE = 'conv'\n",
    "layers = create_layers(data.shape[-1], tp=NEURAL_TYPE)\n",
    "if NEURAL_TYPE == 'conv':\n",
    "    border = create_border_leng(data.shape[-1])\n",
    "node_embeddings = []\n",
    "\n",
    "for value_map in result:\n",
    "    max_value = value_map[-1][-1].astype(int)\n",
    "    for rad in range(min_value, max_value+1):\n",
    "        values = data[value_map == rad]\n",
    "        if NEURAL_TYPE == 'linear':\n",
    "            values = torch.from_numpy(values).float()\n",
    "            node_embeddings.append(layers[rad](values))\n",
    "        elif NEURAL_TYPE == 'conv':\n",
    "            values2d = torch.from_numpy(reshape_2d(values, border[rad])).float()\n",
    "            values2d = values2d.unsqueeze(0)\n",
    "            node_embeddings.append(layers[rad](values2d))\n",
    "    min_value = max_value + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71c661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat  Dog\r\n"
     ]
    }
   ],
   "source": [
    "!ls /raid/data/cats_dogs_dataset/PetImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "968fa3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f415d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 25000\n",
       "    Root location: /raid/data/cats_dogs_dataset/PetImages"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.datasets.ImageFolder(root='/raid/data/cats_dogs_dataset/PetImages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "489be0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff9a3cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_Image(path):\n",
    "    return os.path.getsize(path) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d93bf201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.getsize('/raid/data/cats_dogs_dataset/PetImages/Cat/666.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5145ad40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/v_shaposhnikov/env/lib/python3.9/site-packages (0.13.1)\n",
      "Requirement already satisfied: requests in /home/v_shaposhnikov/env/lib/python3.9/site-packages (from torchvision) (2.26.0)\n",
      "Requirement already satisfied: numpy in /home/v_shaposhnikov/env/lib/python3.9/site-packages (from torchvision) (1.21.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/v_shaposhnikov/env/lib/python3.9/site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: torch==1.12.1 in /home/v_shaposhnikov/env/lib/python3.9/site-packages (from torchvision) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions in /home/v_shaposhnikov/env/lib/python3.9/site-packages (from torchvision) (4.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/v_shaposhnikov/env/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/v_shaposhnikov/env/lib/python3.9/site-packages (from requests->torchvision) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/v_shaposhnikov/env/lib/python3.9/site-packages (from requests->torchvision) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/v_shaposhnikov/env/lib/python3.9/site-packages (from requests->torchvision) (2021.10.8)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24203e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mNORMAL\u001b[0m/  \u001b[01;34mPNEUMONIA\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls ../../chest_xray/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d355507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import os\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "_, small_data = train_test_split(torchvision.datasets.ImageFolder(root='/raid/data/cats_dogs_dataset/PetImages/', \n",
    "                                                                          is_valid_file=check_Image, transform=transform), test_size=0.5, random_state=42)\n",
    "\n",
    "train_data, test_data = train_test_split(small_data, \n",
    "                                         test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa02439b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12178/1513391733.py:26: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.weights)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision.models import resnet18\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=64\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "model = FreqNet(num_classes=2).to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98f5ff39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397588 sum\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()), 'sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5785961b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|███▎                                                                                                                                                                 | 1/49 [02:39<2:07:58, 159.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.5033, Test Acc: 0.4813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|██████▋                                                                                                                                                              | 2/49 [05:18<2:04:47, 159.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Train Acc: 0.4954, Test Acc: 0.4864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|██████████                                                                                                                                                           | 3/49 [07:57<2:02:02, 159.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Train Acc: 0.4987, Test Acc: 0.5051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|█████████████▍                                                                                                                                                       | 4/49 [10:36<1:59:12, 158.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Train Acc: 0.4987, Test Acc: 0.4984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████████████▊                                                                                                                                                    | 5/49 [13:14<1:56:25, 158.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Train Acc: 0.4973, Test Acc: 0.4928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|████████████████████▏                                                                                                                                                | 6/49 [15:53<1:53:48, 158.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Train Acc: 0.4926, Test Acc: 0.4832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|███████████████████████▌                                                                                                                                             | 7/49 [18:32<1:51:03, 158.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Train Acc: 0.4962, Test Acc: 0.5173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████████████████████████▉                                                                                                                                          | 8/49 [21:10<1:48:19, 158.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Train Acc: 0.5029, Test Acc: 0.4976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|██████████████████████████████▎                                                                                                                                      | 9/49 [23:47<1:45:29, 158.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Train Acc: 0.4940, Test Acc: 0.4987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|█████████████████████████████████▍                                                                                                                                  | 10/49 [26:25<1:42:46, 158.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Train Acc: 0.4949, Test Acc: 0.4955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|████████████████████████████████████▊                                                                                                                               | 11/49 [29:03<1:40:02, 157.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 011, Train Acc: 0.4955, Test Acc: 0.4923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|████████████████████████████████████████▏                                                                                                                           | 12/49 [31:41<1:37:28, 158.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 012, Train Acc: 0.5035, Test Acc: 0.4997\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def train():\n",
    "    model.train()\n",
    "    for itt, (data, label) in enumerate(train_loader):  # Iterate in batches over the training dataset.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        out = model(data)  # Perform a single forward pass.\n",
    "        loss = criterion(out, label)  # Compute the loss.\n",
    "\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for itt, (data, label) in enumerate(loader):  # %Iterate in batches over the training/test dataset.\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        out = model(data)  # Perform a single forward pass.\n",
    "        data = data.to(device)\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == label).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "best_train, cur_epoch, best_val = -1, -1, -1\n",
    "for epoch in tqdm(range(1, 50)):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    scheduler.step(test_acc)\n",
    "    best_train = train_acc if train_acc > best_train else best_train\n",
    "    best_val = test_acc if test_acc > best_val else best_val\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "print(f'Best test accuracy - {best_val} on epoch {cur_epoch} with train accuracy -> {best_train}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4076f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms.functional import rgb_to_grayscale\n",
    "\n",
    "\n",
    "class TrainableEltwiseLayer(nn.Module):\n",
    "    def __init__(self, b, n):\n",
    "        super(TrainableEltwiseLayer, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.Tensor(b, n))  # define the trainable parameter\n",
    "        self.model_init()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # assuming x is of size b-1-h-w\n",
    "        # print(x.shape)\n",
    "        return x * self.weights  # element-wise multiplication\n",
    "\n",
    "    def model_init(self):\n",
    "        torch.nn.init.xavier_uniform(self.weights)\n",
    "        self.weights.requires_grad = True\n",
    "\n",
    "\n",
    "# Get fourier batches\n",
    "def split_tensor(imgs, split_coefs=None):\n",
    "    if split_coefs is None:\n",
    "        split_coefs = [1, 2, 4]\n",
    "    result = []\n",
    "\n",
    "    *_, h, w = imgs.shape\n",
    "    for coef in split_coefs:\n",
    "        step = h // coef\n",
    "        # add fft\n",
    "        for y in range(0, w, step):\n",
    "            for x in range(0, h, step):\n",
    "                value = torch.log(1 + torch.abs(torch.fft.fftshift(torch.fft.fft2(imgs[..., x:x + step, y:y + step])))).to('cuda:0')\n",
    "                value -= value.min()\n",
    "                value /= value.max()\n",
    "                result.append(value)\n",
    "        #\n",
    "        # tiles = [torch.abs(torch.fft.fftshift(torch.fft.fft2(imgs[..., x:x + step, y:y + step]))).to('cuda:0')\n",
    "        #          for x in range(0, h, step) for y in range(0, w, step)]\n",
    "        # tiles = [imgs[..., x:x + step, y:y + step].to('cuda:0')\n",
    "        #          for x in range(0, h, step) for y in range(0, w, step)]\n",
    "        # result.extend(tiles)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_circular(h, w, max_numb=None, center=None, tolerance=1):\n",
    "    if center is None:  # use the middle of the image\n",
    "        center = (w / 2 - 1/2, h / 2 - 1/2)\n",
    "\n",
    "    dist_from_center = torch.zeros((h, w))\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            dist_from_center[i][j] = max(abs(i - center[0]), abs(j - center[1])) // tolerance\n",
    "\n",
    "    return dist_from_center.type(torch.float).to('cuda:0')\n",
    "\n",
    "\n",
    "def get_summation(data_list, tolerance=2):\n",
    "    data_dict = {}\n",
    "    result = []\n",
    "    for data in data_list:\n",
    "        *_, h, w = data.shape\n",
    "\n",
    "        if not (h, w) in data_dict:\n",
    "            data_dict[(h, w)] = create_circular(h, w, tolerance=tolerance)\n",
    "        data_map = data_dict[(h, w)]\n",
    "        max_val = int(data_map[0][0])\n",
    "\n",
    "        for i in range(max_val + 1):\n",
    "            result.append(torch.sum(data * (data_map == i).float(), dim=(1, 2, 3)))\n",
    "    return result\n",
    "\n",
    "\n",
    "class FreqNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ew_layer = TrainableEltwiseLayer(64, 784)\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(784, 392),\n",
    "            nn.BatchNorm1d(392),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(392, 98),\n",
    "            nn.BatchNorm1d(98),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(98, num_classes)\n",
    "        )\n",
    "\n",
    "        # self.sm = nn.functional.softmax\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, *_ = x.shape\n",
    "        data = split_tensor(x, split_coefs=[1, 2, 4])\n",
    "\n",
    "        x = get_summation(data, tolerance=1)\n",
    "\n",
    "        x = torch.stack(x).permute(1, 0)\n",
    "\n",
    "        logits = self.linear(self.ew_layer(x))\n",
    "\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18e8e57e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m model(\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch, batch_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "out = model(data.x, data.edge_index, data.batch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70a8003c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "/tmp/ipykernel_36967/3437103213.py:211: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  amp = [torch.tensor(item).float() for item in amp]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15015579 sum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                | 0/49 [00:00<?, ?it/s]/home/v_shaposhnikov/.local/lib/python3.9/site-packages/torch_geometric/data/storage.py:304: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'y', 'edge_attr', 'edge_index', 'x'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4009., device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4010.1028, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4011.0566, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4012.3276, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4013.4641, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4013.9331, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4014.7014, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4015.3340, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4015.2693, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4014.9746, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4014.6628, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4014.6511, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4015.1389, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4015.4143, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4015.4956, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4015.7239, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4015.8716, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4015.6970, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4015.9392, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3953, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.9800, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4017.7339, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4018.2009, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4018.5667, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4018.9663, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4018.7212, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4018.2192, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4017.5876, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.7683, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.6533, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.2021, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.2888, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3647, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.7512, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4017.0635, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.8530, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.6499, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|███▍                                                                                                                                                                  | 1/49 [01:38<1:19:01, 98.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.5183, Test Acc: 0.5462\n",
      "tensor(4016.3850, device='cuda:1', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|███▎                                                                                                                                                                 | 1/49 [01:40<1:20:35, 100.73s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 358\u001b[0m\n\u001b[1;32m    356\u001b[0m best_train, cur_epoch, best_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m)):\n\u001b[0;32m--> 358\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m test(train_loader)\n\u001b[1;32m    360\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m test(test_loader)\n",
      "Cell \u001b[0;32mIn[4], line 333\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    330\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear gradients.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 333\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Perform a single forward pass.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my)  \u001b[38;5;66;03m# Compute the loss.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Derive gradients.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 34\u001b[0m, in \u001b[0;36mGNN.forward\u001b[0;34m(self, x, edge_index, batch, batch_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m get_embedding(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers, iso_amount\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miso_amount, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)  \u001b[38;5;66;03m# current linear\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 34\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mget_long_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miso_amount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miso_amount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_pos_embed \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monly_train\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m add_positional_encoding(x, emb_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_out)\n",
      "Cell \u001b[0;32mIn[4], line 80\u001b[0m, in \u001b[0;36mget_long_embedding\u001b[0;34m(data, layers, iso_amount, batch_size)\u001b[0m\n\u001b[1;32m     78\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mreshape(batch_size, iso_amount, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iso_amount):\n\u001b[0;32m---> 80\u001b[0m         node_embeddings\u001b[38;5;241m.\u001b[39mappend(\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mit_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(node_embeddings, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 69\u001b[0m, in \u001b[0;36mLinearAutoEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# x = self.relu(x)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# return self.decoder(x)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/activation.py:102\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import io\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from yaml import safe_load\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import Dataset, download_url\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import glob\n",
    "import networkx as nx\n",
    "\n",
    "SIZE = 256\n",
    "NEURAL_TYPE = 'linear'\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "\n",
    "def create_layers(img_shape, num_blocks=3, hidden=16, out=10, tp='linear', isoclines=16, div_val=4):\n",
    "    layers = nn.ModuleList()\n",
    "\n",
    "    split_coefs = [2 ** i for i in range(num_blocks)]\n",
    "    for coef in split_coefs:\n",
    "        # Add simular pieces, several times\n",
    "        for _ in range(coef ** 2):\n",
    "            input_layers = (img_shape ** 2) // (isoclines * coef ** 2)\n",
    "            layers.append(LinearAutoEncoder(input_layers, hidden, out, div_val))\n",
    "            if tp == 'linear':\n",
    "                break\n",
    "    return layers\n",
    "\n",
    "\n",
    "class LinearAutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, features: int, hidden: int, out_features: int, div_val=4):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential()\n",
    "        inp = features\n",
    "        while inp // div_val > hidden:\n",
    "            self.layers.append(nn.Linear(inp, inp // div_val))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            inp = inp // div_val\n",
    "\n",
    "        self.layers.append(nn.Linear(inp, hidden))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        # self.relu = nn.ReLU()\n",
    "\n",
    "        while inp * div_val < out_features:\n",
    "            self.layers.append(nn.Linear(inp, inp * div_val))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            inp *= div_val\n",
    "\n",
    "        # self.decoder = nn.Linear(hidden, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        # x = self.relu(x)\n",
    "        # return self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_long_embedding(data, layers, iso_amount=16, batch_size=2):\n",
    "    node_embeddings = []\n",
    "    for it_val, value in enumerate(data):\n",
    "        value = value.reshape(batch_size, iso_amount, -1)\n",
    "        for i in range(iso_amount):\n",
    "            node_embeddings.append(layers[it_val](value[:, i, :]))\n",
    "    return torch.stack(node_embeddings, dim=1)\n",
    "\n",
    "\n",
    "def getPositionEncoding(seq_len, d, n=10000):\n",
    "    P = torch.zeros((seq_len, d))\n",
    "    for k in range(seq_len):\n",
    "        for i in torch.arange(int(d / 2)):\n",
    "            denominator = torch.pow(n, 2 * i / d)\n",
    "            P[k, 2 * i] = torch.sin(k / denominator)\n",
    "            P[k, 2 * i + 1] = torch.cos(k / denominator)\n",
    "    return P\n",
    "\n",
    "\n",
    "def get_embedding(data, layers, iso_amount=16, batch_size=2):\n",
    "    node_embeddings = []\n",
    "    for it_val, value in enumerate(data):\n",
    "        value = value.reshape(batch_size, iso_amount, -1)\n",
    "        if it_val == 0:\n",
    "            layer_id = 0\n",
    "        elif 1 <= it_val <= 4:\n",
    "            layer_id = 1\n",
    "        elif 5 <= it_val <= 21:\n",
    "            layer_id = 2\n",
    "        for i in range(iso_amount):\n",
    "            node_embeddings.append(layers[layer_id](value[:, i, :]))\n",
    "\n",
    "    return torch.stack(node_embeddings, dim=1)\n",
    "\n",
    "\n",
    "def add_positional_encoding(embedding, iso_amount=16, emb_len=10):\n",
    "    b, h, c = embedding.shape\n",
    "    big_embed = torch.zeros(b, iso_amount, c)\n",
    "\n",
    "    four_split_embed_func = getPositionEncoding(4, c)\n",
    "    medium_embed = torch.cat([four_split_embed_func[i].unsqueeze(0).repeat(iso_amount, 1).unsqueeze(0).repeat(b, 1, 1)\n",
    "                              for i in range(4)], dim=1)\n",
    "\n",
    "    sixt_split_embed_func = getPositionEncoding(16, c)\n",
    "\n",
    "    small_embed = torch.cat([sixt_split_embed_func[i].unsqueeze(0).repeat(iso_amount, 1).unsqueeze(0).repeat(b, 1, 1)\n",
    "                             for i in range(16)], dim=1)\n",
    "    positional_embed = torch.cat([big_embed, medium_embed, small_embed], dim=1).to(device)\n",
    "    return embedding + positional_embed\n",
    "\n",
    "\n",
    "\n",
    "class MyOwnDataset(Dataset):\n",
    "    def __init__(self, root, files_list, is_train, size=256, allow_loops=False, transform=None, pre_transform=None,\n",
    "                 pre_filter=None, iso_amount=16, split_amount=21):\n",
    "        self.data = files_list\n",
    "        self.allow_loops = allow_loops\n",
    "        self.is_train = is_train\n",
    "        root_path = Path(root)\n",
    "        self._processed_dir = str(Path(root).parent) + '/processed_' + str(Path(root).name)\n",
    "        # TODO fix this shit\n",
    "        self.iso_amount = iso_amount\n",
    "        self.split_amount = split_amount\n",
    "        # 4099 and 1757\n",
    "        # 15015579 sum\n",
    "        # self.gl_count = 17407 if self.is_train else 7461\n",
    "        self.gl_count = 5270 if self.is_train else 586\n",
    "\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def num_nodes(self):\n",
    "        return self.iso_amount * self.split_amount\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f'xdata_{idx}_is_train_{self.is_train}_loops={self.allow_loops}_isoc={self.iso_amount}' \\\n",
    "                f'_split={self.split_amount}.pt' for idx in range(self.gl_count)]\n",
    "\n",
    "    def _create_cco_matrix(self):\n",
    "        vert_amount = int(self.iso_amount * self.split_amount)\n",
    "\n",
    "        adj_matrix = np.zeros((vert_amount, vert_amount))\n",
    "        for i in range(vert_amount - 1):\n",
    "            if self.allow_loops:\n",
    "                adj_matrix[i][i] = nn.Parameter(1)\n",
    "                # adj_matrix[i][i] = nn.Parameter(torch.tensor(1.0))\n",
    "            if i % self.iso_amount != 1:\n",
    "                # adj_matrix[i][i + 1] = nn.Parameter(torch.tensor(1.0))\n",
    "                adj_matrix[i][i + 1] = 1\n",
    "            for j in range(self.split_amount):\n",
    "                if self.iso_amount * j + i < vert_amount:\n",
    "                    # adj_matrix[i][self.iso_amount * j + i] = nn.Parameter(torch.tensor(1.0))\n",
    "                    adj_matrix[i][self.iso_amount * j + i] = 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        source_nodes = []\n",
    "        target_nodes = []\n",
    "        edge_list = []\n",
    "        for iy, ix in np.ndindex(adj_matrix.shape):\n",
    "            # small changes\n",
    "            if adj_matrix[iy, ix] != 0:\n",
    "                source_nodes.append(ix)\n",
    "                target_nodes.append(iy)\n",
    "                edge_list.append(1.0)\n",
    "            # edge_list.append(adj_matrix[iy, ix])\n",
    "            # if adj_matrix[iy, ix] != 0:\n",
    "                # unweighted solution\n",
    "                # edge_list.append(1.0)\n",
    "        return source_nodes, target_nodes, edge_list\n",
    "\n",
    "    def process(self):\n",
    "        idx = 0\n",
    "        source_vert, target_vert, edge_list = self._create_cco_matrix()\n",
    "        edge_idx = torch.tensor([source_vert, target_vert])\n",
    "        # DEBUG ROW\n",
    "        _, small_data = train_test_split(self.data, test_size=0.1, random_state=42)\n",
    "#         small_data = self.data\n",
    "        train_data, test_data = train_test_split(small_data, test_size=0.1, random_state=42)\n",
    "        data = train_data if self.is_train else test_data\n",
    "        # p;rint(edge_list)\n",
    "\n",
    "        for file in data:\n",
    "            # if os.path.exists(osp.join(self.processed_dir,\n",
    "            #                               f'data_{idx}_is_train_{self.is_train}_loops={self.allow_loops}'\n",
    "            #                               f'_isoc={self.iso_amount}_split={self.split_amount}.pt')):\n",
    "            #     print('skip')\n",
    "                # continue\n",
    "            # Read data from `raw_path`.\n",
    "            amp, phase, target = np.load(file, allow_pickle=True)\n",
    "            try:\n",
    "                amp = [torch.tensor(item).float() for item in amp]\n",
    "                # lst = [torch.from_numpy(item).float() for item in lst]\n",
    "\n",
    "                # temporary only amp|\n",
    "                data = Data(x=amp,\n",
    "                            # edge_index=torch.tensor(edge_idx).clone().detach().float().requires_grad_(True),\n",
    "                            edge_index=edge_idx,\n",
    "                            edge_attr=edge_list,\n",
    "                            y=torch.tensor([target]))\n",
    "                torch.save(data, osp.join(self.processed_dir,\n",
    "                                          f'xdata_{idx}_is_train_{self.is_train}_loops={self.allow_loops}'\n",
    "                                          f'_isoc={self.iso_amount}_split={self.split_amount}.pt'))\n",
    "\n",
    "                idx += 1\n",
    "            except:\n",
    "                continue\n",
    "        self.gl_count = idx\n",
    "        print(self.gl_count)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(\n",
    "            osp.join(self.processed_dir,\n",
    "                     f'xdata_{idx}_is_train_{self.is_train}_loops={self.allow_loops}'\n",
    "                     f'_isoc={self.iso_amount}_split={self.split_amount}.pt'))\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class GraphInformation:\n",
    "    def __init__(self, graph_struct):\n",
    "        self.max_val = 0\n",
    "        self.graph_struct = graph_struct.detach().cpu().numpy()\n",
    "        self.adj_matrix = self._get_adj()\n",
    "        self.G = nx.from_numpy_array(self.adj_matrix)\n",
    "\n",
    "\n",
    "    def _get_adj(self):\n",
    "        _, edge_amount = self.graph_struct.shape\n",
    "        self.max_val = np.amax(self.graph_struct)\n",
    "        adj_matrix = np.zeros((self.max_val + 1,  self.max_val + 1))\n",
    "        for i in range(edge_amount):\n",
    "            x, y = self.graph_struct[..., i]\n",
    "            adj_matrix[x][y] = 1\n",
    "            adj_matrix[y][x] = 1\n",
    "        return adj_matrix\n",
    "\n",
    "\n",
    "    def get_adj_view(self, connectivity_array):\n",
    "        _, edge_amount = self.graph_struct.shape\n",
    "        image = np.zeros((self.max_val + 1,  self.max_val + 1))\n",
    "        for i in range(edge_amount):\n",
    "            x, y = self.graph_struct[..., i]\n",
    "            image[x][y] = connectivity_array[i]\n",
    "        return image\n",
    "\n",
    "    def get_matrix_view(self, connectivity_array):\n",
    "        colors = []\n",
    "        for i, j in self.G.edges():\n",
    "            colors.append(connectivity_array[i][j].item())\n",
    "\n",
    "        im_io = io.BytesIO()\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        nx.draw_circular(self.G, ax=ax, edge_color=colors)\n",
    "        fig.savefig(im_io, format='png')\n",
    "        return Image.open(im_io)\n",
    "\n",
    "\n",
    "\n",
    "def get_config(path):\n",
    "    with open(path, 'r') as stream:\n",
    "        config = safe_load(stream)\n",
    "    return config\n",
    "\n",
    "\n",
    "config = get_config('diploma/conf.yml')\n",
    "train_params = config['train_params']\n",
    "model_params = config['model']\n",
    "files_params = config['files_params']\n",
    "\n",
    "device = torch.device(f'cuda:{train_params[\"device_num\"]}')\n",
    "batch_size = config['train_params']['batch_size']\n",
    "\n",
    "save_root = files_params['save_root']\n",
    "files = glob.glob(files_params['path_to_files'], recursive=True)\n",
    "\n",
    "train_dataset = MyOwnDataset(save_root, files, is_train=True, iso_amount=model_params['iso_amount'],\n",
    "                             split_amount=21 if model_params['num_blocks'] == 3 else 85)\n",
    "test_dataset = MyOwnDataset(save_root, files, is_train=False, iso_amount=model_params['iso_amount'],\n",
    "                            split_amount=21 if model_params['num_blocks'] == 3 else 85)\n",
    "graph = train_dataset[0].edge_attr\n",
    "\n",
    "model = GNN(num_classes=2, hidden_encoder=model_params['hidden_encoder_embed'], edge_weight=graph,\n",
    "            hidden_GCN=model_params['hidden_GCN_embed'], encoder_out=model_params['encoder_out'],\n",
    "            emb_type=model_params['emb_type'], div_val=model_params['div_val'],\n",
    "            is_pos_embed=model_params['pos_embed'], depth=model_params['depth'],\n",
    "            num_blocks=model_params['num_blocks'], iso_amount=model_params['iso_amount'],\n",
    "            b_size=train_params['batch_size'], is_edges_trainable=model_params['is_edges_trainable']).to(device)\n",
    "# print(model.parameters)\n",
    "# for name, param in model.named_parameters():\n",
    "    # print(name, param.shape, param.numel())\n",
    "print(sum(p.numel() for p in model.parameters()), 'sum')\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "GraphLogger = GraphInformation(train_dataset[0].edge_index) if model_params['is_edges_trainable'] else None\n",
    "adj_hist, adj_counter = [], 0\n",
    "\n",
    "def train():\n",
    "\n",
    "    model.train()\n",
    "    for itt, data in enumerate(train_loader):  # Iterate in batches over the training dataset.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch, batch_size)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    for itt, data in enumerate(loader):  # %Iterate in batches over the training/test dataset.\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch, batch_size)  # Perform a single forward pass.\n",
    "        data = data.to(device)\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    # adj_view = GraphLogger.get_adj_view(model.edge_weight.detach())\n",
    "    #\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "best_train, cur_epoch, best_val = -1, -1, -1\n",
    "for epoch in tqdm(range(1, 50)):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    scheduler.step(test_acc)\n",
    "    best_train = train_acc if train_acc > best_train else best_train\n",
    "    best_val = test_acc if test_acc > best_val else best_val\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "print(f'Best test accuracy - {best_val} on epoch {cur_epoch} with train accuracy -> {best_train}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a1de27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4009])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.edge_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23ccd6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   1,  16,  ..., 334, 334, 335], device='cuda:1')\n",
      "tensor([  0,   0,   0,  ..., 333, 334, 334], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "print(data.edge_index[0][:4009])\n",
    "print(data.edge_index[1][:4009])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a58ab7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data.edge_index[:4009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fe78973",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x, y) \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39medge_index[:\u001b[38;5;241m4009\u001b[39m]:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x, y)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for (x, y) in data.edge_index[:4009]:\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9851f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vals = {}\n",
    "for pos, (x, y) in enumerate(zip(data.edge_index[0][:4009], data.edge_index[1][:4009])):\n",
    "    dict_vals[(x.item(), y.item())] = model.edge_weight[pos].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebb79c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('diploma/dicts/dct_isoclines_hid=256_out=512_enc=256.pkl', 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "706f563c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(287, 271): 1.0496597290039062,\n",
       " (177, 176): 1.038428544998169,\n",
       " (223, 207): 1.0378994941711426,\n",
       " (209, 208): 1.036803126335144,\n",
       " (257, 256): 1.0346049070358276,\n",
       " (335, 319): 1.033868670463562,\n",
       " (287, 255): 1.0338099002838135,\n",
       " (273, 272): 1.033699870109558,\n",
       " (241, 240): 1.033613920211792,\n",
       " (287, 287): 1.0326157808303833,\n",
       " (289, 273): 1.0325384140014648,\n",
       " (271, 271): 1.0323714017868042,\n",
       " (223, 223): 1.031805396080017,\n",
       " (335, 303): 1.0312334299087524,\n",
       " (193, 192): 1.0310077667236328,\n",
       " (287, 239): 1.0308737754821777,\n",
       " (223, 191): 1.030660629272461,\n",
       " (257, 257): 1.0306452512741089,\n",
       " (209, 209): 1.030386209487915,\n",
       " (319, 319): 1.0303577184677124,\n",
       " (256, 256): 1.0296953916549683,\n",
       " (303, 303): 1.0295610427856445,\n",
       " (177, 161): 1.0295510292053223,\n",
       " (257, 241): 1.0294208526611328,\n",
       " (321, 320): 1.0292924642562866,\n",
       " (334, 318): 1.0289602279663086,\n",
       " (241, 241): 1.0288432836532593,\n",
       " (305, 304): 1.0286391973495483,\n",
       " (304, 304): 1.0282412767410278,\n",
       " (305, 305): 1.0280476808547974,\n",
       " (319, 303): 1.027999758720398,\n",
       " (176, 160): 1.0279104709625244,\n",
       " (255, 255): 1.0278613567352295,\n",
       " (256, 240): 1.0278337001800537,\n",
       " (176, 176): 1.0278245210647583,\n",
       " (257, 225): 1.0277903079986572,\n",
       " (289, 288): 1.0276881456375122,\n",
       " (289, 289): 1.027571678161621,\n",
       " (321, 321): 1.0274624824523926,\n",
       " (320, 320): 1.0274149179458618,\n",
       " (318, 318): 1.0274083614349365,\n",
       " (177, 177): 1.0274029970169067,\n",
       " (304, 288): 1.0273703336715698,\n",
       " (191, 191): 1.0272482633590698,\n",
       " (308, 307): 1.027151107788086,\n",
       " (317, 317): 1.027119517326355,\n",
       " (309, 308): 1.0269912481307983,\n",
       " (315, 314): 1.026929497718811,\n",
       " (305, 273): 1.0269092321395874,\n",
       " (314, 313): 1.026896595954895,\n",
       " (318, 317): 1.0268681049346924,\n",
       " (313, 312): 1.0267537832260132,\n",
       " (316, 316): 1.0267382860183716,\n",
       " (313, 313): 1.0267328023910522,\n",
       " (311, 310): 1.0267192125320435,\n",
       " (288, 272): 1.0267117023468018,\n",
       " (314, 314): 1.0267114639282227,\n",
       " (316, 315): 1.0266892910003662,\n",
       " (319, 318): 1.0266506671905518,\n",
       " (307, 307): 1.026636004447937,\n",
       " (310, 309): 1.0266329050064087,\n",
       " (315, 315): 1.0266320705413818,\n",
       " (312, 311): 1.0266215801239014,\n",
       " (303, 271): 1.0265902280807495,\n",
       " (308, 308): 1.0265707969665527,\n",
       " (222, 221): 1.0265580415725708,\n",
       " (317, 316): 1.0265542268753052,\n",
       " (312, 312): 1.0264976024627686,\n",
       " (310, 310): 1.0264296531677246,\n",
       " (307, 306): 1.0263861417770386,\n",
       " (309, 309): 1.0263786315917969,\n",
       " (311, 311): 1.0263639688491821,\n",
       " (307, 291): 1.0262736082077026,\n",
       " (305, 289): 1.0262513160705566,\n",
       " (308, 292): 1.0261187553405762,\n",
       " (221, 220): 1.025948405265808,\n",
       " (302, 302): 1.0259443521499634,\n",
       " (303, 302): 1.0258251428604126,\n",
       " (218, 217): 1.0257608890533447,\n",
       " (306, 306): 1.0257564783096313,\n",
       " (225, 225): 1.0256247520446777,\n",
       " (240, 240): 1.0255961418151855,\n",
       " (255, 239): 1.025580883026123,\n",
       " (217, 216): 1.025546908378601,\n",
       " (191, 159): 1.0254981517791748,\n",
       " (326, 325): 1.0254944562911987,\n",
       " (259, 258): 1.0254740715026855,\n",
       " (334, 334): 1.0254453420639038,\n",
       " (292, 291): 1.0254316329956055,\n",
       " (220, 219): 1.025403618812561,\n",
       " (302, 301): 1.0253922939300537,\n",
       " (327, 326): 1.0253688097000122,\n",
       " (286, 270): 1.0252739191055298,\n",
       " (260, 259): 1.0252270698547363,\n",
       " (325, 325): 1.025210976600647,\n",
       " (191, 175): 1.0252060890197754,\n",
       " (328, 327): 1.0251349210739136,\n",
       " (326, 326): 1.0250405073165894,\n",
       " (329, 328): 1.024975299835205,\n",
       " (219, 218): 1.0249680280685425,\n",
       " (293, 292): 1.024938941001892,\n",
       " (330, 329): 1.024926781654358,\n",
       " (327, 327): 1.0249093770980835,\n",
       " (301, 301): 1.0247972011566162,\n",
       " (286, 254): 1.0247889757156372,\n",
       " (328, 328): 1.0247671604156494,\n",
       " (294, 293): 1.0247372388839722,\n",
       " (333, 333): 1.0247352123260498,\n",
       " (288, 288): 1.0247248411178589,\n",
       " (331, 330): 1.0247238874435425,\n",
       " (329, 329): 1.0247185230255127,\n",
       " (300, 299): 1.0246731042861938,\n",
       " (180, 179): 1.0246351957321167,\n",
       " (295, 294): 1.024608850479126,\n",
       " (297, 296): 1.0246071815490723,\n",
       " (298, 297): 1.024580955505371,\n",
       " (299, 298): 1.024580717086792,\n",
       " (330, 330): 1.0245791673660278,\n",
       " (301, 300): 1.0245765447616577,\n",
       " (332, 331): 1.0245580673217773,\n",
       " (331, 331): 1.0245146751403809,\n",
       " (332, 332): 1.0244934558868408,\n",
       " (193, 193): 1.0244672298431396,\n",
       " (296, 295): 1.0244463682174683,\n",
       " (334, 333): 1.0244340896606445,\n",
       " (333, 332): 1.0243980884552002,\n",
       " (285, 253): 1.0243699550628662,\n",
       " (323, 322): 1.0243693590164185,\n",
       " (335, 334): 1.0242271423339844,\n",
       " (324, 323): 1.0241533517837524,\n",
       " (306, 290): 1.024117350578308,\n",
       " (324, 324): 1.0240899324417114,\n",
       " (309, 293): 1.0240763425827026,\n",
       " (333, 317): 1.0240737199783325,\n",
       " (261, 260): 1.0239753723144531,\n",
       " (325, 324): 1.0239475965499878,\n",
       " (207, 207): 1.0239344835281372,\n",
       " (300, 300): 1.0239332914352417,\n",
       " (299, 299): 1.0238629579544067,\n",
       " (323, 323): 1.0237523317337036,\n",
       " (216, 215): 1.0237042903900146,\n",
       " (291, 291): 1.0236700773239136,\n",
       " (322, 322): 1.0236512422561646,\n",
       " (291, 290): 1.0235819816589355,\n",
       " (179, 178): 1.0235025882720947,\n",
       " (225, 224): 1.023496389389038,\n",
       " (259, 259): 1.0234830379486084,\n",
       " (241, 225): 1.0234706401824951,\n",
       " (181, 180): 1.0234379768371582,\n",
       " (296, 296): 1.0234320163726807,\n",
       " (297, 297): 1.0234248638153076,\n",
       " (298, 298): 1.0234203338623047,\n",
       " (292, 292): 1.0234071016311646,\n",
       " (293, 293): 1.0234006643295288,\n",
       " (294, 294): 1.0233573913574219,\n",
       " (289, 257): 1.0233548879623413,\n",
       " (287, 286): 1.0233300924301147,\n",
       " (285, 269): 1.0233111381530762,\n",
       " (215, 214): 1.0232207775115967,\n",
       " (284, 252): 1.0232120752334595,\n",
       " (310, 294): 1.0232032537460327,\n",
       " (223, 222): 1.0231850147247314,\n",
       " (214, 213): 1.02316415309906,\n",
       " (295, 295): 1.0231460332870483,\n",
       " (284, 268): 1.023117184638977,\n",
       " (213, 212): 1.0230834484100342,\n",
       " (182, 181): 1.0228825807571411,\n",
       " (258, 258): 1.0228664875030518,\n",
       " (286, 285): 1.0228614807128906,\n",
       " (311, 295): 1.0228281021118164,\n",
       " (262, 261): 1.022809386253357,\n",
       " (256, 224): 1.0228067636489868,\n",
       " (192, 192): 1.0226739645004272,\n",
       " (283, 267): 1.0226483345031738,\n",
       " (212, 211): 1.0226303339004517,\n",
       " (260, 260): 1.0225909948349,\n",
       " (221, 221): 1.0225595235824585,\n",
       " (259, 243): 1.0224417448043823,\n",
       " (312, 296): 1.022294044494629,\n",
       " (222, 222): 1.022273302078247,\n",
       " (263, 262): 1.0221894979476929,\n",
       " (260, 244): 1.0221567153930664,\n",
       " (179, 163): 1.0220794677734375,\n",
       " (332, 316): 1.021958351135254,\n",
       " (244, 243): 1.021902322769165,\n",
       " (304, 272): 1.0218636989593506,\n",
       " (313, 297): 1.0218056440353394,\n",
       " (283, 251): 1.0217933654785156,\n",
       " (265, 264): 1.021743893623352,\n",
       " (264, 263): 1.0217413902282715,\n",
       " (314, 298): 1.021633505821228,\n",
       " (183, 182): 1.021610975265503,\n",
       " (255, 254): 1.0215461254119873,\n",
       " (321, 305): 1.0214569568634033,\n",
       " (220, 220): 1.021449327468872,\n",
       " (261, 261): 1.021416187286377,\n",
       " (282, 266): 1.0213823318481445,\n",
       " (254, 253): 1.021360158920288,\n",
       " (315, 299): 1.0213258266448975,\n",
       " (243, 242): 1.0212386846542358,\n",
       " (217, 217): 1.0212193727493286,\n",
       " (191, 143): 1.0211738348007202,\n",
       " (245, 244): 1.0211548805236816,\n",
       " (219, 219): 1.0211011171340942,\n",
       " (316, 300): 1.0210440158843994,\n",
       " (266, 265): 1.0210245847702026,\n",
       " (270, 269): 1.0210072994232178,\n",
       " (282, 250): 1.020993947982788,\n",
       " (331, 315): 1.0209025144577026,\n",
       " (281, 249): 1.020890474319458,\n",
       " (253, 252): 1.0208748579025269,\n",
       " (273, 273): 1.0208429098129272,\n",
       " (262, 262): 1.0208386182785034,\n",
       " (261, 245): 1.0207643508911133,\n",
       " (256, 208): 1.0207597017288208,\n",
       " (184, 183): 1.0207053422927856,\n",
       " (180, 164): 1.0206782817840576,\n",
       " (317, 301): 1.0206223726272583,\n",
       " (267, 266): 1.0206007957458496,\n",
       " (182, 166): 1.0205703973770142,\n",
       " (290, 290): 1.0205196142196655,\n",
       " (218, 218): 1.0204663276672363,\n",
       " (179, 179): 1.0204274654388428,\n",
       " (216, 216): 1.0204249620437622,\n",
       " (268, 267): 1.0203888416290283,\n",
       " (246, 245): 1.0203436613082886,\n",
       " (190, 189): 1.0203015804290771,\n",
       " (263, 263): 1.0202655792236328,\n",
       " (269, 268): 1.0202393531799316,\n",
       " (181, 165): 1.0202089548110962,\n",
       " (318, 302): 1.0201886892318726,\n",
       " (264, 264): 1.0201610326766968,\n",
       " (286, 238): 1.0201443433761597,\n",
       " (252, 251): 1.0200666189193726,\n",
       " (249, 248): 1.0200616121292114,\n",
       " (281, 265): 1.0200334787368774,\n",
       " (209, 193): 1.0199973583221436,\n",
       " (284, 283): 1.019982099533081,\n",
       " (211, 210): 1.0199300050735474,\n",
       " (286, 286): 1.0199029445648193,\n",
       " (159, 159): 1.0198696851730347,\n",
       " (239, 239): 1.0198417901992798,\n",
       " (262, 246): 1.0198262929916382,\n",
       " (183, 167): 1.0198228359222412,\n",
       " (180, 180): 1.0198227167129517,\n",
       " (270, 270): 1.019808292388916,\n",
       " (185, 184): 1.0197901725769043,\n",
       " (247, 246): 1.0197887420654297,\n",
       " (188, 187): 1.019704818725586,\n",
       " (328, 312): 1.019673466682434,\n",
       " (265, 265): 1.0196434259414673,\n",
       " (330, 314): 1.0196324586868286,\n",
       " (285, 284): 1.0195969343185425,\n",
       " (283, 282): 1.0195642709732056,\n",
       " (189, 188): 1.019546627998352,\n",
       " (181, 181): 1.0195331573486328,\n",
       " (329, 313): 1.01945960521698,\n",
       " (248, 247): 1.0194507837295532,\n",
       " (259, 227): 1.019400954246521,\n",
       " (191, 190): 1.0193828344345093,\n",
       " (250, 249): 1.0193735361099243,\n",
       " (327, 311): 1.0193345546722412,\n",
       " (287, 223): 1.0193099975585938,\n",
       " (285, 285): 1.0193021297454834,\n",
       " (281, 280): 1.0193010568618774,\n",
       " (251, 250): 1.0192629098892212,\n",
       " (276, 275): 1.019223928451538,\n",
       " (282, 281): 1.0191891193389893,\n",
       " (266, 266): 1.0190911293029785,\n",
       " (177, 145): 1.0190565586090088,\n",
       " (257, 209): 1.0190352201461792,\n",
       " (258, 242): 1.0190234184265137,\n",
       " (263, 247): 1.0189980268478394,\n",
       " (187, 186): 1.0189775228500366,\n",
       " (186, 185): 1.01897394657135,\n",
       " (280, 279): 1.0189461708068848,\n",
       " (267, 267): 1.0188864469528198,\n",
       " (277, 276): 1.0188219547271729,\n",
       " (280, 264): 1.0187866687774658,\n",
       " (325, 309): 1.018678903579712,\n",
       " (280, 248): 1.018642544746399,\n",
       " (269, 269): 1.0185940265655518,\n",
       " (326, 310): 1.01852548122406,\n",
       " (260, 228): 1.0185080766677856,\n",
       " (230, 229): 1.0185052156448364,\n",
       " (184, 168): 1.018501877784729,\n",
       " (264, 248): 1.018471360206604,\n",
       " (243, 243): 1.0184320211410522,\n",
       " (268, 268): 1.0183894634246826,\n",
       " (279, 278): 1.0183786153793335,\n",
       " (324, 292): 1.018357276916504,\n",
       " (244, 244): 1.0182639360427856,\n",
       " (278, 277): 1.0182520151138306,\n",
       " (261, 229): 1.0182243585586548,\n",
       " (186, 170): 1.0182075500488281,\n",
       " (252, 252): 1.0181663036346436,\n",
       " (182, 182): 1.018160343170166,\n",
       " (185, 169): 1.0181094408035278,\n",
       " (285, 237): 1.0181015729904175,\n",
       " (215, 215): 1.0179972648620605,\n",
       " (325, 293): 1.0178664922714233,\n",
       " (253, 253): 1.0177738666534424,\n",
       " (196, 195): 1.017742395401001,\n",
       " (265, 249): 1.0176950693130493,\n",
       " (229, 228): 1.017653465270996,\n",
       " (279, 247): 1.0175918340682983,\n",
       " (245, 245): 1.0174425840377808,\n",
       " (248, 248): 1.0174058675765991,\n",
       " (262, 230): 1.0173969268798828,\n",
       " (228, 227): 1.0173782110214233,\n",
       " (214, 214): 1.0173178911209106,\n",
       " (251, 251): 1.0172622203826904,\n",
       " (183, 183): 1.017258882522583,\n",
       " (324, 308): 1.0172191858291626,\n",
       " (239, 238): 1.0172028541564941,\n",
       " (231, 230): 1.0172017812728882,\n",
       " (254, 254): 1.0171831846237183,\n",
       " (263, 231): 1.0171642303466797,\n",
       " (323, 291): 1.017138123512268,\n",
       " (238, 237): 1.01712167263031,\n",
       " (246, 246): 1.0170611143112183,\n",
       " (266, 250): 1.017046570777893,\n",
       " (326, 294): 1.0170072317123413,\n",
       " (327, 295): 1.0169893503189087,\n",
       " (187, 171): 1.0169427394866943,\n",
       " (232, 231): 1.01693594455719,\n",
       " (279, 263): 1.0169289112091064,\n",
       " (233, 232): 1.0167789459228516,\n",
       " (264, 232): 1.0167337656021118,\n",
       " (249, 249): 1.0167100429534912,\n",
       " (247, 247): 1.0166887044906616,\n",
       " (250, 250): 1.016571283340454,\n",
       " (187, 187): 1.0165423154830933,\n",
       " (330, 298): 1.0165295600891113,\n",
       " (222, 206): 1.0164575576782227,\n",
       " (303, 287): 1.016302466392517,\n",
       " (284, 236): 1.0162711143493652,\n",
       " (278, 246): 1.0162557363510132,\n",
       " (188, 172): 1.0162509679794312,\n",
       " (145, 144): 1.0162326097488403,\n",
       " (189, 173): 1.0161948204040527,\n",
       " (265, 233): 1.0161858797073364,\n",
       " (331, 299): 1.016176700592041,\n",
       " (237, 236): 1.0161113739013672,\n",
       " (213, 213): 1.0161106586456299,\n",
       " (266, 234): 1.0160877704620361,\n",
       " (224, 224): 1.0160785913467407,\n",
       " (184, 184): 1.0160670280456543,\n",
       " (229, 229): 1.016055703163147,\n",
       " (197, 196): 1.0159717798233032,\n",
       " (332, 300): 1.0159536600112915,\n",
       " (270, 238): 1.015902042388916,\n",
       " (235, 234): 1.015873908996582,\n",
       " (206, 205): 1.0158586502075195,\n",
       " (178, 178): 1.0158493518829346,\n",
       " (223, 175): 1.0158053636550903,\n",
       " (188, 188): 1.015796422958374,\n",
       " (189, 189): 1.0157802104949951,\n",
       " (283, 235): 1.0157654285430908,\n",
       " (234, 233): 1.0157601833343506,\n",
       " (328, 296): 1.0157495737075806,\n",
       " (221, 205): 1.0156970024108887,\n",
       " (267, 251): 1.0156793594360352,\n",
       " (283, 283): 1.0156662464141846,\n",
       " (236, 235): 1.0156461000442505,\n",
       " (329, 297): 1.0156307220458984,\n",
       " (242, 242): 1.0155525207519531,\n",
       " (278, 262): 1.015548586845398,\n",
       " (195, 194): 1.0154491662979126,\n",
       " (186, 186): 1.0154365301132202,\n",
       " (220, 204): 1.0154045820236206,\n",
       " (267, 235): 1.0153517723083496,\n",
       " (185, 185): 1.0153234004974365,\n",
       " (282, 234): 1.0152256488800049,\n",
       " (323, 307): 1.0151780843734741,\n",
       " (281, 233): 1.0151538848876953,\n",
       " (269, 237): 1.0150723457336426,\n",
       " (333, 301): 1.015039086341858,\n",
       " (228, 228): 1.0149579048156738,\n",
       " (271, 255): 1.0149555206298828,\n",
       " (284, 284): 1.0149317979812622,\n",
       " (190, 174): 1.0149309635162354,\n",
       " (212, 212): 1.0148645639419556,\n",
       " (268, 236): 1.014799952507019,\n",
       " (227, 226): 1.0147308111190796,\n",
       " (207, 206): 1.014703631401062,\n",
       " (198, 197): 1.014654517173767,\n",
       " (271, 270): 1.0145632028579712,\n",
       " (268, 252): 1.0145328044891357,\n",
       " (230, 230): 1.014450192451477,\n",
       " (277, 245): 1.0143210887908936,\n",
       " (270, 254): 1.014286756515503,\n",
       " (227, 227): 1.014281153678894,\n",
       " (178, 162): 1.0142340660095215,\n",
       " (282, 282): 1.0142287015914917,\n",
       " (199, 198): 1.0141929388046265,\n",
       " (240, 224): 1.0141891241073608,\n",
       " (175, 175): 1.0140538215637207,\n",
       " (277, 261): 1.0139869451522827,\n",
       " (269, 253): 1.0139402151107788,\n",
       " (231, 231): 1.0138877630233765,\n",
       " (225, 209): 1.0138126611709595,\n",
       " (258, 226): 1.0137182474136353,\n",
       " (232, 232): 1.0135945081710815,\n",
       " (281, 281): 1.0135142803192139,\n",
       " (205, 204): 1.0134508609771729,\n",
       " (219, 203): 1.0133835077285767,\n",
       " (190, 190): 1.0133613348007202,\n",
       " (200, 199): 1.013288974761963,\n",
       " (279, 279): 1.013095736503601,\n",
       " (215, 199): 1.013094425201416,\n",
       " (286, 222): 1.013075351715088,\n",
       " (334, 302): 1.0130550861358643,\n",
       " (280, 232): 1.0130103826522827,\n",
       " (202, 201): 1.012741208076477,\n",
       " (234, 234): 1.0126173496246338,\n",
       " (203, 202): 1.0126166343688965,\n",
       " (233, 233): 1.012585997581482,\n",
       " (280, 280): 1.012561559677124,\n",
       " (201, 200): 1.0125489234924316,\n",
       " (216, 200): 1.0125455856323242,\n",
       " (235, 235): 1.0124962329864502,\n",
       " (276, 260): 1.0124709606170654,\n",
       " (218, 202): 1.012377381324768,\n",
       " (158, 157): 1.0123627185821533,\n",
       " (224, 208): 1.0123612880706787,\n",
       " (275, 274): 1.0123363733291626,\n",
       " (255, 223): 1.0122133493423462,\n",
       " (204, 203): 1.0121880769729614,\n",
       " (211, 211): 1.0121814012527466,\n",
       " (304, 256): 1.0121735334396362,\n",
       " (236, 236): 1.0121291875839233,\n",
       " (145, 145): 1.012115478515625,\n",
       " (276, 244): 1.012101173400879,\n",
       " (217, 201): 1.0120863914489746,\n",
       " (195, 195): 1.0118286609649658,\n",
       " (271, 239): 1.0116183757781982,\n",
       " (279, 231): 1.0115641355514526,\n",
       " (237, 237): 1.0115560293197632,\n",
       " (238, 238): 1.0114578008651733,\n",
       " (193, 177): 1.0113468170166016,\n",
       " (278, 278): 1.011200189590454,\n",
       " (244, 228): 1.011123538017273,\n",
       " (285, 221): 1.011062502861023,\n",
       " (159, 158): 1.0109726190567017,\n",
       " (159, 143): 1.0109623670578003,\n",
       " (284, 220): 1.01087486743927,\n",
       " (208, 208): 1.0108107328414917,\n",
       " (214, 198): 1.0108081102371216,\n",
       " (129, 129): 1.0106956958770752,\n",
       " (307, 275): 1.010694980621338,\n",
       " (226, 226): 1.0106937885284424,\n",
       " (243, 227): 1.0106407403945923,\n",
       " (241, 209): 1.0106306076049805,\n",
       " (196, 196): 1.0105221271514893,\n",
       " (247, 231): 1.010387659072876,\n",
       " (248, 232): 1.010373830795288,\n",
       " (245, 229): 1.0103296041488647,\n",
       " (277, 277): 1.0103284120559692,\n",
       " (259, 211): 1.010303258895874,\n",
       " (276, 276): 1.0100942850112915,\n",
       " (282, 218): 1.0100122690200806,\n",
       " (278, 230): 1.009897232055664,\n",
       " (283, 219): 1.0098520517349243,\n",
       " (246, 230): 1.009841799736023,\n",
       " (197, 197): 1.0096567869186401,\n",
       " (249, 233): 1.009581208229065,\n",
       " (320, 288): 1.0095280408859253,\n",
       " (161, 160): 1.0095086097717285,\n",
       " (275, 243): 1.0093694925308228,\n",
       " (127, 127): 1.009333848953247,\n",
       " (161, 161): 1.009307622909546,\n",
       " (291, 275): 1.009296178817749,\n",
       " (275, 259): 1.009225606918335,\n",
       " (319, 287): 1.009185552597046,\n",
       " (308, 276): 1.0091584920883179,\n",
       " (213, 197): 1.009145736694336,\n",
       " (255, 207): 1.0091381072998047,\n",
       " (198, 198): 1.0090916156768799,\n",
       " (250, 234): 1.0090813636779785,\n",
       " (281, 217): 1.0090303421020508,\n",
       " (254, 238): 1.0087460279464722,\n",
       " (277, 229): 1.008621335029602,\n",
       " (287, 207): 1.0085833072662354,\n",
       " (275, 275): 1.0085713863372803,\n",
       " (157, 156): 1.0085655450820923,\n",
       " (225, 193): 1.008499026298523,\n",
       " (175, 174): 1.008436679840088,\n",
       " (252, 236): 1.0083914995193481,\n",
       " (251, 235): 1.0083574056625366,\n",
       " (305, 257): 1.008043646812439,\n",
       " (199, 199): 1.0080169439315796,\n",
       " (206, 206): 1.0077831745147705,\n",
       " (253, 237): 1.0077056884765625,\n",
       " (129, 128): 1.0075879096984863,\n",
       " (309, 277): 1.0075498819351196,\n",
       " (292, 276): 1.0074880123138428,\n",
       " (175, 159): 1.0074325799942017,\n",
       " (201, 201): 1.0074139833450317,\n",
       " (260, 212): 1.0072882175445557,\n",
       " (290, 274): 1.007262110710144,\n",
       " (205, 205): 1.0072468519210815,\n",
       " (320, 304): 1.0070959329605103,\n",
       " (276, 228): 1.0069998502731323,\n",
       " (202, 202): 1.0069397687911987,\n",
       " (200, 200): 1.0069060325622559,\n",
       " (293, 277): 1.0068858861923218,\n",
       " (280, 216): 1.0066778659820557,\n",
       " (239, 207): 1.0065577030181885,\n",
       " (204, 204): 1.0062975883483887,\n",
       " (212, 196): 1.0061272382736206,\n",
       " (279, 215): 1.0061225891113281,\n",
       " (203, 203): 1.0061004161834717,\n",
       " (306, 274): 1.0060560703277588,\n",
       " (156, 155): 1.0059773921966553,\n",
       " (207, 191): 1.0058515071868896,\n",
       " (207, 175): 1.0058122873306274,\n",
       " (294, 278): 1.0057942867279053,\n",
       " (310, 278): 1.0057413578033447,\n",
       " (148, 147): 1.0057228803634644,\n",
       " (286, 206): 1.005520224571228,\n",
       " (288, 256): 1.0053731203079224,\n",
       " (273, 241): 1.0053603649139404,\n",
       " (304, 240): 1.005312442779541,\n",
       " (275, 227): 1.0052800178527832,\n",
       " (153, 152): 1.0051242113113403,\n",
       " (278, 214): 1.0051195621490479,\n",
       " (194, 194): 1.005013346672058,\n",
       " (155, 154): 1.0048376321792603,\n",
       " (150, 149): 1.0047367811203003,\n",
       " (242, 226): 1.0047293901443481,\n",
       " (311, 279): 1.0047272443771362,\n",
       " (284, 204): 1.004576563835144,\n",
       " (295, 279): 1.0043611526489258,\n",
       " (273, 257): 1.0043209791183472,\n",
       " (274, 258): 1.004300594329834,\n",
       " (149, 148): 1.0042569637298584,\n",
       " (152, 151): 1.004183053970337,\n",
       " (239, 223): 1.0041823387145996,\n",
       " (151, 150): 1.0041344165802002,\n",
       " (113, 113): 1.0040943622589111,\n",
       " (285, 205): 1.0040371417999268,\n",
       " (154, 153): 1.0040273666381836,\n",
       " (127, 126): 1.004011869430542,\n",
       " (322, 290): 1.0040042400360107,\n",
       " (176, 144): 1.0037415027618408,\n",
       " (282, 202): 1.0037294626235962,\n",
       " (274, 242): 1.0035628080368042,\n",
       " (288, 240): 1.003488302230835,\n",
       " (227, 211): 1.0034868717193604,\n",
       " (307, 259): 1.0034493207931519,\n",
       " (283, 203): 1.0033564567565918,\n",
       " (277, 213): 1.00324547290802,\n",
       " (272, 272): 1.003121256828308,\n",
       " (175, 143): 1.0030508041381836,\n",
       " (321, 289): 1.002967357635498,\n",
       " (322, 306): 1.0029176473617554,\n",
       " (223, 159): 1.0029151439666748,\n",
       " (228, 212): 1.0028377771377563,\n",
       " (280, 200): 1.0028351545333862,\n",
       " (289, 241): 1.0028053522109985,\n",
       " (296, 280): 1.0027706623077393,\n",
       " (127, 111): 1.0027296543121338,\n",
       " (210, 210): 1.0027161836624146,\n",
       " (312, 280): 1.0026434659957886,\n",
       " (261, 213): 1.002510666847229,\n",
       " (281, 201): 1.00242280960083,\n",
       " (111, 111): 1.0023143291473389,\n",
       " (240, 208): 1.002191424369812,\n",
       " (143, 143): 1.002044439315796,\n",
       " (276, 212): 1.0019341707229614,\n",
       " (180, 148): 1.0019159317016602,\n",
       " (279, 199): 1.0018894672393799,\n",
       " (313, 281): 1.001823902130127,\n",
       " (274, 274): 1.0015677213668823,\n",
       " (308, 260): 1.0014845132827759,\n",
       " (95, 95): 1.0014305114746094,\n",
       " (211, 195): 1.0012290477752686,\n",
       " (314, 282): 1.001193642616272,\n",
       " (309, 261): 1.0011842250823975,\n",
       " (174, 173): 1.0010287761688232,\n",
       " (297, 281): 1.0009777545928955,\n",
       " (229, 213): 1.0009450912475586,\n",
       " (183, 151): 1.0009397268295288,\n",
       " (226, 210): 1.0008631944656372,\n",
       " (224, 192): 1.0008612871170044,\n",
       " (193, 161): 1.0007034540176392,\n",
       " (182, 150): 1.0005323886871338,\n",
       " (181, 149): 1.0005179643630981,\n",
       " (319, 271): 1.000495195388794,\n",
       " (128, 128): 1.0004328489303589,\n",
       " (298, 282): 1.0003947019577026,\n",
       " (144, 144): 1.0003807544708252,\n",
       " (179, 147): 1.000353217124939,\n",
       " (230, 214): 1.0002223253250122,\n",
       " (278, 198): 1.0001147985458374,\n",
       " (310, 262): 0.9999901056289673,\n",
       " (262, 214): 0.9999361038208008,\n",
       " (315, 283): 0.9999046921730042,\n",
       " (129, 113): 0.9998109936714172,\n",
       " (299, 283): 0.9997091889381409,\n",
       " (274, 226): 0.9996867179870605,\n",
       " (275, 211): 0.999465823173523,\n",
       " (277, 197): 0.9992977976799011,\n",
       " (127, 95): 0.9990429282188416,\n",
       " (273, 225): 0.9990247488021851,\n",
       " (184, 152): 0.9989910125732422,\n",
       " (231, 215): 0.9988876581192017,\n",
       " (243, 211): 0.9988479614257812,\n",
       " (320, 272): 0.9988062381744385,\n",
       " (316, 284): 0.9987842440605164,\n",
       " (191, 127): 0.9987702369689941,\n",
       " (335, 271): 0.9984740614891052,\n",
       " (147, 146): 0.9983400106430054,\n",
       " (263, 215): 0.998320460319519,\n",
       " (164, 163): 0.9981650710105896,\n",
       " (311, 263): 0.9981203079223633,\n",
       " (143, 142): 0.9978901147842407,\n",
       " (185, 153): 0.9977297782897949,\n",
       " (300, 284): 0.9977074861526489,\n",
       " (132, 131): 0.9976803064346313,\n",
       " (306, 258): 0.997557520866394,\n",
       " (186, 154): 0.9974732995033264,\n",
       " (307, 243): 0.997469961643219,\n",
       " (276, 196): 0.9971982836723328,\n",
       " (126, 125): 0.9971960783004761,\n",
       " (161, 145): 0.9971778988838196,\n",
       " (232, 216): 0.9971126317977905,\n",
       " (312, 264): 0.9971100687980652,\n",
       " (303, 255): 0.997002363204956,\n",
       " (271, 223): 0.9969618916511536,\n",
       " (165, 164): 0.9969521760940552,\n",
       " (305, 241): 0.9969268441200256,\n",
       " (159, 127): 0.9968571066856384,\n",
       " (227, 195): 0.9967688918113708,\n",
       " (173, 172): 0.9967116117477417,\n",
       " (301, 285): 0.9967067241668701,\n",
       " (244, 212): 0.9966266751289368,\n",
       " (187, 155): 0.996602475643158,\n",
       " (271, 207): 0.996586263179779,\n",
       " (302, 286): 0.996563732624054,\n",
       " (235, 203): 0.9964677691459656,\n",
       " (234, 202): 0.996299147605896,\n",
       " (133, 132): 0.996232271194458,\n",
       " (222, 174): 0.9962019920349121,\n",
       " (308, 244): 0.9961079955101013,\n",
       " (233, 217): 0.9960557818412781,\n",
       " (79, 79): 0.9960491061210632,\n",
       " (223, 143): 0.9960381984710693,\n",
       " (313, 265): 0.9958195686340332,\n",
       " (291, 243): 0.9956812262535095,\n",
       " (236, 204): 0.9956790208816528,\n",
       " (245, 213): 0.9956750273704529,\n",
       " (190, 158): 0.9956017136573792,\n",
       " (134, 133): 0.995598554611206,\n",
       " (166, 165): 0.9955860376358032,\n",
       " (188, 156): 0.9955730438232422,\n",
       " (234, 218): 0.9955633878707886,\n",
       " (309, 245): 0.9954782724380493,\n",
       " (145, 129): 0.9954202175140381,\n",
       " (172, 171): 0.9953640103340149,\n",
       " (167, 166): 0.9953080415725708,\n",
       " (189, 157): 0.9952850341796875,\n",
       " (264, 216): 0.9952700734138489,\n",
       " (265, 217): 0.9952545762062073,\n",
       " (175, 127): 0.995250403881073,\n",
       " (157, 157): 0.9952414631843567,\n",
       " (317, 285): 0.9952386021614075,\n",
       " (228, 196): 0.995214581489563,\n",
       " (273, 209): 0.9951945543289185,\n",
       " (233, 201): 0.9951305985450745,\n",
       " (168, 167): 0.9951296448707581,\n",
       " (209, 177): 0.9951047897338867,\n",
       " (293, 245): 0.9949489235877991,\n",
       " (237, 205): 0.994943380355835,\n",
       " (169, 168): 0.99489426612854,\n",
       " (292, 244): 0.9948698282241821,\n",
       " (306, 242): 0.9947710633277893,\n",
       " (314, 266): 0.9947267770767212,\n",
       " (235, 219): 0.994694709777832,\n",
       " (156, 156): 0.9946717023849487,\n",
       " (266, 218): 0.9946375489234924,\n",
       " (239, 191): 0.9945895075798035,\n",
       " (177, 129): 0.9945639967918396,\n",
       " (310, 246): 0.9945002198219299,\n",
       " (258, 210): 0.9944612979888916,\n",
       " (229, 197): 0.9942991733551025,\n",
       " (236, 220): 0.9942541718482971,\n",
       " (275, 195): 0.9942368268966675,\n",
       " (294, 262): 0.9941650032997131,\n",
       " (171, 170): 0.9941234588623047,\n",
       " (230, 198): 0.994061291217804,\n",
       " (294, 246): 0.9939623475074768,\n",
       " (311, 247): 0.9939306378364563,\n",
       " (142, 141): 0.9938850998878479,\n",
       " (246, 214): 0.9938673377037048,\n",
       " (170, 169): 0.9938324689865112,\n",
       " (286, 190): 0.9937372207641602,\n",
       " (155, 155): 0.9937154054641724,\n",
       " (141, 140): 0.9936733841896057,\n",
       " (304, 224): 0.9935804605484009,\n",
       " (152, 152): 0.9935685396194458,\n",
       " (295, 263): 0.9935011863708496,\n",
       " (293, 261): 0.9934871792793274,\n",
       " (267, 219): 0.9934441447257996,\n",
       " (231, 199): 0.9933798909187317,\n",
       " (297, 249): 0.9933640956878662,\n",
       " (295, 247): 0.9933361411094666,\n",
       " (237, 221): 0.9933348894119263,\n",
       " (135, 134): 0.9933276176452637,\n",
       " (140, 139): 0.993259847164154,\n",
       " (232, 200): 0.9932444095611572,\n",
       " (154, 154): 0.9931095838546753,\n",
       " (287, 191): 0.9930592179298401,\n",
       " (113, 112): 0.9930533766746521,\n",
       " (315, 267): 0.9930300116539001,\n",
       " (272, 256): 0.9928725957870483,\n",
       " (286, 174): 0.9928635954856873,\n",
       " (247, 215): 0.9928591847419739,\n",
       " (221, 173): 0.992850124835968,\n",
       " (316, 268): 0.9928350448608398,\n",
       " (318, 286): 0.9928168058395386,\n",
       " (160, 160): 0.9927546381950378,\n",
       " (299, 251): 0.9927212595939636,\n",
       " (238, 222): 0.9927191138267517,\n",
       " (153, 153): 0.9926434755325317,\n",
       " (296, 264): 0.9926201105117798,\n",
       " (312, 248): 0.9925979375839233,\n",
       " (298, 250): 0.9925342798233032,\n",
       " (296, 248): 0.9925060868263245,\n",
       " (222, 190): 0.9924365878105164,\n",
       " (300, 252): 0.9923809766769409,\n",
       " (292, 260): 0.9923748970031738,\n",
       " (151, 151): 0.9922761917114258,\n",
       " (297, 265): 0.9922754764556885,\n",
       " (285, 173): 0.9922478795051575,\n",
       " (111, 95): 0.9921251535415649,\n",
       " (317, 269): 0.9920206069946289,\n",
       " (321, 273): 0.991975724697113,\n",
       " (150, 150): 0.9919571280479431,\n",
       " (274, 210): 0.9919382929801941,\n",
       " (302, 270): 0.9918964505195618,\n",
       " (137, 136): 0.9918791651725769,\n",
       " (298, 266): 0.9918299317359924,\n",
       " (136, 135): 0.9917876720428467,\n",
       " (284, 172): 0.9917784929275513,\n",
       " (149, 149): 0.9917502403259277,\n",
       " (225, 177): 0.9917271733283997,\n",
       " (285, 189): 0.9916872382164001,\n",
       " (301, 269): 0.9916653037071228,\n",
       " (192, 160): 0.9916582107543945,\n",
       " (163, 162): 0.9916151165962219,\n",
       " (220, 172): 0.9915880560874939,\n",
       " (63, 63): 0.9915810823440552,\n",
       " (131, 130): 0.9915071725845337,\n",
       " (111, 110): 0.9915064573287964,\n",
       " (313, 249): 0.9914929866790771,\n",
       " (217, 169): 0.9914546608924866,\n",
       " (335, 287): 0.9914287328720093,\n",
       " (138, 137): 0.9913874268531799,\n",
       " (139, 138): 0.9913629293441772,\n",
       " (314, 250): 0.9913554191589355,\n",
       " (283, 171): 0.9913493394851685,\n",
       " (291, 259): 0.9913457632064819,\n",
       " (300, 268): 0.9912983179092407,\n",
       " (125, 124): 0.9911929368972778,\n",
       " (299, 267): 0.9911872148513794,\n",
       " (192, 176): 0.9910363554954529,\n",
       " (208, 192): 0.9910103678703308,\n",
       " (116, 115): 0.9909986853599548,\n",
       " (158, 158): 0.9909905195236206,\n",
       " (218, 170): 0.9909446835517883,\n",
       " (284, 188): 0.9909407496452332,\n",
       " (219, 171): 0.9909082651138306,\n",
       " (268, 220): 0.9908015727996826,\n",
       " (272, 240): 0.9907919764518738,\n",
       " (95, 79): 0.9907028079032898,\n",
       " (301, 253): 0.9906520247459412,\n",
       " (315, 251): 0.9905831217765808,\n",
       " (290, 242): 0.9905703067779541,\n",
       " (305, 225): 0.9904704689979553,\n",
       " (248, 216): 0.990433931350708,\n",
       " (304, 208): 0.9904264211654663,\n",
       " (221, 189): 0.9903870224952698,\n",
       " (283, 187): 0.9903649687767029,\n",
       " (216, 168): 0.9903193116188049,\n",
       " (147, 147): 0.9901741743087769,\n",
       " (117, 116): 0.990152895450592,\n",
       " (303, 239): 0.9900960922241211,\n",
       " (222, 142): 0.9900473356246948,\n",
       " (148, 148): 0.9900465607643127,\n",
       " (323, 275): 0.9900307059288025,\n",
       " (282, 170): 0.9899954795837402,\n",
       " (281, 169): 0.9898666739463806,\n",
       " (238, 206): 0.9898459315299988,\n",
       " (281, 185): 0.9897372126579285,\n",
       " (322, 274): 0.9896889328956604,\n",
       " (282, 186): 0.9896814823150635,\n",
       " (286, 158): 0.9895949363708496,\n",
       " (95, 94): 0.9895620346069336,\n",
       " (223, 127): 0.9894176125526428,\n",
       " (307, 227): 0.9893988370895386,\n",
       " (221, 141): 0.9893213510513306,\n",
       " (207, 159): 0.9892906546592712,\n",
       " (124, 123): 0.9892346262931824,\n",
       " (118, 117): 0.9892181158065796,\n",
       " (302, 254): 0.9891342520713806,\n",
       " (143, 127): 0.9890495538711548,\n",
       " (241, 193): 0.9890413284301758,\n",
       " (47, 47): 0.9888493418693542,\n",
       " (316, 252): 0.988838791847229,\n",
       " (220, 188): 0.9888373613357544,\n",
       " (324, 276): 0.9888125658035278,\n",
       " (318, 270): 0.9887640476226807,\n",
       " (215, 167): 0.9886966347694397,\n",
       " (257, 193): 0.9886837005615234,\n",
       " (308, 228): 0.9886749982833862,\n",
       " (285, 157): 0.9886484146118164,\n",
       " (288, 224): 0.9886288642883301,\n",
       " (290, 258): 0.9885149002075195,\n",
       " (280, 184): 0.988493800163269,\n",
       " (210, 194): 0.9884894490242004,\n",
       " (280, 168): 0.9884472489356995,\n",
       " (119, 118): 0.9884197115898132,\n",
       " (284, 156): 0.9883629083633423,\n",
       " (178, 146): 0.988345205783844,\n",
       " (272, 224): 0.9882904291152954,\n",
       " (220, 140): 0.9881877303123474,\n",
       " (226, 194): 0.9881856441497803,\n",
       " (309, 229): 0.9880663752555847,\n",
       " (249, 217): 0.9880632758140564,\n",
       " (250, 218): 0.9880608320236206,\n",
       " (252, 220): 0.9879928231239319,\n",
       " (317, 253): 0.9879789352416992,\n",
       " (283, 155): 0.9879021644592285,\n",
       " (325, 277): 0.9878455400466919,\n",
       " (251, 219): 0.9877969026565552,\n",
       " (123, 122): 0.987773597240448,\n",
       " (113, 97): 0.9877054691314697,\n",
       " (97, 97): 0.9876706004142761,\n",
       " (214, 166): 0.9876638650894165,\n",
       " (120, 119): 0.9874751567840576,\n",
       " (306, 226): 0.9873983263969421,\n",
       " (222, 158): 0.9873090982437134,\n",
       " (121, 120): 0.9873017072677612,\n",
       " (310, 230): 0.9872838258743286,\n",
       " (143, 111): 0.9872127771377563,\n",
       " (122, 121): 0.9871678948402405,\n",
       " (286, 142): 0.9871615171432495,\n",
       " (326, 278): 0.9871475100517273,\n",
       " (219, 139): 0.987091064453125,\n",
       " (287, 175): 0.9870837330818176,\n",
       " (269, 221): 0.9870158433914185,\n",
       " (15, 15): 0.9869926571846008,\n",
       " (289, 225): 0.9869544506072998,\n",
       " (219, 187): 0.9869410395622253,\n",
       " (218, 186): 0.9869394302368164,\n",
       " (279, 183): 0.9868749976158142,\n",
       " (164, 164): 0.9868286848068237,\n",
       " (279, 167): 0.9868052005767822,\n",
       " (311, 231): 0.9866774678230286,\n",
       " (213, 165): 0.986650824546814,\n",
       " (163, 163): 0.9865958094596863,\n",
       " (165, 165): 0.9865582585334778,\n",
       " (242, 210): 0.9864721894264221,\n",
       " (166, 166): 0.9864705204963684,\n",
       " (167, 167): 0.9864470362663269,\n",
       " (127, 79): 0.9864445328712463,\n",
       " (285, 141): 0.9864331483840942,\n",
       " (282, 154): 0.9863870739936829,\n",
       " (31, 31): 0.9863867163658142,\n",
       " (133, 133): 0.986375629901886,\n",
       " (284, 140): 0.986230731010437,\n",
       " (312, 232): 0.9862174391746521,\n",
       " (327, 279): 0.9861843585968018,\n",
       " (195, 163): 0.9861765503883362,\n",
       " (318, 254): 0.986153781414032,\n",
       " (281, 153): 0.9860894083976746,\n",
       " (196, 164): 0.9860770106315613,\n",
       " (191, 111): 0.9860445261001587,\n",
       " (63, 47): 0.986011803150177,\n",
       " (197, 165): 0.9859838485717773,\n",
       " (217, 185): 0.9859809279441833,\n",
       " (216, 184): 0.9859708547592163,\n",
       " (168, 168): 0.9859218597412109,\n",
       " (273, 193): 0.9859139919281006,\n",
       " (157, 141): 0.9859025478363037,\n",
       " (274, 194): 0.9858123660087585,\n",
       " (253, 221): 0.9857986569404602,\n",
       " (131, 131): 0.9857766628265381,\n",
       " (255, 191): 0.985775887966156,\n",
       " (171, 171): 0.9857321977615356,\n",
       " (218, 138): 0.985704779624939,\n",
       " (298, 234): 0.9856302738189697,\n",
       " (140, 140): 0.985622227191925,\n",
       " (183, 135): 0.9856032133102417,\n",
       " (278, 166): 0.9855898022651672,\n",
       " (328, 280): 0.9854642152786255,\n",
       " (182, 134): 0.9854307174682617,\n",
       " (327, 263): 0.9854093194007874,\n",
       " (172, 172): 0.9853345155715942,\n",
       " (270, 222): 0.9853072762489319,\n",
       " (155, 139): 0.985292375087738,\n",
       " (313, 233): 0.9852694869041443,\n",
       " (156, 140): 0.9852608442306519,\n",
       " (128, 112): 0.9852325916290283,\n",
       " (325, 261): 0.985211968421936,\n",
       " (200, 168): 0.9852005243301392,\n",
       " (215, 183): 0.9851841926574707,\n",
       " (278, 182): 0.9851824045181274,\n",
       " (333, 269): 0.985173761844635,\n",
       " (254, 222): 0.9851544499397278,\n",
       " (158, 142): 0.9851216077804565,\n",
       " (173, 173): 0.9851094484329224,\n",
       " (323, 259): 0.9850875735282898,\n",
       " (332, 268): 0.9850674867630005,\n",
       " (132, 132): 0.9850435853004456,\n",
       " (314, 234): 0.9850330352783203,\n",
       " (283, 139): 0.9849985241889954,\n",
       " (170, 170): 0.9849971532821655,\n",
       " (199, 167): 0.9849809408187866,\n",
       " (198, 166): 0.9849638938903809,\n",
       " (324, 260): 0.9849581718444824,\n",
       " (300, 236): 0.9849562048912048,\n",
       " (299, 235): 0.9849175810813904,\n",
       " (203, 171): 0.9849129915237427,\n",
       " (217, 137): 0.9848880767822266,\n",
       " (216, 136): 0.9848838448524475,\n",
       " (221, 157): 0.9848685264587402,\n",
       " (326, 262): 0.9848480224609375,\n",
       " (169, 169): 0.9848345518112183,\n",
       " (181, 133): 0.9848102927207947,\n",
       " (330, 266): 0.9848089218139648,\n",
       " (329, 265): 0.9847583770751953,\n",
       " (193, 145): 0.9847381114959717,\n",
       " (331, 267): 0.9846537709236145,\n",
       " (280, 152): 0.9846351742744446,\n",
       " (154, 138): 0.9846146702766418,\n",
       " (328, 264): 0.9845531582832336,\n",
       " (329, 281): 0.9845457673072815,\n",
       " (159, 111): 0.984504759311676,\n",
       " (185, 137): 0.9844543933868408,\n",
       " (202, 170): 0.984402596950531,\n",
       " (187, 139): 0.9843575358390808,\n",
       " (134, 134): 0.9843246936798096,\n",
       " (334, 270): 0.9843236207962036,\n",
       " (223, 111): 0.9843121767044067,\n",
       " (222, 126): 0.9842919111251831,\n",
       " (315, 235): 0.9842208027839661,\n",
       " (186, 138): 0.9842052459716797,\n",
       " (200, 184): 0.9841793179512024,\n",
       " (79, 63): 0.9841721653938293,\n",
       " (184, 136): 0.984150767326355,\n",
       " (139, 139): 0.9841012358665466,\n",
       " (188, 140): 0.9840776324272156,\n",
       " (212, 164): 0.9840215444564819,\n",
       " (201, 169): 0.9840208888053894,\n",
       " (320, 256): 0.9840165972709656,\n",
       " (330, 282): 0.9840042591094971,\n",
       " (197, 181): 0.9839878678321838,\n",
       " (206, 190): 0.9839349985122681,\n",
       " (301, 237): 0.9838910102844238,\n",
       " (201, 185): 0.9837709665298462,\n",
       " (331, 251): 0.9837674498558044,\n",
       " (282, 138): 0.9837411046028137,\n",
       " (277, 165): 0.983738362789154,\n",
       " (215, 135): 0.9836742281913757,\n",
       " (333, 253): 0.9836560487747192,\n",
       " (153, 137): 0.983603835105896,\n",
       " (297, 233): 0.9835759997367859,\n",
       " (291, 227): 0.9835670590400696,\n",
       " (266, 202): 0.9835608005523682,\n",
       " (205, 173): 0.9835200905799866,\n",
       " (196, 180): 0.9835172891616821,\n",
       " (195, 179): 0.9834875464439392,\n",
       " (334, 254): 0.9834498763084412,\n",
       " (331, 283): 0.9834334850311279,\n",
       " (316, 236): 0.9833941459655762,\n",
       " (292, 228): 0.9833822250366211,\n",
       " (304, 192): 0.9833818674087524,\n",
       " (332, 284): 0.9833201766014099,\n",
       " (288, 208): 0.9833098649978638,\n",
       " (287, 159): 0.9833042025566101,\n",
       " (295, 231): 0.9832693338394165,\n",
       " (330, 250): 0.983267605304718,\n",
       " (277, 181): 0.9832659363746643,\n",
       " (332, 252): 0.9832339882850647,\n",
       " (189, 141): 0.9832303524017334,\n",
       " (279, 151): 0.9832050800323486,\n",
       " (328, 248): 0.9831976890563965,\n",
       " (286, 110): 0.98319411277771,\n",
       " (203, 187): 0.9831928014755249,\n",
       " (65, 64): 0.9831818342208862,\n",
       " (294, 230): 0.9831794500350952,\n",
       " (174, 174): 0.9831472635269165,\n",
       " (202, 186): 0.9831297397613525,\n",
       " (286, 126): 0.9831273555755615,\n",
       " (267, 203): 0.9831257462501526,\n",
       " (220, 156): 0.9830432534217834,\n",
       " (322, 258): 0.9830349683761597,\n",
       " ...}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "982b29b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.875"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "222 / 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ceb266b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(222, 222): 1.01083242893219,\n",
       " (223, 222): 1.0106792449951172,\n",
       " (211, 211): 1.0106325149536133,\n",
       " (212, 211): 1.0105767250061035,\n",
       " (212, 212): 1.0105624198913574,\n",
       " (217, 217): 1.01055908203125,\n",
       " (225, 193): 1.0105476379394531,\n",
       " (218, 217): 1.01052987575531,\n",
       " (216, 216): 1.0105193853378296,\n",
       " (213, 212): 1.0105186700820923,\n",
       " (218, 218): 1.0105125904083252,\n",
       " (215, 215): 1.0105113983154297,\n",
       " (221, 221): 1.0105092525482178,\n",
       " (213, 213): 1.0105016231536865,\n",
       " (222, 221): 1.010494351387024,\n",
       " (217, 216): 1.010489821434021,\n",
       " (220, 220): 1.01048743724823,\n",
       " (219, 218): 1.0104867219924927,\n",
       " (219, 219): 1.0104838609695435,\n",
       " (216, 215): 1.0104776620864868,\n",
       " (214, 214): 1.0104737281799316,\n",
       " (223, 223): 1.0104681253433228,\n",
       " (221, 220): 1.0104604959487915,\n",
       " (220, 219): 1.010460376739502,\n",
       " (214, 213): 1.0104600191116333,\n",
       " (215, 214): 1.0104374885559082,\n",
       " (273, 241): 1.0101580619812012,\n",
       " (208, 208): 1.0100829601287842,\n",
       " (272, 240): 1.0099990367889404,\n",
       " (210, 210): 1.0099852085113525,\n",
       " (211, 210): 1.0099111795425415,\n",
       " (275, 243): 1.009455680847168,\n",
       " (276, 244): 1.0094525814056396,\n",
       " (277, 245): 1.0094269514083862,\n",
       " (278, 246): 1.0094149112701416,\n",
       " (279, 247): 1.0094114542007446,\n",
       " (280, 248): 1.0093882083892822,\n",
       " (286, 254): 1.0093821287155151,\n",
       " (281, 249): 1.0093811750411987,\n",
       " (282, 250): 1.0093774795532227,\n",
       " (283, 251): 1.0093759298324585,\n",
       " (284, 252): 1.0093728303909302,\n",
       " (285, 253): 1.0093715190887451,\n",
       " (227, 195): 1.009355068206787,\n",
       " (238, 206): 1.0093504190444946,\n",
       " (237, 205): 1.0092791318893433,\n",
       " (236, 204): 1.0092682838439941,\n",
       " (228, 196): 1.009264349937439,\n",
       " (235, 203): 1.009257197380066,\n",
       " (234, 202): 1.009252667427063,\n",
       " (209, 193): 1.0092474222183228,\n",
       " (233, 201): 1.0092350244522095,\n",
       " (229, 197): 1.0092304944992065,\n",
       " (232, 200): 1.009217381477356,\n",
       " (231, 199): 1.0092140436172485,\n",
       " (230, 198): 1.0092023611068726,\n",
       " (224, 192): 1.0091960430145264,\n",
       " (225, 177): 1.0091509819030762,\n",
       " (274, 242): 1.0091174840927124,\n",
       " (208, 207): 1.009057641029358,\n",
       " (239, 207): 1.0089119672775269,\n",
       " (226, 194): 1.0088796615600586,\n",
       " (287, 255): 1.0085994005203247,\n",
       " (209, 177): 1.0085769891738892,\n",
       " (208, 192): 1.0085079669952393,\n",
       " (223, 207): 1.0085067749023438,\n",
       " (211, 195): 1.0084859132766724,\n",
       " (212, 196): 1.0084389448165894,\n",
       " (272, 271): 1.0084164142608643,\n",
       " (213, 197): 1.0084140300750732,\n",
       " (222, 206): 1.0084116458892822,\n",
       " (217, 201): 1.0084036588668823,\n",
       " (218, 202): 1.0084034204483032,\n",
       " (215, 199): 1.0084021091461182,\n",
       " (214, 198): 1.008400797843933,\n",
       " (216, 200): 1.0084004402160645,\n",
       " (219, 203): 1.0083999633789062,\n",
       " (220, 204): 1.0083993673324585,\n",
       " (221, 205): 1.0083988904953003,\n",
       " (210, 194): 1.0083110332489014,\n",
       " (208, 176): 1.0081027746200562,\n",
       " (224, 176): 1.0080687999725342,\n",
       " (238, 190): 1.00801682472229,\n",
       " (209, 208): 1.0080167055130005,\n",
       " (222, 190): 1.007973551750183,\n",
       " (211, 179): 1.0079584121704102,\n",
       " (212, 180): 1.0079505443572998,\n",
       " (236, 188): 1.007947325706482,\n",
       " (213, 181): 1.0079425573349,\n",
       " (235, 187): 1.007939100265503,\n",
       " (214, 182): 1.0079387426376343,\n",
       " (215, 183): 1.0079357624053955,\n",
       " (234, 186): 1.007935643196106,\n",
       " (229, 181): 1.0079349279403687,\n",
       " (233, 185): 1.0079327821731567,\n",
       " (216, 184): 1.007930040359497,\n",
       " (228, 180): 1.0079296827316284,\n",
       " (217, 185): 1.0079271793365479,\n",
       " (232, 184): 1.007926344871521,\n",
       " (231, 183): 1.0079243183135986,\n",
       " (218, 186): 1.0079240798950195,\n",
       " (227, 179): 1.0079227685928345,\n",
       " (230, 182): 1.0079225301742554,\n",
       " (219, 187): 1.0079209804534912,\n",
       " (220, 188): 1.0079193115234375,\n",
       " (237, 189): 1.007917046546936,\n",
       " (221, 189): 1.0079115629196167,\n",
       " (210, 178): 1.0078599452972412,\n",
       " (209, 145): 1.007800817489624,\n",
       " (209, 161): 1.0077612400054932,\n",
       " (223, 191): 1.0077369213104248,\n",
       " (226, 178): 1.0077112913131714,\n",
       " (208, 144): 1.0074940919876099,\n",
       " (225, 161): 1.0074483156204224,\n",
       " (208, 160): 1.0074403285980225,\n",
       " (225, 145): 1.0074315071105957,\n",
       " (222, 158): 1.0074166059494019,\n",
       " (209, 129): 1.0074131488800049,\n",
       " (211, 147): 1.0073856115341187,\n",
       " (212, 148): 1.0073847770690918,\n",
       " (320, 256): 1.0073822736740112,\n",
       " (213, 149): 1.0073784589767456,\n",
       " (214, 150): 1.0073755979537964,\n",
       " (215, 151): 1.0073742866516113,\n",
       " (221, 157): 1.0073723793029785,\n",
       " (220, 156): 1.0073702335357666,\n",
       " (219, 155): 1.0073683261871338,\n",
       " (216, 152): 1.0073676109313965,\n",
       " (218, 154): 1.0073660612106323,\n",
       " (217, 153): 1.0073643922805786,\n",
       " (223, 175): 1.0073565244674683,\n",
       " (222, 174): 1.0073554515838623,\n",
       " (239, 191): 1.007338285446167,\n",
       " (212, 164): 1.007293939590454,\n",
       " (213, 165): 1.0072910785675049,\n",
       " (211, 163): 1.007285714149475,\n",
       " (214, 166): 1.0072799921035767,\n",
       " (210, 146): 1.0072637796401978,\n",
       " (221, 173): 1.0072606801986694,\n",
       " (215, 167): 1.0072600841522217,\n",
       " (220, 172): 1.0072582960128784,\n",
       " (217, 169): 1.007257103919983,\n",
       " (218, 170): 1.0072537660598755,\n",
       " (219, 171): 1.0072531700134277,\n",
       " (216, 168): 1.0072510242462158,\n",
       " (223, 159): 1.0072450637817383,\n",
       " (209, 209): 1.0072442293167114,\n",
       " (272, 192): 1.007238507270813,\n",
       " (210, 162): 1.0072101354599,\n",
       " (238, 158): 1.0071420669555664,\n",
       " (208, 128): 1.0071418285369873,\n",
       " (224, 144): 1.0071324110031128,\n",
       " (237, 157): 1.0070832967758179,\n",
       " (236, 156): 1.0070735216140747,\n",
       " (222, 142): 1.0070711374282837,\n",
       " (235, 155): 1.0070648193359375,\n",
       " (231, 151): 1.0070616006851196,\n",
       " (234, 154): 1.007056713104248,\n",
       " (228, 148): 1.0070562362670898,\n",
       " (230, 150): 1.0070558786392212,\n",
       " (211, 131): 1.0070557594299316,\n",
       " (233, 153): 1.0070538520812988,\n",
       " (232, 152): 1.007053017616272,\n",
       " (212, 132): 1.007051706314087,\n",
       " (229, 149): 1.0070511102676392,\n",
       " (227, 147): 1.007041573524475,\n",
       " (213, 133): 1.0070403814315796,\n",
       " (214, 134): 1.0070348978042603,\n",
       " (215, 135): 1.007032871246338,\n",
       " (221, 141): 1.0070303678512573,\n",
       " (216, 136): 1.0070301294326782,\n",
       " (220, 140): 1.0070277452468872,\n",
       " (217, 137): 1.0070245265960693,\n",
       " (219, 139): 1.007019281387329,\n",
       " (218, 138): 1.0070172548294067,\n",
       " (272, 224): 1.0070154666900635,\n",
       " (272, 256): 1.0070122480392456,\n",
       " (238, 174): 1.0069950819015503,\n",
       " (208, 112): 1.0069653987884521,\n",
       " (274, 226): 1.0069553852081299,\n",
       " (209, 113): 1.0069520473480225,\n",
       " (275, 195): 1.0069383382797241,\n",
       " (276, 196): 1.0069323778152466,\n",
       " (277, 197): 1.0069308280944824,\n",
       " (278, 198): 1.006925344467163,\n",
       " (279, 199): 1.0069196224212646,\n",
       " (210, 130): 1.0069137811660767,\n",
       " (280, 200): 1.0069129467010498,\n",
       " (281, 201): 1.0069029331207275,\n",
       " (282, 202): 1.0068953037261963,\n",
       " (283, 203): 1.0068888664245605,\n",
       " (284, 204): 1.0068814754486084,\n",
       " (223, 143): 1.0068756341934204,\n",
       " (285, 205): 1.0068728923797607,\n",
       " (237, 173): 1.006858229637146,\n",
       " (226, 146): 1.0068511962890625,\n",
       " (224, 160): 1.0068495273590088,\n",
       " (236, 172): 1.0068461894989014,\n",
       " (286, 206): 1.0068455934524536,\n",
       " (229, 165): 1.006845474243164,\n",
       " (273, 193): 1.006842851638794,\n",
       " (230, 166): 1.0068411827087402,\n",
       " (272, 176): 1.0068364143371582,\n",
       " (228, 164): 1.0068354606628418,\n",
       " (274, 194): 1.006834864616394,\n",
       " (235, 171): 1.0068293809890747,\n",
       " (234, 170): 1.006822943687439,\n",
       " (233, 169): 1.0068217515945435,\n",
       " (231, 167): 1.0068210363388062,\n",
       " (289, 193): 1.0068128108978271,\n",
       " (208, 96): 1.0068118572235107,\n",
       " (232, 168): 1.0068106651306152,\n",
       " (227, 163): 1.0068062543869019,\n",
       " (211, 115): 1.0067884922027588,\n",
       " (212, 116): 1.0067814588546753,\n",
       " (222, 126): 1.006777048110962,\n",
       " (213, 117): 1.0067662000656128,\n",
       " (214, 118): 1.0067565441131592,\n",
       " (215, 119): 1.0067533254623413,\n",
       " (216, 120): 1.0067516565322876,\n",
       " (217, 121): 1.0067508220672607,\n",
       " (218, 122): 1.0067448616027832,\n",
       " (220, 124): 1.0067442655563354,\n",
       " (221, 125): 1.006744146347046,\n",
       " (219, 123): 1.006743311882019,\n",
       " (301, 205): 1.00674307346344,\n",
       " (300, 204): 1.0067369937896729,\n",
       " (274, 258): 1.0067334175109863,\n",
       " (225, 129): 1.0067291259765625,\n",
       " (297, 201): 1.006727933883667,\n",
       " (299, 203): 1.0067275762557983,\n",
       " (298, 202): 1.0067254304885864,\n",
       " (272, 144): 1.0067241191864014,\n",
       " (275, 227): 1.0067178010940552,\n",
       " (291, 195): 1.0067172050476074,\n",
       " (296, 200): 1.0067121982574463,\n",
       " (302, 206): 1.0067081451416016,\n",
       " (226, 162): 1.0066956281661987,\n",
       " (295, 199): 1.0066953897476196,\n",
       " (294, 198): 1.0066804885864258,\n",
       " (292, 196): 1.006679892539978,\n",
       " (273, 257): 1.0066783428192139,\n",
       " (210, 114): 1.0066739320755005,\n",
       " (276, 228): 1.0066720247268677,\n",
       " (293, 197): 1.006670594215393,\n",
       " (288, 192): 1.0066580772399902,\n",
       " (277, 229): 1.0066542625427246,\n",
       " (290, 242): 1.0066407918930054,\n",
       " (209, 97): 1.0066360235214233,\n",
       " (211, 99): 1.0066280364990234,\n",
       " (212, 100): 1.0066173076629639,\n",
       " (213, 101): 1.0066131353378296,\n",
       " (214, 102): 1.0066123008728027,\n",
       " (278, 230): 1.006611943244934,\n",
       " (238, 142): 1.006609559059143,\n",
       " (215, 103): 1.0066049098968506,\n",
       " (216, 104): 1.0066027641296387,\n",
       " (217, 105): 1.0066009759902954,\n",
       " (290, 194): 1.0065996646881104,\n",
       " (220, 108): 1.0065981149673462,\n",
       " (221, 109): 1.0065981149673462,\n",
       " (279, 231): 1.0065959692001343,\n",
       " (218, 106): 1.0065948963165283,\n",
       " (219, 107): 1.0065935850143433,\n",
       " (222, 110): 1.0065901279449463,\n",
       " (272, 160): 1.0065820217132568,\n",
       " (280, 232): 1.006580114364624,\n",
       " (223, 127): 1.0065698623657227,\n",
       " (281, 233): 1.0065540075302124,\n",
       " (282, 234): 1.0065422058105469,\n",
       " (283, 235): 1.0065358877182007,\n",
       " (237, 141): 1.0065340995788574,\n",
       " (236, 140): 1.0065230131149292,\n",
       " (284, 236): 1.0065200328826904,\n",
       " (208, 80): 1.0065114498138428,\n",
       " (232, 136): 1.0065068006515503,\n",
       " (227, 131): 1.0065058469772339,\n",
       " (223, 111): 1.0065056085586548,\n",
       " (231, 135): 1.006503701210022,\n",
       " (228, 132): 1.0065029859542847,\n",
       " (285, 237): 1.0065009593963623,\n",
       " (210, 98): 1.0065006017684937,\n",
       " (235, 139): 1.0064992904663086,\n",
       " (230, 134): 1.0064964294433594,\n",
       " (233, 137): 1.0064955949783325,\n",
       " (229, 133): 1.0064911842346191,\n",
       " (224, 128): 1.0064910650253296,\n",
       " (234, 138): 1.0064865350723267,\n",
       " (301, 253): 1.0064829587936401,\n",
       " (287, 207): 1.006482481956482,\n",
       " (275, 179): 1.0064762830734253,\n",
       " (274, 178): 1.0064666271209717,\n",
       " (275, 259): 1.0064579248428345,\n",
       " (276, 180): 1.0064561367034912,\n",
       " (286, 238): 1.0064477920532227,\n",
       " (277, 181): 1.0064457654953003,\n",
       " (273, 225): 1.0064444541931152,\n",
       " (278, 182): 1.0064432621002197,\n",
       " (279, 183): 1.0064407587051392,\n",
       " (273, 145): 1.00643789768219,\n",
       " (239, 159): 1.0064342021942139,\n",
       " (280, 184): 1.0064338445663452,\n",
       " (272, 128): 1.0064327716827393,\n",
       " (286, 270): 1.006429672241211,\n",
       " (281, 185): 1.0064263343811035,\n",
       " (275, 147): 1.006421446800232,\n",
       " (282, 186): 1.006421446800232,\n",
       " (287, 239): 1.00641930103302,\n",
       " (276, 260): 1.0064164400100708,\n",
       " (283, 187): 1.006414771080017,\n",
       " (291, 243): 1.0064140558242798,\n",
       " (276, 148): 1.0064129829406738,\n",
       " (277, 149): 1.0064079761505127,\n",
       " (284, 188): 1.0064060688018799,\n",
       " (300, 252): 1.0064058303833008,\n",
       " (278, 150): 1.0064038038253784,\n",
       " (279, 151): 1.006399154663086,\n",
       " (285, 189): 1.0063984394073486,\n",
       " (280, 152): 1.0063979625701904,\n",
       " (281, 153): 1.006392240524292,\n",
       " (299, 251): 1.0063884258270264,\n",
       " (282, 154): 1.0063880681991577,\n",
       " (278, 262): 1.006387710571289,\n",
       " (277, 261): 1.006384253501892,\n",
       " (279, 263): 1.006384253501892,\n",
       " (283, 155): 1.0063834190368652,\n",
       " (284, 268): 1.0063825845718384,\n",
       " (293, 245): 1.0063822269439697,\n",
       " (280, 264): 1.0063821077346802,\n",
       " (295, 247): 1.0063806772232056,\n",
       " (285, 269): 1.0063806772232056,\n",
       " (224, 112): 1.0063799619674683,\n",
       " (284, 156): 1.0063796043395996,\n",
       " (297, 249): 1.0063787698745728,\n",
       " (286, 190): 1.0063769817352295,\n",
       " (281, 265): 1.006375789642334,\n",
       " (285, 157): 1.0063751935958862,\n",
       " (298, 250): 1.0063729286193848,\n",
       " (296, 248): 1.006372332572937,\n",
       " (283, 267): 1.006369709968567,\n",
       " (208, 64): 1.0063692331314087,\n",
       " (294, 246): 1.0063691139221191,\n",
       " (292, 244): 1.0063645839691162,\n",
       " (286, 158): 1.0063620805740356,\n",
       " (282, 266): 1.0063616037368774,\n",
       " (273, 177): 1.0063601732254028,\n",
       " (211, 83): 1.0063550472259521,\n",
       " (212, 84): 1.0063539743423462,\n",
       " (213, 85): 1.0063472986221313,\n",
       " (214, 86): 1.0063438415527344,\n",
       " (334, 254): 1.0063402652740479,\n",
       " (215, 87): 1.0063389539718628,\n",
       " (217, 89): 1.0063385963439941,\n",
       " (221, 93): 1.0063377618789673,\n",
       " (218, 90): 1.0063360929489136,\n",
       " (274, 146): 1.0063358545303345,\n",
       " (216, 88): 1.006335735321045,\n",
       " (226, 130): 1.0063347816467285,\n",
       " (219, 91): 1.006333827972412,\n",
       " (220, 92): 1.0063329935073853,\n",
       " (222, 94): 1.0063248872756958,\n",
       " (332, 252): 1.0063202381134033,\n",
       " (330, 250): 1.0063198804855347,\n",
       " (331, 251): 1.0063190460205078,\n",
       " (329, 249): 1.0063146352767944,\n",
       " (320, 240): 1.0063138008117676,\n",
       " (209, 81): 1.0063109397888184,\n",
       " (328, 248): 1.0063068866729736,\n",
       " (223, 95): 1.0062980651855469,\n",
       " (239, 175): 1.0062979459762573,\n",
       " (272, 112): 1.0062967538833618,\n",
       " (302, 254): 1.0062965154647827,\n",
       " (333, 269): 1.0062956809997559,\n",
       " (272, 96): 1.0062756538391113,\n",
       " (333, 253): 1.006273865699768,\n",
       " (326, 246): 1.0062642097473145,\n",
       " (327, 247): 1.0062593221664429,\n",
       " (325, 245): 1.0062587261199951,\n",
       " (330, 266): 1.006255865097046,\n",
       " (332, 268): 1.006248950958252,\n",
       " (324, 244): 1.0062434673309326,\n",
       " (331, 267): 1.0062370300292969,\n",
       " (329, 265): 1.0062257051467896,\n",
       " (210, 82): 1.0062241554260254,\n",
       " (225, 113): 1.0062193870544434,\n",
       " (323, 243): 1.0062004327774048,\n",
       " (324, 260): 1.0061964988708496,\n",
       " (325, 261): 1.0061933994293213,\n",
       " (238, 126): 1.0061899423599243,\n",
       " (326, 262): 1.0061891078948975,\n",
       " (327, 263): 1.0061854124069214,\n",
       " (228, 116): 1.0061789751052856,\n",
       " (273, 161): 1.0061784982681274,\n",
       " (227, 115): 1.0061781406402588,\n",
       " (328, 264): 1.0061761140823364,\n",
       " (287, 191): 1.0061726570129395,\n",
       " (273, 129): 1.0061722993850708,\n",
       " (321, 241): 1.0061709880828857,\n",
       " (229, 117): 1.0061687231063843,\n",
       " (236, 124): 1.0061687231063843,\n",
       " (237, 125): 1.0061687231063843,\n",
       " (233, 121): 1.0061670541763306,\n",
       " (275, 163): 1.006163239479065,\n",
       " (235, 123): 1.0061627626419067,\n",
       " (232, 120): 1.0061618089675903,\n",
       " (234, 122): 1.0061603784561157,\n",
       " (224, 96): 1.0061568021774292,\n",
       " (231, 119): 1.0061564445495605,\n",
       " (230, 118): 1.0061545372009277,\n",
       " (276, 164): 1.006149411201477,\n",
       " (334, 270): 1.006149172782898,\n",
       " (274, 162): 1.0061489343643188,\n",
       " (208, 48): 1.0061423778533936,\n",
       " (277, 165): 1.006141185760498,\n",
       " (287, 159): 1.0061345100402832,\n",
       " (278, 166): 1.0061331987380981,\n",
       " (279, 167): 1.0061286687850952,\n",
       " (280, 168): 1.0061246156692505,\n",
       " (281, 169): 1.0061228275299072,\n",
       " (282, 170): 1.006118893623352,\n",
       " (283, 171): 1.0061146020889282,\n",
       " (284, 172): 1.0061123371124268,\n",
       " (286, 174): 1.0061098337173462,\n",
       " (285, 173): 1.0061079263687134,\n",
       " (211, 67): 1.006106972694397,\n",
       " (272, 80): 1.0061062574386597,\n",
       " (212, 68): 1.0061041116714478,\n",
       " (213, 69): 1.006098985671997,\n",
       " (323, 259): 1.0060979127883911,\n",
       " (214, 70): 1.0060968399047852,\n",
       " (335, 255): 1.0060968399047852,\n",
       " (215, 71): 1.006095290184021,\n",
       " (216, 72): 1.0060926675796509,\n",
       " (220, 76): 1.0060912370681763,\n",
       " (221, 77): 1.0060904026031494,\n",
       " (217, 73): 1.0060898065567017,\n",
       " (219, 75): 1.0060893297195435,\n",
       " (218, 74): 1.006089210510254,\n",
       " (222, 78): 1.006084680557251,\n",
       " (275, 131): 1.0060653686523438,\n",
       " (276, 132): 1.006051778793335,\n",
       " (226, 114): 1.0060454607009888,\n",
       " (277, 133): 1.006044864654541,\n",
       " (322, 242): 1.0060428380966187,\n",
       " (287, 175): 1.0060375928878784,\n",
       " (278, 134): 1.0060365200042725,\n",
       " (279, 135): 1.0060312747955322,\n",
       " (280, 136): 1.0060268640518188,\n",
       " (274, 130): 1.0060237646102905,\n",
       " (210, 66): 1.0060216188430786,\n",
       " (281, 137): 1.0060211420059204,\n",
       " (282, 138): 1.0060170888900757,\n",
       " (283, 139): 1.006014108657837,\n",
       " (284, 140): 1.006011962890625,\n",
       " (209, 65): 1.0060111284255981,\n",
       " (286, 142): 1.0060076713562012,\n",
       " (285, 141): 1.0060073137283325,\n",
       " (223, 79): 1.006003975868225,\n",
       " (272, 64): 1.0059994459152222,\n",
       " (303, 207): 1.0059925317764282,\n",
       " (273, 113): 1.0059765577316284,\n",
       " (208, 32): 1.005964756011963,\n",
       " (237, 109): 1.0059640407562256,\n",
       " (230, 102): 1.0059634447097778,\n",
       " (275, 115): 1.0059627294540405,\n",
       " (227, 99): 1.005961298942566,\n",
       " (233, 105): 1.0059590339660645,\n",
       " (229, 101): 1.0059585571289062,\n",
       " (232, 104): 1.0059584379196167,\n",
       " (236, 108): 1.0059584379196167,\n",
       " (275, 99): 1.0059565305709839,\n",
       " (228, 100): 1.0059558153152466,\n",
       " (231, 103): 1.0059551000595093,\n",
       " (276, 116): 1.0059529542922974,\n",
       " (234, 106): 1.0059528350830078,\n",
       " (235, 107): 1.0059503316879272,\n",
       " (277, 117): 1.0059481859207153,\n",
       " (276, 100): 1.0059455633163452,\n",
       " (211, 51): 1.0059442520141602,\n",
       " (278, 118): 1.0059422254562378,\n",
       " (212, 52): 1.0059400796890259,\n",
       " (277, 101): 1.0059394836425781,\n",
       " (279, 119): 1.0059367418289185,\n",
       " (278, 102): 1.0059354305267334,\n",
       " (213, 53): 1.0059353113174438,\n",
       " (238, 110): 1.0059345960617065,\n",
       " (214, 54): 1.0059328079223633,\n",
       " (280, 120): 1.0059324502944946,\n",
       " (216, 56): 1.0059317350387573,\n",
       " (215, 55): 1.00593101978302,\n",
       " (217, 57): 1.0059309005737305,\n",
       " (279, 103): 1.0059289932250977,\n",
       " (281, 121): 1.0059272050857544,\n",
       " (218, 58): 1.0059263706207275,\n",
       " (280, 104): 1.0059257745742798,\n",
       " (282, 122): 1.0059248208999634,\n",
       " (220, 60): 1.0059247016906738,\n",
       " (219, 59): 1.0059242248535156,\n",
       " (281, 105): 1.0059212446212769,\n",
       " (283, 123): 1.005920648574829,\n",
       " (221, 61): 1.0059195756912231,\n",
       " (282, 106): 1.0059188604354858,\n",
       " (284, 124): 1.0059168338775635,\n",
       " (283, 107): 1.005915641784668,\n",
       " (284, 108): 1.005913496017456,\n",
       " (285, 125): 1.0059120655059814,\n",
       " (285, 109): 1.0059090852737427,\n",
       " (320, 192): 1.0059071779251099,\n",
       " (274, 114): 1.0059069395065308,\n",
       " (286, 126): 1.005906343460083,\n",
       " (286, 110): 1.005904197692871,\n",
       " (273, 97): 1.0058975219726562,\n",
       " (222, 62): 1.0058919191360474,\n",
       " (272, 48): 1.0058863162994385,\n",
       " (225, 97): 1.0058790445327759,\n",
       " (274, 98): 1.005875825881958,\n",
       " (303, 191): 1.0058510303497314,\n",
       " (239, 143): 1.0058329105377197,\n",
       " (210, 50): 1.0058324337005615,\n",
       " (128, 127): 1.0058236122131348,\n",
       " (226, 98): 1.0058190822601318,\n",
       " (208, 16): 1.005818486213684,\n",
       " (288, 240): 1.0058130025863647,\n",
       " (209, 49): 1.0058060884475708,\n",
       " (321, 257): 1.0057929754257202,\n",
       " (287, 143): 1.0057927370071411,\n",
       " (224, 80): 1.0057907104492188,\n",
       " (275, 83): 1.0057858228683472,\n",
       " (223, 63): 1.0057817697525024,\n",
       " (276, 84): 1.0057756900787354,\n",
       " (322, 258): 1.0057746171951294,\n",
       " (277, 85): 1.0057706832885742,\n",
       " (272, 32): 1.0057700872421265,\n",
       " (278, 86): 1.005765676498413,\n",
       " (279, 87): 1.005760669708252,\n",
       " (280, 88): 1.0057564973831177,\n",
       " (281, 89): 1.005751371383667,\n",
       " (273, 81): 1.0057507753372192,\n",
       " (282, 90): 1.0057488679885864,\n",
       " (283, 91): 1.0057456493377686,\n",
       " (286, 94): 1.005744218826294,\n",
       " (284, 92): 1.005742073059082,\n",
       " (287, 271): 1.0057419538497925,\n",
       " (285, 93): 1.005736231803894,\n",
       " (211, 35): 1.0057308673858643,\n",
       " (320, 80): 1.0057294368743896,\n",
       " (214, 38): 1.0057283639907837,\n",
       " (212, 36): 1.0057274103164673,\n",
       " (215, 39): 1.0057270526885986,\n",
       " (213, 37): 1.0057268142700195,\n",
       " (216, 40): 1.0057264566421509,\n",
       " (217, 41): 1.005724310874939,\n",
       " (320, 144): 1.0057235956192017,\n",
       " (218, 42): 1.0057151317596436,\n",
       " (221, 45): 1.005715012550354,\n",
       " (274, 82): 1.0057133436203003,\n",
       " (220, 44): 1.0057132244110107,\n",
       " (219, 43): 1.0057109594345093,\n",
       " (222, 46): 1.00569748878479,\n",
       " (224, 64): 1.0056880712509155,\n",
       " (272, 16): 1.0056877136230469,\n",
       " (320, 160): 1.0056744813919067,\n",
       " (276, 68): 1.0056664943695068,\n",
       " (275, 67): 1.0056647062301636,\n",
       " (277, 69): 1.0056627988815308,\n",
       " (287, 111): 1.005661129951477,\n",
       " (278, 70): 1.0056589841842651,\n",
       " (279, 71): 1.005655288696289,\n",
       " (282, 74): 1.005651593208313,\n",
       " (280, 72): 1.0056512355804443,\n",
       " (281, 73): 1.005649447441101,\n",
       " (283, 75): 1.0056473016738892,\n",
       " (284, 76): 1.0056439638137817,\n",
       " (286, 78): 1.0056434869766235,\n",
       " (288, 144): 1.0056411027908325,\n",
       " (285, 77): 1.0056402683258057,\n",
       " (237, 93): 1.0056337118148804,\n",
       " (230, 86): 1.0056277513504028,\n",
       " (228, 84): 1.0056270360946655,\n",
       " (233, 89): 1.0056270360946655,\n",
       " (236, 92): 1.0056266784667969,\n",
       " (229, 85): 1.0056265592575073,\n",
       " (210, 34): 1.0056257247924805,\n",
       " (234, 90): 1.0056257247924805,\n",
       " (235, 91): 1.005624771118164,\n",
       " (227, 83): 1.005623459815979,\n",
       " (231, 87): 1.005623459815979,\n",
       " (232, 88): 1.005623459815979,\n",
       " (320, 128): 1.005620002746582,\n",
       " (287, 127): 1.0056188106536865,\n",
       " (238, 94): 1.0056120157241821,\n",
       " (274, 66): 1.0056061744689941,\n",
       " (273, 65): 1.005599856376648,\n",
       " (212, 20): 1.0055967569351196,\n",
       " (213, 21): 1.00559663772583,\n",
       " (214, 22): 1.005593180656433,\n",
       " (211, 19): 1.005592703819275,\n",
       " (215, 23): 1.0055909156799316,\n",
       " (321, 1): 1.005590558052063,\n",
       " (320, 176): 1.0055869817733765,\n",
       " (216, 24): 1.0055867433547974,\n",
       " (221, 29): 1.005584716796875,\n",
       " (217, 25): 1.0055838823318481,\n",
       " (223, 47): 1.0055835247039795,\n",
       " (218, 26): 1.0055824518203735,\n",
       " (219, 27): 1.005579948425293,\n",
       " (220, 28): 1.0055760145187378,\n",
       " (287, 95): 1.0055756568908691,\n",
       " (222, 30): 1.0055747032165527,\n",
       " (320, 96): 1.0055698156356812,\n",
       " (275, 51): 1.0055677890777588,\n",
       " (209, 33): 1.0055642127990723,\n",
       " (289, 161): 1.005561351776123,\n",
       " (276, 52): 1.0055601596832275,\n",
       " (277, 53): 1.005556583404541,\n",
       " (278, 54): 1.0055525302886963,\n",
       " (279, 55): 1.0055503845214844,\n",
       " (280, 56): 1.0055466890335083,\n",
       " (281, 57): 1.0055440664291382,\n",
       " (282, 58): 1.0055429935455322,\n",
       " (286, 62): 1.0055423974990845,\n",
       " (320, 112): 1.0055415630340576,\n",
       " (283, 59): 1.0055407285690308,\n",
       " (284, 60): 1.0055389404296875,\n",
       " (285, 61): 1.0055381059646606,\n",
       " (335, 159): 1.0055286884307861,\n",
       " (272, 0): 1.0055253505706787,\n",
       " (335, 127): 1.0055150985717773,\n",
       " (274, 210): 1.0055146217346191,\n",
       " (302, 158): 1.0055136680603027,\n",
       " (334, 206): 1.0055104494094849,\n",
       " (335, 143): 1.0055084228515625,\n",
       " (273, 49): 1.005501627922058,\n",
       " (274, 50): 1.0054986476898193,\n",
       " (289, 241): 1.0054951906204224,\n",
       " (226, 82): 1.0054947137832642,\n",
       " (335, 271): 1.005486249923706,\n",
       " (225, 81): 1.0054856538772583,\n",
       " (320, 48): 1.0054837465286255,\n",
       " (320, 0): 1.0054796934127808,\n",
       " (224, 48): 1.0054773092269897,\n",
       " (326, 198): 1.0054734945297241,\n",
       " (289, 177): 1.0054676532745361,\n",
       " (288, 112): 1.0054599046707153,\n",
       " (210, 18): 1.0054569244384766,\n",
       " (327, 199): 1.005456805229187,\n",
       " (328, 200): 1.0054563283920288,\n",
       " (301, 157): 1.0054558515548706,\n",
       " (325, 197): 1.0054540634155273,\n",
       " (300, 156): 1.0054508447647095,\n",
       " (275, 35): 1.0054492950439453,\n",
       " (289, 145): 1.0054471492767334,\n",
       " (299, 155): 1.005444884300232,\n",
       " (223, 31): 1.0054404735565186,\n",
       " (276, 36): 1.0054404735565186,\n",
       " (208, 0): 1.0054395198822021,\n",
       " (298, 154): 1.0054394006729126,\n",
       " (287, 79): 1.0054385662078857,\n",
       " (277, 37): 1.005436658859253,\n",
       " (329, 201): 1.0054363012313843,\n",
       " (331, 203): 1.0054335594177246,\n",
       " (332, 204): 1.005433201789856,\n",
       " (330, 202): 1.0054324865341187,\n",
       " (278, 38): 1.0054305791854858,\n",
       " (333, 205): 1.0054303407669067,\n",
       " (279, 39): 1.005427360534668,\n",
       " (297, 153): 1.0054253339767456,\n",
       " (288, 96): 1.005424976348877,\n",
       " (280, 40): 1.0054224729537964,\n",
       " (282, 42): 1.00542151927948,\n",
       " (296, 152): 1.0054206848144531,\n",
       " (283, 43): 1.0054200887680054,\n",
       " (281, 41): 1.0054199695587158,\n",
       " (286, 46): 1.0054190158843994,\n",
       " (295, 151): 1.0054184198379517,\n",
       " (284, 44): 1.0054175853729248,\n",
       " (294, 150): 1.0054141283035278,\n",
       " (285, 45): 1.0054138898849487,\n",
       " (324, 196): 1.005411982536316,\n",
       " (293, 149): 1.0054116249084473,\n",
       " (211, 3): 1.0054011344909668,\n",
       " (291, 147): 1.0054010152816772,\n",
       " (292, 148): 1.0054010152816772,\n",
       " (209, 17): 1.005396842956543,\n",
       " (213, 5): 1.0053945779800415,\n",
       " (212, 4): 1.0053942203521729,\n",
       " (214, 6): 1.0053926706314087,\n",
       " (321, 193): 1.0053913593292236,\n",
       " (215, 7): 1.0053901672363281,\n",
       " (222, 14): 1.0053844451904297,\n",
       " (320, 64): 1.0053842067718506,\n",
       " (216, 8): 1.0053826570510864,\n",
       " (274, 34): 1.0053790807724,\n",
       " (217, 9): 1.0053788423538208,\n",
       " (218, 10): 1.0053768157958984,\n",
       " (219, 11): 1.0053750276565552,\n",
       " (220, 12): 1.0053730010986328,\n",
       " (221, 13): 1.0053715705871582,\n",
       " (273, 33): 1.00537109375,\n",
       " (275, 19): 1.0053682327270508,\n",
       " (290, 146): 1.0053678750991821,\n",
       " (335, 63): 1.0053632259368896,\n",
       " (237, 77): 1.0053619146347046,\n",
       " (329, 153): 1.0053614377975464,\n",
       " (328, 152): 1.00536048412323,\n",
       " (276, 20): 1.0053592920303345,\n",
       " (236, 76): 1.005359172821045,\n",
       " (232, 72): 1.0053586959838867,\n",
       " (233, 73): 1.0053575038909912,\n",
       " (277, 21): 1.0053573846817017,\n",
       " (228, 68): 1.005357027053833,\n",
       " (231, 71): 1.005357027053833,\n",
       " (230, 70): 1.0053563117980957,\n",
       " (227, 67): 1.0053555965423584,\n",
       " (278, 22): 1.0053552389144897,\n",
       " (229, 69): 1.0053552389144897,\n",
       " (234, 74): 1.0053539276123047,\n",
       " (235, 75): 1.005353331565857,\n",
       " (279, 23): 1.0053532123565674,\n",
       " (280, 24): 1.0053513050079346,\n",
       " (281, 25): 1.0053507089614868,\n",
       " (282, 26): 1.0053489208221436,\n",
       " (283, 27): 1.005347490310669,\n",
       " (288, 128): 1.0053470134735107,\n",
       " (284, 28): 1.0053459405899048,\n",
       " (286, 30): 1.00533926486969,\n",
       " (330, 154): 1.0053390264511108,\n",
       " (331, 155): 1.005338430404663,\n",
       " (332, 156): 1.005338430404663,\n",
       " (334, 62): 1.0053380727767944,\n",
       " (285, 29): 1.0053375959396362,\n",
       " (320, 32): 1.0053328275680542,\n",
       " (327, 151): 1.005332589149475,\n",
       " (238, 78): 1.0053315162658691,\n",
       " (333, 157): 1.00533127784729,\n",
       " (326, 150): 1.0053297281265259,\n",
       " (323, 147): 1.0053291320800781,\n",
       " (287, 63): 1.0053280591964722,\n",
       " (289, 113): 1.0053256750106812,\n",
       " (325, 149): 1.0053240060806274,\n",
       " (302, 174): 1.005321741104126,\n",
       " (239, 127): 1.0053198337554932,\n",
       " (293, 165): 1.0053178071975708,\n",
       " (294, 166): 1.0053141117095947,\n",
       " (324, 148): 1.0053128004074097,\n",
       " (321, 17): 1.0053123235702515,\n",
       " (292, 164): 1.005311131477356,\n",
       " (224, 32): 1.0053104162216187,\n",
       " (321, 33): 1.0053060054779053,\n",
       " (274, 18): 1.00529944896698,\n",
       " (223, 15): 1.0052975416183472,\n",
       " (335, 191): 1.005294680595398,\n",
       " (335, 207): 1.0052939653396606,\n",
       " (226, 66): 1.005289077758789,\n",
       " (290, 162): 1.0052883625030518,\n",
       " (323, 195): 1.005281925201416,\n",
       " (228, 52): 1.005280613899231,\n",
       " (233, 57): 1.0052802562713623,\n",
       " (227, 51): 1.0052798986434937,\n",
       " (295, 167): 1.0052798986434937,\n",
       " (335, 47): 1.0052789449691772,\n",
       " (273, 17): 1.0052779912948608,\n",
       " (210, 2): 1.0052777528762817,\n",
       " (229, 53): 1.0052777528762817,\n",
       " (231, 55): 1.0052776336669922,\n",
       " (230, 54): 1.005277156829834,\n",
       " (232, 56): 1.0052770376205444,\n",
       " (236, 60): 1.0052762031555176,\n",
       " (234, 58): 1.005275845527649,\n",
       " (321, 49): 1.0052753686904907,\n",
       " (291, 163): 1.0052753686904907,\n",
       " (296, 168): 1.0052753686904907,\n",
       " (237, 61): 1.005275011062622,\n",
       " (235, 59): 1.0052741765975952,\n",
       " (301, 173): 1.0052717924118042,\n",
       " (288, 80): 1.0052682161331177,\n",
       " (300, 172): 1.0052648782730103,\n",
       " (320, 16): 1.0052611827850342,\n",
       " (299, 171): 1.0052597522735596,\n",
       " (334, 46): 1.0052580833435059,\n",
       " (297, 169): 1.0052570104599,\n",
       " (298, 170): 1.0052565336227417,\n",
       " (335, 175): 1.005255103111267,\n",
       " (320, 224): 1.0052496194839478,\n",
       " (334, 158): 1.005249261856079,\n",
       " (334, 110): 1.0052459239959717,\n",
       " (334, 78): 1.005240559577942,\n",
       " (322, 146): 1.0052403211593628,\n",
       " (335, 111): 1.0052398443222046,\n",
       " (275, 3): 1.0052388906478882,\n",
       " (335, 239): 1.0052369832992554,\n",
       " (238, 62): 1.0052343606948853,\n",
       " (276, 4): 1.005232572555542,\n",
       " (129, 113): 1.0052318572998047,\n",
       " (289, 97): 1.005231261253357,\n",
       " (277, 5): 1.005226492881775,\n",
       " (280, 8): 1.0052260160446167,\n",
       " (281, 9): 1.0052258968353271,\n",
       " (287, 47): 1.0052255392074585,\n",
       " (282, 10): 1.0052242279052734,\n",
       " (278, 6): 1.0052238702774048,\n",
       " (283, 11): 1.0052227973937988,\n",
       " (279, 7): 1.005222201347351,\n",
       " (302, 126): 1.0052214860916138,\n",
       " (284, 12): 1.0052213668823242,\n",
       " (225, 65): 1.0052205324172974,\n",
       " (285, 13): 1.0052202939987183,\n",
       " (321, 65): 1.0052077770233154,\n",
       " (334, 94): 1.0052047967910767,\n",
       " (286, 14): 1.0052032470703125,\n",
       " (288, 176): 1.0052026510238647,\n",
       " (288, 48): 1.0051995515823364,\n",
       " (330, 106): 1.0051987171173096,\n",
       " (333, 189): 1.0051974058151245,\n",
       " (329, 105): 1.0051950216293335,\n",
       " (331, 107): 1.0051944255828857,\n",
       " (328, 104): 1.005192756652832,\n",
       " (327, 103): 1.0051920413970947,\n",
       " (333, 109): 1.0051888227462769,\n",
       " (331, 187): 1.0051884651184082,\n",
       " (330, 186): 1.0051875114440918,\n",
       " (332, 188): 1.0051864385604858,\n",
       " (288, 160): 1.0051844120025635,\n",
       " (329, 185): 1.0051844120025635,\n",
       " (326, 102): 1.0051831007003784,\n",
       " (335, 31): 1.0051829814910889,\n",
       " (332, 108): 1.0051829814910889,\n",
       " (328, 184): 1.0051829814910889,\n",
       " (325, 101): 1.005181908607483,\n",
       " (324, 100): 1.0051817893981934,\n",
       " (327, 183): 1.0051783323287964,\n",
       " (226, 50): 1.0051766633987427,\n",
       " (224, 16): 1.005176067352295,\n",
       " (239, 111): 1.0051753520965576,\n",
       " (326, 182): 1.0051746368408203,\n",
       " (320, 208): 1.0051746368408203,\n",
       " (326, 70): 1.0051742792129517,\n",
       " (302, 110): 1.0051738023757935,\n",
       " (325, 69): 1.0051723718643188,\n",
       " (301, 109): 1.0051709413528442,\n",
       " (335, 79): 1.0051707029342651,\n",
       " (300, 108): 1.0051695108413696,\n",
       " (330, 74): 1.00516939163208,\n",
       " (328, 72): 1.005169153213501,\n",
       " (274, 2): 1.0051686763763428,\n",
       " (324, 68): 1.0051684379577637,\n",
       " (327, 71): 1.0051676034927368,\n",
       " (329, 73): 1.0051671266555786,\n",
       " (299, 107): 1.0051614046096802,\n",
       " (287, 31): 1.0051589012145996,\n",
       " (331, 123): 1.005158543586731,\n",
       " (332, 124): 1.0051578283309937,\n",
       " (323, 99): 1.005157232284546,\n",
       " (330, 122): 1.005157232284546,\n",
       " (297, 105): 1.0051552057266235,\n",
       " (333, 125): 1.0051548480987549,\n",
       " (331, 75): 1.0051544904708862,\n",
       " (301, 125): 1.0051543712615967,\n",
       " (327, 119): 1.0051538944244385,\n",
       " (298, 106): 1.005153775215149,\n",
       " (328, 120): 1.0051535367965698,\n",
       " (325, 181): 1.0051535367965698,\n",
       " (329, 121): 1.0051525831222534,\n",
       " (326, 118): 1.0051518678665161,\n",
       " (335, 95): 1.0051517486572266,\n",
       " (334, 190): 1.0051511526107788,\n",
       " (296, 104): 1.0051496028900146,\n",
       " (209, 1): 1.005149006843567,\n",
       " (295, 103): 1.0051487684249878,\n",
       " (322, 194): 1.0051475763320923,\n",
       " (332, 76): 1.005147099494934,\n",
       " (288, 16): 1.0051461458206177,\n",
       " (273, 1): 1.0051460266113281,\n",
       " (333, 77): 1.0051459074020386,\n",
       " (324, 180): 1.0051442384719849,\n",
       " (323, 179): 1.0051424503326416,\n",
       " (323, 67): 1.0051416158676147,\n",
       " (291, 115): 1.0051416158676147,\n",
       " (300, 124): 1.0051406621932983,\n",
       " (293, 101): 1.0051381587982178,\n",
       " (325, 117): 1.0051381587982178,\n",
       " (332, 172): 1.0051380395889282,\n",
       " (333, 173): 1.0051378011703491,\n",
       " (331, 171): 1.0051370859146118,\n",
       " (333, 61): 1.0051360130310059,\n",
       " (292, 100): 1.0051360130310059,\n",
       " (299, 123): 1.0051360130310059,\n",
       " (330, 170): 1.0051352977752686,\n",
       " (294, 102): 1.005135178565979,\n",
       " (297, 121): 1.0051350593566895,\n",
       " (328, 88): 1.005134105682373,\n",
       " (328, 168): 1.005134105682373,\n",
       " (295, 119): 1.0051336288452148,\n",
       " (298, 122): 1.0051335096359253,\n",
       " (296, 120): 1.005133032798767,\n",
       " (291, 99): 1.005132794380188,\n",
       " (292, 116): 1.0051324367523193,\n",
       " (327, 87): 1.0051323175430298,\n",
       " (290, 114): 1.0051319599151611,\n",
       " (329, 169): 1.0051318407058716,\n",
       " (322, 82): 1.005131721496582,\n",
       " (294, 118): 1.0051298141479492,\n",
       " (326, 86): 1.0051279067993164,\n",
       " (288, 32): 1.0051276683807373,\n",
       " (325, 85): 1.0051261186599731,\n",
       " (327, 167): 1.0051246881484985,\n",
       " (302, 142): 1.0051227807998657,\n",
       " (326, 54): 1.0051226615905762,\n",
       " (324, 84): 1.0051225423812866,\n",
       " (322, 178): 1.0051220655441284,\n",
       " (325, 53): 1.005118727684021,\n",
       " (293, 117): 1.005118727684021,\n",
       " (327, 55): 1.005118489265442,\n",
       " (323, 35): 1.0051182508468628,\n",
       " (323, 83): 1.0051177740097046,\n",
       " (322, 98): 1.005116581916809,\n",
       " (324, 52): 1.0051144361495972,\n",
       " (289, 81): 1.0051090717315674,\n",
       " (331, 91): 1.0051072835922241,\n",
       " (332, 92): 1.0051066875457764,\n",
       " (287, 15): 1.0051065683364868,\n",
       " (323, 51): 1.0051058530807495,\n",
       " (330, 90): 1.0051052570343018,\n",
       " (289, 129): 1.0051047801971436,\n",
       " (331, 59): 1.005103349685669,\n",
       " (329, 89): 1.005103349685669,\n",
       " (301, 141): 1.0051020383834839,\n",
       " (330, 58): 1.0051010847091675,\n",
       " (328, 56): 1.0050996541976929,\n",
       " (324, 116): 1.0050989389419556,\n",
       " (334, 30): 1.0050979852676392,\n",
       " (329, 57): 1.0050978660583496,\n",
       " (290, 98): 1.0050972700119019,\n",
       " (332, 60): 1.0050969123840332,\n",
       " (300, 140): 1.0050959587097168,\n",
       " (297, 137): 1.0050935745239258,\n",
       " (288, 64): 1.0050932168960571,\n",
       " (298, 138): 1.005091905593872,\n",
       " (299, 139): 1.0050901174545288,\n",
       " (334, 126): 1.005089521408081,\n",
       " (302, 190): 1.0050876140594482,\n",
       " (225, 49): 1.0050870180130005,\n",
       " (324, 36): 1.00508451461792,\n",
       " (325, 37): 1.0050839185714722,\n",
       " (273, 209): 1.0050835609436035,\n",
       " (323, 115): 1.0050833225250244,\n",
       " (321, 97): 1.0050822496414185,\n",
       " (296, 136): 1.0050818920135498,\n",
       " (295, 135): 1.0050791501998901,\n",
       " (294, 134): 1.0050781965255737,\n",
       " (293, 133): 1.0050779581069946,\n",
       " (333, 93): 1.0050746202468872,\n",
       " (322, 162): 1.0050736665725708,\n",
       " (301, 93): 1.0050731897354126,\n",
       " (331, 43): 1.005072832107544,\n",
       " (292, 132): 1.0050725936889648,\n",
       " (230, 38): 1.0050716400146484,\n",
       " (231, 39): 1.0050708055496216,\n",
       " (330, 42): 1.0050692558288574,\n",
       " (272, 208): 1.0050692558288574,\n",
       " (229, 37): 1.0050690174102783,\n",
       " (227, 35): 1.0050685405731201,\n",
       " (233, 41): 1.0050684213638306,\n",
       " (323, 163): 1.005068063735962,\n",
       " (232, 40): 1.0050679445266724,\n",
       " (322, 130): 1.0050673484802246,\n",
       " (228, 36): 1.0050653219223022,\n",
       " (237, 45): 1.0050647258758545,\n",
       " (291, 131): 1.005063772201538,\n",
       " (234, 42): 1.005061149597168,\n",
       " (300, 92): 1.005058765411377,\n",
       " (129, 97): 1.0050581693649292,\n",
       " (322, 50): 1.0050580501556396,\n",
       " (236, 44): 1.0050578117370605,\n",
       " (326, 166): 1.0050570964813232,\n",
       " (235, 43): 1.0050567388534546,\n",
       " (299, 91): 1.0050549507141113,\n",
       " (321, 81): 1.005053997039795,\n",
       " (298, 90): 1.005052924156189,\n",
       " (297, 89): 1.0050524473190308,\n",
       " (326, 38): 1.005050539970398,\n",
       " (332, 44): 1.0050493478775024,\n",
       " (238, 46): 1.0050488710403442,\n",
       " (327, 39): 1.0050474405288696,\n",
       " (333, 45): 1.005046010017395,\n",
       " (302, 94): 1.0050454139709473,\n",
       " (322, 114): 1.005043387413025,\n",
       " (290, 130): 1.0050382614135742,\n",
       " (322, 66): 1.0050362348556519,\n",
       " (296, 88): 1.0050361156463623,\n",
       " (295, 87): 1.0050337314605713,\n",
       " (330, 138): 1.005031943321228,\n",
       " (325, 165): 1.005030632019043,\n",
       " (331, 139): 1.0050303936004639,\n",
       " (329, 137): 1.0050286054611206,\n",
       " (324, 164): 1.005028247833252,\n",
       " (294, 86): 1.005027413368225,\n",
       " (293, 85): 1.005023717880249,\n",
       " (334, 174): 1.0050214529037476,\n",
       " (329, 41): 1.005021333694458,\n",
       " ...}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dict(sorted(dict_vals.items(), key=lambda item: item[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62fa648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_encoder, hidden_GCN, num_classes, edge_weight, size=256,\n",
    "                 encoder_out=10, emb_type='linear', div_val=2, is_pos_embed='full', depth=1,\n",
    "                 num_blocks=3, iso_amount=16, b_size=60, is_edges_trainable=True):\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.is_pos_embed = is_pos_embed.lower()\n",
    "        self.depth = depth\n",
    "        self.enc_out = encoder_out\n",
    "        self.emb_type = emb_type\n",
    "        self.b_size = b_size\n",
    "        self.iso_amount = iso_amount\n",
    "        self.edge_weight = nn.Parameter(torch.tensor([float(ed) for ed in edge_weight])) if is_edges_trainable else None\n",
    "\n",
    "\n",
    "        if depth not in [1, 2]:\n",
    "            raise ValueError\n",
    "\n",
    "        self.layers = create_layers(size, num_blocks=num_blocks, hidden=hidden_encoder, tp=emb_type,\n",
    "                                    out=encoder_out, div_val=div_val, isoclines=self.iso_amount)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(b_size, 21 * self.iso_amount, hidden_encoder))\n",
    "        # self.conv1 = SAGEConv(hidden_GCN, hidden_GCN)\n",
    "        self.conv1 = GCNConv(hidden_GCN, hidden_GCN)\n",
    "        if depth == 2:\n",
    "            self.conv2 = GCNConv(hidden_GCN, hidden_GCN)\n",
    "        self.lin = nn.Linear(hidden_GCN, num_classes)\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, batch_size):\n",
    "        if self.emb_type == 'linear':\n",
    "            x = get_embedding(x, self.layers, iso_amount=self.iso_amount, batch_size=batch_size)  # current linear\n",
    "        elif self.emb_type == 'long':\n",
    "            x = get_long_embedding(x, self.layers, iso_amount=self.iso_amount, batch_size=batch_size)\n",
    "\n",
    "        if self.is_pos_embed == 'only_train':\n",
    "            x = add_positional_encoding(x, emb_len=self.enc_out)\n",
    "        elif self.is_pos_embed == 'only_param':\n",
    "            x += self.pos_embed\n",
    "        elif self.is_pos_embed == 'full':\n",
    "            x = add_positional_encoding(x, emb_len=self.enc_out) + self.pos_embed\n",
    "        # Maybe add this in get_embedding?\n",
    "        print(sum(self.edge_weight))\n",
    "        x = x.view(batch_size * 21 * self.iso_amount, -1)\n",
    "        edges = self.edge_weight.repeat(self.b_size).sigmoid() if self.edge_weight is not None else None\n",
    "        x = self.conv1(x, edge_index, edges)\n",
    "        # x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        if self.depth == 2:\n",
    "            x = self.conv2(x, edge_index, edges)\n",
    "            # x = self.conv2(x, edge_index)\n",
    "            x = x.relu()\n",
    "        # x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return self.soft(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "793442b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4016.0627, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[0.5357, 0.4643],\n",
      "        [0.5221, 0.4779],\n",
      "        [0.5309, 0.4691],\n",
      "        [0.5444, 0.4556],\n",
      "        [0.5377, 0.4623],\n",
      "        [0.5220, 0.4780],\n",
      "        [0.5375, 0.4625],\n",
      "        [0.5177, 0.4823],\n",
      "        [0.5452, 0.4548],\n",
      "        [0.5431, 0.4569],\n",
      "        [0.5458, 0.4542],\n",
      "        [0.5424, 0.4576],\n",
      "        [0.5228, 0.4772],\n",
      "        [0.5427, 0.4573],\n",
      "        [0.5405, 0.4595],\n",
      "        [0.5366, 0.4634],\n",
      "        [0.5338, 0.4662],\n",
      "        [0.5433, 0.4567],\n",
      "        [0.5248, 0.4752],\n",
      "        [0.5423, 0.4577],\n",
      "        [0.5275, 0.4725],\n",
      "        [0.5352, 0.4648],\n",
      "        [0.5322, 0.4678],\n",
      "        [0.5396, 0.4604],\n",
      "        [0.5417, 0.4583],\n",
      "        [0.5405, 0.4595],\n",
      "        [0.5200, 0.4800],\n",
      "        [0.5443, 0.4557],\n",
      "        [0.5335, 0.4665],\n",
      "        [0.5470, 0.4530],\n",
      "        [0.5484, 0.4516],\n",
      "        [0.5322, 0.4678],\n",
      "        [0.5336, 0.4664],\n",
      "        [0.5320, 0.4680],\n",
      "        [0.5384, 0.4616],\n",
      "        [0.5357, 0.4643],\n",
      "        [0.5352, 0.4648],\n",
      "        [0.5438, 0.4562],\n",
      "        [0.5448, 0.4552],\n",
      "        [0.5388, 0.4612],\n",
      "        [0.5289, 0.4711],\n",
      "        [0.5504, 0.4496],\n",
      "        [0.5496, 0.4504],\n",
      "        [0.5355, 0.4645],\n",
      "        [0.5439, 0.4561],\n",
      "        [0.5438, 0.4562],\n",
      "        [0.5410, 0.4590],\n",
      "        [0.5501, 0.4499],\n",
      "        [0.5408, 0.4592],\n",
      "        [0.5133, 0.4867],\n",
      "        [0.5182, 0.4818],\n",
      "        [0.5394, 0.4606],\n",
      "        [0.5399, 0.4601],\n",
      "        [0.5427, 0.4573],\n",
      "        [0.5409, 0.4591],\n",
      "        [0.5423, 0.4577],\n",
      "        [0.5365, 0.4635],\n",
      "        [0.5211, 0.4789],\n",
      "        [0.5429, 0.4571],\n",
      "        [0.5274, 0.4726]], device='cuda:1', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for itt, data in enumerate(train_loader):  # Iterate in batches over the training dataset.\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "    data = data.to(device)\n",
    "    out = model(data.x, data.edge_index, data.batch, batch_size)  # Perform a single forward pass.\n",
    "    print(out)\n",
    "    break\n",
    "    loss = criterion(out, data.y)  # Compute the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8416c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_res()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85e51e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GNN_isoclines.ipynb\t       conf.yml\t\t   main.py     preprocessing.py\r\n",
      "'Positional embedding.ipynb'   full_isoclines.py   nohup.out   test.py\r\n",
      " circle.py\t\t       isoclines.py\t   pickles     wandb\r\n"
     ]
    }
   ],
   "source": [
    "!ls diploma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "584fb5a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\r\n",
      "  model_type: 'GCNConv'\r\n",
      "  hidden_encoder_embed: 16\r\n",
      "  hidden_GCN_embed: 16\r\n",
      "  encoder_out: 8\r\n",
      "  allow_loops: True\r\n",
      "  #linear, long\r\n",
      "  emb_type: 'linear'\r\n",
      "  div_val: 2\r\n",
      "  #none, only_train, only_param, full\r\n",
      "  pos_embed: 'only_param'\r\n",
      "  depth: 2\r\n",
      "  #Experiment with long embed\r\n",
      "  num_blocks: 3\r\n",
      "  iso_amount: 16\r\n",
      "  is_edges_trainable: True\r\n",
      "  suffix: log\r\n",
      "\r\n",
      "train_params:\r\n",
      "  batch_size: 60\r\n",
      "  device_num: 1\r\n",
      "#  'online', 'offline', 'disabled'\r\n",
      "  wandb_mode: 'online'\r\n",
      "# CURRENT ON LOGS DATA\r\n",
      "files_params:\r\n",
      "  save_root: '/raid/data/chest_xray/'\r\n",
      "  path_to_files: '/raid/data/cats_dogs_dataset/preprocessed/*/*.npy'\r\n",
      "\r\n",
      "exp_name: 'Circle'\r\n"
     ]
    }
   ],
   "source": [
    "!cat diploma/conf.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a683925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GINConv, global_max_pool\n",
    "from torch_geometric.explain import Explainer, GNNExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6ea6ba06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNN(\n",
       "  (layers): ModuleList(\n",
       "    (0): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (9): ReLU()\n",
       "        (10): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (11): ReLU()\n",
       "        (12): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (13): ReLU()\n",
       "        (14): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (15): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (1): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (9): ReLU()\n",
       "        (10): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (11): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (2): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (9): ReLU()\n",
       "        (10): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (11): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (3): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (9): ReLU()\n",
       "        (10): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (11): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (4): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (9): ReLU()\n",
       "        (10): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (11): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (5): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (7): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (6): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (7): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (7): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (7): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (8): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (7): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (9): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (7): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (10): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (7): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (11): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (7): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (12): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (7): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (13): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (7): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (14): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (7): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (15): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (7): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (16): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (7): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (17): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (7): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (18): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (7): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (19): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (7): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (20): LinearAutoEncoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (7): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv1): GCNConv(16, 16)\n",
       "  (conv2): GCNConv(16, 16)\n",
       "  (lin): Linear(in_features=16, out_features=2, bias=True)\n",
       "  (soft): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ab79b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_long_embedding(data.x, model.layers, iso_amount=model.iso_amount, batch_size=60)  # current linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5f15073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = model.edge_weight.repeat(model.b_size).sigmoid() if model.edge_weight is not None else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b54c9925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7298, 0.7266, 0.7294,  ..., 0.7302, 0.7303, 0.7303], device='cuda:1',\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bbbb3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "31492e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "18a419f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.nn.conv.gcn_conv.GCNConv"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "53a16c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = deepcopy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "493ade5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.nn.conv.gcn_conv.GCNConv"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ee5621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0221ace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_expl(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(GNN_expl, self).__init__()\n",
    "        self.conv1 = deepcopy(model.conv1)\n",
    "        self.conv2 = deepcopy(model.conv2)\n",
    "        print(self.conv2)\n",
    "        self.lin = deepcopy(model.lin)\n",
    "        print(self.lin)\n",
    "        \n",
    "\n",
    "    def forward(self, x, edge_index, batch, b_size=60):\n",
    "        x = x.view(b_size * 21 * 16, -1)\n",
    "        edges = model.edge_weight.repeat(b_size).sigmoid() if model.edge_weight is not None else None\n",
    "        x = self.conv1(x, edge_index, edges)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edges)\n",
    "        x = x.relu()\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        x = F.dropout(x, p=0.1, training=Tr)\n",
    "        x = self.lin(x)\n",
    "        return self.soft(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa2caeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNConv(16, 16)\n",
      "Linear(in_features=16, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "md = GNN_expl(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2107d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a66b333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = get_long_embedding(data.x, model.layers, iso_amount=model.iso_amount, batch_size=60)  # current linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de59a125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 336, 16])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f592ee06",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 94874130472256 is out of bounds for dimension 0 with size 20160",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# md = md.to(device)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[35], line 15\u001b[0m, in \u001b[0;36mGNN_expl.forward\u001b[0;34m(self, x, edge_index, batch, b_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(b_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m21\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m edges \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39medge_weight\u001b[38;5;241m.\u001b[39mrepeat(b_size)\u001b[38;5;241m.\u001b[39msigmoid() \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39medge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mrelu()\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index, edges)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/nn/conv/gcn_conv.py:210\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    208\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/nn/conv/gcn_conv.py:103\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m deg_inv_sqrt \u001b[38;5;241m=\u001b[39m deg\u001b[38;5;241m.\u001b[39mpow_(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    102\u001b[0m deg_inv_sqrt\u001b[38;5;241m.\u001b[39mmasked_fill_(deg_inv_sqrt \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mdeg_inv_sqrt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m edge_weight \u001b[38;5;241m*\u001b[39m deg_inv_sqrt[col]\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m edge_index, edge_weight\n",
      "\u001b[0;31mIndexError\u001b[0m: index 94874130472256 is out of bounds for dimension 0 with size 20160"
     ]
    }
   ],
   "source": [
    "# md = md.to(device)\n",
    "md(x.to('cpu'), data.edge_index.to('cpu'), batch=data.batch.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dec65e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-1.7041e-22,  3.0953e-41,  1.0764e-20,  ...,  1.4013e-45,\n",
       "         4.5701e-41, -4.9772e-25], requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf1395b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [1,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [2,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [3,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [4,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [5,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [6,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [7,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [8,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [9,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [10,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [11,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [12,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [13,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [14,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [15,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [16,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [17,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [18,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [19,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [20,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [21,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [22,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [23,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [24,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [25,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [26,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [27,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [28,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [29,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [30,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [31,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [96,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [97,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [98,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [99,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [100,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [101,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [102,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [103,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [104,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [105,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [106,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [107,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [108,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [109,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [110,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [111,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [112,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [113,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [114,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [115,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [116,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [117,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [118,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [119,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [120,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [121,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [122,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [123,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [124,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [125,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [126,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [127,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [64,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [65,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [66,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [67,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [68,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [69,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [70,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [71,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [72,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [73,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [74,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [75,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [76,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [77,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [78,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [79,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [80,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [81,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [82,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [83,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [84,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [85,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [86,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [87,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [88,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [89,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [90,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [91,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [92,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [93,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [94,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [95,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [32,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [33,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [34,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [35,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [36,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [37,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [38,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [39,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [40,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [41,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [42,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [43,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [44,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [45,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [46,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [47,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [48,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [49,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [50,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [51,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [52,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [53,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [54,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [55,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [56,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [57,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [58,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [59,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [60,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [61,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [62,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [31,0,0], thread: [63,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m\n\u001b[1;32m      4\u001b[0m explainer \u001b[38;5;241m=\u001b[39m Explainer(\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39mmd,\n\u001b[1;32m      6\u001b[0m     algorithm\u001b[38;5;241m=\u001b[39mGNNExplainer(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     ),\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m node_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 17\u001b[0m explanation \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerated explanations in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexplanation\u001b[38;5;241m.\u001b[39mavailable_explanations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/explain/explainer.py:192\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[0;34m(self, x, edge_index, target, index, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    190\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should not be provided for the explanation \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplanation_type\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 192\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_target(prediction)\n\u001b[1;32m    195\u001b[0m training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtraining\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/explain/explainer.py:115\u001b[0m, in \u001b[0;36mExplainer.get_prediction\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 115\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain(training)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m, in \u001b[0;36mGNN_expl.forward\u001b[0;34m(self, x, edge_index, batch, b_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, batch, b_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m):\n\u001b[1;32m     13\u001b[0m     edges \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39medge_weight\u001b[38;5;241m.\u001b[39mrepeat(b_size)\u001b[38;5;241m.\u001b[39msigmoid() \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39medge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mrelu()\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index, edges)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/nn/conv/gcn_conv.py:210\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    208\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/nn/conv/gcn_conv.py:91\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     88\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_self_loops:\n\u001b[0;32m---> 91\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43madd_remaining_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), ), dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m     96\u001b[0m                              device\u001b[38;5;241m=\u001b[39medge_index\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/utils/loop.py:368\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    365\u001b[0m     inv_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mmask\n\u001b[1;32m    366\u001b[0m     loop_attr[edge_index[\u001b[38;5;241m0\u001b[39m][inv_mask]] \u001b[38;5;241m=\u001b[39m edge_attr[inv_mask]\n\u001b[0;32m--> 368\u001b[0m     edge_attr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43medge_attr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m, loop_attr], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    370\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([edge_index[:, mask], loop_index], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m edge_index, edge_attr\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# new_model = nn.Sequential(model.conv1, nn.ReLU(), model.conv2, nn.Dropout(0.1), model.lin)\n",
    "x = get_long_embedding(data.x, model.layers, iso_amount=model.iso_amount, batch_size=60)  # current linear\n",
    "\n",
    "explainer = Explainer(\n",
    "    model=md,\n",
    "    algorithm=GNNExplainer(epochs=200),\n",
    "    explanation_type='model',\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type='object',\n",
    "    model_config=dict(\n",
    "        mode='binary_classification',\n",
    "        task_level='node',\n",
    "        return_type='probs',\n",
    "    ),\n",
    ")\n",
    "node_index = 10\n",
    "explanation = explainer(x, data.edge_index, batch=data.batch, index=node_index)\n",
    "print(f'Generated explanations in {explanation.available_explanations}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "11ebf0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !watch nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8792bc63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_aggregate_forward_hooks',\n",
       " '_aggregate_forward_pre_hooks',\n",
       " '_apply',\n",
       " '_apply_sigmoid',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_cached_adj_t',\n",
       " '_cached_edge_index',\n",
       " '_call_impl',\n",
       " '_check_input',\n",
       " '_collect',\n",
       " '_edge_mask',\n",
       " '_edge_update_forward_hooks',\n",
       " '_edge_update_forward_pre_hooks',\n",
       " '_edge_user_args',\n",
       " '_explain',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_fused_user_args',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_lift',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_loop_mask',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_message_and_aggregate_forward_hooks',\n",
       " '_message_and_aggregate_forward_pre_hooks',\n",
       " '_message_forward_hooks',\n",
       " '_message_forward_pre_hooks',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_propagate_forward_hooks',\n",
       " '_propagate_forward_pre_hooks',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_set_size',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_user_args',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'add_self_loops',\n",
       " 'aggr',\n",
       " 'aggr_module',\n",
       " 'aggregate',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'bias',\n",
       " 'buffers',\n",
       " 'cached',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'decomposed_layers',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'edge_update',\n",
       " 'edge_updater',\n",
       " 'eval',\n",
       " 'explain',\n",
       " 'explain_message',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'flow',\n",
       " 'forward',\n",
       " 'fuse',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'improved',\n",
       " 'in_channels',\n",
       " 'inspector',\n",
       " 'ipu',\n",
       " 'jittable',\n",
       " 'lin',\n",
       " 'load_state_dict',\n",
       " 'message',\n",
       " 'message_and_aggregate',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'node_dim',\n",
       " 'normalize',\n",
       " 'out_channels',\n",
       " 'parameters',\n",
       " 'propagate',\n",
       " 'register_aggregate_forward_hook',\n",
       " 'register_aggregate_forward_pre_hook',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_edge_update_forward_hook',\n",
       " 'register_edge_update_forward_pre_hook',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_message_and_aggregate_forward_hook',\n",
       " 'register_message_and_aggregate_forward_pre_hook',\n",
       " 'register_message_forward_hook',\n",
       " 'register_message_forward_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_propagate_forward_hook',\n",
       " 'register_propagate_forward_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_parameters',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'special_args',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'update',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model.conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ef1cd8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4111, -0.3638, -0.3013,  0.2580,  0.2040, -0.2634, -0.3565, -0.4033,\n",
       "          0.0194,  0.1746,  0.1150,  0.3265,  0.4172, -0.0634, -0.3543, -0.4132],\n",
       "        [ 0.0386, -0.1096,  0.1130, -0.1322,  0.3457, -0.0725, -0.1573, -0.4054,\n",
       "         -0.2494,  0.2267,  0.3079,  0.0317,  0.0759, -0.3231, -0.3921, -0.2765],\n",
       "        [ 0.1066,  0.1532, -0.0809,  0.0019,  0.1933,  0.2524,  0.1391, -0.2895,\n",
       "         -0.0598, -0.1626, -0.2472,  0.0803, -0.3079, -0.1013, -0.1077,  0.2992],\n",
       "        [ 0.2965, -0.0845,  0.0430, -0.2189, -0.3162, -0.2142,  0.4066,  0.2465,\n",
       "          0.0122, -0.0159, -0.3255,  0.1520,  0.4043,  0.2214, -0.2900,  0.1825],\n",
       "        [ 0.2894, -0.0730, -0.3779,  0.2699, -0.1082,  0.3078,  0.3399,  0.0677,\n",
       "          0.4091, -0.3292,  0.0844,  0.2699,  0.4338,  0.3355, -0.2987, -0.1723],\n",
       "        [ 0.3455,  0.2767, -0.4111,  0.1859,  0.3466,  0.0559,  0.1483, -0.3299,\n",
       "          0.0881, -0.2911, -0.3115, -0.1289,  0.4157,  0.3018,  0.0813,  0.4375],\n",
       "        [ 0.2415, -0.0789,  0.2000, -0.1862,  0.2065,  0.0528, -0.1380, -0.1414,\n",
       "          0.2214, -0.2182,  0.1355,  0.3433,  0.1983, -0.3235,  0.3659, -0.2043],\n",
       "        [-0.4300, -0.3221,  0.1097,  0.0731,  0.3848, -0.0168,  0.1672, -0.1279,\n",
       "          0.3832,  0.0245, -0.4339, -0.1262, -0.2826,  0.3266, -0.4320, -0.0559],\n",
       "        [ 0.0824, -0.0207,  0.3368,  0.0776,  0.4203,  0.4064, -0.0939, -0.0070,\n",
       "          0.2260, -0.1629, -0.0394,  0.1155, -0.1304, -0.0881, -0.0049, -0.2224],\n",
       "        [ 0.4073, -0.1493, -0.2637,  0.1497,  0.1514,  0.1018,  0.2182, -0.0522,\n",
       "         -0.1912, -0.3372, -0.1553, -0.3626,  0.1697, -0.0477, -0.1026,  0.2350],\n",
       "        [-0.0752, -0.4197, -0.2855, -0.2528, -0.1233, -0.0583, -0.0966,  0.0244,\n",
       "          0.2434,  0.1406, -0.1139, -0.2195, -0.1022,  0.1680,  0.0129,  0.0455],\n",
       "        [ 0.4287,  0.0995,  0.3220,  0.3682,  0.2145,  0.2920,  0.3034,  0.0486,\n",
       "          0.2788, -0.3226, -0.3031,  0.3179, -0.3830, -0.3562,  0.3011, -0.3481],\n",
       "        [ 0.0828, -0.0619,  0.3339, -0.0022,  0.2749, -0.0503, -0.2918,  0.2689,\n",
       "         -0.0471, -0.2788,  0.4252, -0.4139, -0.2174,  0.1556,  0.3945, -0.0246],\n",
       "        [ 0.0918, -0.1932, -0.1922,  0.3301, -0.3760, -0.0821,  0.0611,  0.1982,\n",
       "         -0.1419, -0.2606,  0.2740, -0.0237, -0.1540, -0.1078, -0.1809,  0.2150],\n",
       "        [-0.0939,  0.0115,  0.3094, -0.4442,  0.3989,  0.2455, -0.0245, -0.2883,\n",
       "         -0.1717, -0.0154,  0.0537, -0.2253,  0.3732, -0.1590, -0.0698, -0.3370],\n",
       "        [-0.3033,  0.0368,  0.3583, -0.3091,  0.1374,  0.1748,  0.0913,  0.2747,\n",
       "          0.1718, -0.2664,  0.1805, -0.3923,  0.4032, -0.0251, -0.0118, -0.3452]],\n",
       "       device='cuda:1', requires_grad=True)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.conv1.parameters())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b7ed70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
