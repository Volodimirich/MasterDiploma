{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a5e8fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82fe31b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, features: int, hidden: int, out_features:int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential()\n",
    "        inp = features\n",
    "        while inp // 4 > out_features:\n",
    "            self.layers.append(nn.Linear(inp, inp//4))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            inp = inp // 4\n",
    "            \n",
    "        self.layers.append(nn.Linear(inp, hidden))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.decoder = nn.Linear(hidden, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = self.relu(x)\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a978b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/raid/data/cats_dogs_dataset/PetImages/Cat/758.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebca8cbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36353/2087992867.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mANTIALIAS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'split_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "data = np.array(Image.open(file).convert('L').resize((256, 256), Image.ANTIALIAS))\n",
    "t = split_tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8377c983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0f68e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create ModuleList\n",
    "def create_layers(img_shape, split_coefs=None, hidden=16, out=10, tp = 'linear', isoclines=16):\n",
    "    layers = nn.ModuleList()\n",
    "\n",
    "    if split_coefs is None:\n",
    "        split_coefs = [1, 2, 4]\n",
    "\n",
    "    for coef in split_coefs:\n",
    "        #Add simular pieces\n",
    "        input_layers = (img_shape ** 2) // (16 * coef ** 2)\n",
    "        layers.append(LinearAutoEncoder(input_layers, hidden, out))\n",
    "\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05b4ec9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 8, 3, 7] 10 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 4, 5, 6, 9])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_list = [np.random.randint(1, 10) for _ in range(4)]\n",
    "line = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "res = np.delete(line, idx_list)\n",
    "print(idx_list, len(line), len(res))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6317b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def create_perc_split(img, amount_of_steps):\n",
    "    previous = np.percentile(img, 100)\n",
    "    draw_img = np.zeros_like(img)\n",
    "    step = 100 / amount_of_steps\n",
    "    for itt_num in range(0, amount_of_steps):\n",
    "        border = np.percentile(img, 100 - step * (itt_num + 1))\n",
    "        result = np.where((previous >= img) & (img > border), itt_num, 0)\n",
    "        previous = border\n",
    "        draw_img += result\n",
    "    return draw_img\n",
    "\n",
    "def prepare_data(img, mask, amount_of_steps):\n",
    "    h, w = img.shape\n",
    "    results = []\n",
    "    pixel_per_isocline = h * w // amount_of_steps\n",
    "    \n",
    "    for i in range(amount_of_steps):\n",
    "        line = img[mask == i]\n",
    "        if line.shape[0] != pixel_per_isocline:\n",
    "            idx_list = random.sample(range(1, line.shape[0] - 1), abs(pixel_per_isocline - line.shape[0]))\n",
    "#             [np.random.randint(1, line.shape[0] - 1) for _ in range(abs(pixel_per_isocline - line.shape[0]))]\n",
    "            is_insert = True if line.shape[0] < pixel_per_isocline else False\n",
    "            \n",
    "            if not is_insert:\n",
    "                line = np.delete(line, idx_list)\n",
    "            else:\n",
    "                mean_val = line.mean()\n",
    "                line = np.insert(line, idx_list, mean_val)\n",
    "        results.append(line)\n",
    "    return results\n",
    "\n",
    "# Get fourier batches \n",
    "def split_tensor(imgs, split_coefs=None, amount_of_isoc=16):\n",
    "    if split_coefs is None:\n",
    "        split_coefs = [1, 2, 4, 8]\n",
    "    result = []\n",
    "    result_ph = []\n",
    "\n",
    "    *_, h, w = imgs.shape\n",
    "    last_val = 0\n",
    "    for coef in split_coefs:\n",
    "        step = h // coef\n",
    "        row_data = []\n",
    "        for y in range(0, w, step):\n",
    "            step_result = []\n",
    "            for x in range(0, h, step):\n",
    "                #temperory unused\n",
    "#                value = torch.log(1 + torch.abs(torch.fft.fftshift(torch.fft.fft2(imgs[..., x:x + step, y:y + step]))))\n",
    "                value = np.abs(np.fft.fftshift(np.fft.fft2(imgs[..., x:x + step, y:y + step])))\n",
    "                phase = np.angle(np.fft.fftshift(np.fft.fft2(imgs[..., x:x + step, y:y + step])))\n",
    "\n",
    "                value -= value.min()\n",
    "                value /= value.max()\n",
    "\n",
    "                phase -= phase.min()\n",
    "                phase /= phase.max()\n",
    "\n",
    "                mask = create_perc_split(value, amount_of_steps=amount_of_isoc)\n",
    "                data = prepare_data(value, mask, amount_of_steps=amount_of_isoc)\n",
    "                data_ph = prepare_data(phase, mask, amount_of_steps=amount_of_isoc)\n",
    "\n",
    "\n",
    "                result.append(data)\n",
    "                result_ph.append(data_ph)\n",
    "    return result, result_ph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "180d08f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAT - 0, DOG - 1\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "bad_files = ['/raid/data/cats_dogs_dataset/PetImages/Cat/666.jpg', '/raid/data/cats_dogs_dataset/PetImages/Cat/Thumbs.db',\n",
    "            '/raid/data/cats_dogs_dataset/PetImages/Dog/Thumbs.db', '/raid/data/cats_dogs_dataset/PetImages/Dog/11702.jpg',]\n",
    "def dataset_preprocessing(directory, type_list = [('Cat', 0) , ('Dog', 1)], \n",
    "                          img_shape=(256,256), target_dirname='preprocessed_8'):\n",
    "    result_data = []\n",
    "    if not os.path.exists(f'/raid/data/cats_dogs_dataset/{target_dirname}'):\n",
    "        os.mkdir(f'/raid/data/cats_dogs_dataset/{target_dirname}')\n",
    "    \n",
    "    for dir_name, target_type in type_list:\n",
    "        for file in tqdm(glob.glob(f'{directory}/{dir_name}/*')):\n",
    "            if file in bad_files:\n",
    "                continue\n",
    "            data = np.array(Image.open(file).convert('L').resize(img_shape, Image.ANTIALIAS))\n",
    "            try:\n",
    "                data_amp, data_phase = split_tensor(data)\n",
    "            except:\n",
    "                bad_files.append(file)\n",
    "                continue\n",
    "            if not os.path.exists(f'/raid/data/cats_dogs_dataset/{target_dirname}/{dir_name}'):\n",
    "                os.mkdir(f'/raid/data/cats_dogs_dataset/{target_dirname}/{dir_name}')\n",
    "            np.save(f'/raid/data/cats_dogs_dataset/{target_dirname}/{dir_name}/{Path(file).stem}.npy', \n",
    "                    [data_amp, data_phase, target_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dedc816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "224 * 224 / (16 * 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6022aa38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raid/data/cats_dogs_dataset/PetImages/Cat/666.jpg',\n",
       " '/raid/data/cats_dogs_dataset/PetImages/Cat/Thumbs.db',\n",
       " '/raid/data/cats_dogs_dataset/PetImages/Dog/Thumbs.db',\n",
       " '/raid/data/cats_dogs_dataset/PetImages/Dog/11702.jpg']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4378dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_names = ['/raid/data/cats_dogs_dataset/PetImages/Cat/758.jpg', '/raid/data/cats_dogs_dataset/PetImages/Cat/3130.jpg', \n",
    "             '/raid/data/cats_dogs_dataset/PetImages/Cat/10151.jpg', '/raid/data/cats_dogs_dataset/PetImages/Cat/3216.jpg',\n",
    "             '/raid/data/cats_dogs_dataset/PetImages/Cat/11819.jpg', '/raid/data/cats_dogs_dataset/PetImages/Cat/11928.jpg',\n",
    "             '/raid/data/cats_dogs_dataset/PetImages/Cat/8811.jpg', '/raid/data/cats_dogs_dataset/PetImages/Cat/947.jpg', \n",
    "             '/raid/data/cats_dogs_dataset/PetImages/Cat/6145.jpg', '/raid/data/cats_dogs_dataset/PetImages/Cat/8456.jpg', \n",
    "             '/raid/data/cats_dogs_dataset/PetImages/Cat/4522.jpg', '/raid/data/cats_dogs_dataset/PetImages/Cat/7469.jpg',\n",
    "             '/raid/data/cats_dogs_dataset/PetImages/Cat/2771.jpg', '/raid/data/cats_dogs_dataset/PetImages/Cat/11184.jpg', \n",
    "             '/raid/data/cats_dogs_dataset/PetImages/Cat/7396.jpg', '/raid/data/cats_dogs_dataset/PetImages/Cat/9765.jpg', \n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d186a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                            | 0/12501 [00:00<?, ?it/s]/home/v_shaposhnikov/env/lib/python3.9/site-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n",
      "  0%|                                                 | 20/12501 [00:08<1:25:02,  2.45it/s]/tmp/ipykernel_36353/2220269237.py:57: RuntimeWarning: invalid value encountered in true_divide\n",
      "  phase /= phase.max()\n",
      "  3%|█▎                                              | 353/12501 [02:21<1:21:39,  2.48it/s]/tmp/ipykernel_36353/2220269237.py:54: RuntimeWarning: invalid value encountered in true_divide\n",
      "  value /= value.max()\n",
      " 17%|███████▊                                       | 2078/12501 [13:53<1:10:06,  2.48it/s]"
     ]
    }
   ],
   "source": [
    "dataset_preprocessing('/raid/data/cats_dogs_dataset/PetImages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a9c9bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(data, layers, iso_amount = 16, batch_size=2):\n",
    "    node_embeddings = []\n",
    "    min_value = 0\n",
    "    \n",
    "    it_val = 0\n",
    "    for it_val, value in enumerate(data):\n",
    "        value = value.reshape(batch_size, iso_amount, -1)\n",
    "        if it_val == 0:\n",
    "            layer_id = 0\n",
    "        elif 1 <= it_val <= 4:\n",
    "            layer_id = 1\n",
    "        elif 5 <= it_val <= 21:\n",
    "            layer_id = 2\n",
    "        for i in range(iso_amount):\n",
    "            node_embeddings.append(layers[layer_id](value[:, i, :]))\n",
    "        \n",
    "    return torch.stack(node_embeddings, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0fc631f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(data, layers, iso_amount = 16):\n",
    "    result = []\n",
    "    for batch in data:\n",
    "        min_value = 0\n",
    "        node_embeddings = []\n",
    "        it_val = 0\n",
    "        for it_val, value in enumerate(batch):\n",
    "            if it_val == 0:\n",
    "                layer_id = 0\n",
    "            elif 1 <= it_val <= 4:\n",
    "                layer_id = 1\n",
    "            elif 5 <= it_val <= 21:\n",
    "                layer_id = 2\n",
    "            for i in range(iso_amount):\n",
    "                node_embeddings.append(layers[layer_id](value[i]))\n",
    "                \n",
    "        result.append(torch.stack(node_embeddings))\n",
    "    return torch.stack(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fa124f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 256\n",
    "NEURAL_TYPE = 'linear'\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv,SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_classes, size=256, out=10, emb_type = 'linear'):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.neural_type = emb_type\n",
    "        self.layers = create_layers(size, tp=NEURAL_TYPE, out=out)\n",
    "\n",
    "        self.conv1 = SAGEConv(out, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.lin = nn.Linear(hidden_channels, num_classes)\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch, batch_size):\n",
    "        x = get_embedding(x, self.layers, iso_amount=16, batch_size=batch_size) # current linear\n",
    "        #Maybe add this in get_embedding?\n",
    "        x = x.view(batch_size * 336, -1)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        #x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return self.soft(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041297ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "17493"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "38e34292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset, download_url\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class MyOwnDataset(Dataset):\n",
    "    def __init__(self, root, files_list, is_train, size=256, allow_loops=False, transform=None, pre_transform=None,\n",
    "                 pre_filter=None, iso_amount=16, split_amount=21):\n",
    "        self.data = files_list\n",
    "        self.allow_loops = allow_loops\n",
    "        self.is_train = is_train\n",
    "        # TODO fix this shit\n",
    "        self.iso_amount = iso_amount\n",
    "        self.split_amount = split_amount\n",
    "        self.gl_count = 1750 if self.is_train else 750\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def num_nodes(self):\n",
    "        return self.iso_amount * self.split_amount\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f'data_{idx}_is_train_{self.is_train}_loops={self.allow_loops}.pt' for idx in range(self.gl_count)]\n",
    "\n",
    "    def _create_cco_matrix(self):\n",
    "        vert_amount = int(self.iso_amount * self.split_amount)\n",
    "\n",
    "        adj_matrix = np.zeros((vert_amount, vert_amount))\n",
    "        for i in range(vert_amount - 1):\n",
    "            if self.allow_loops:\n",
    "                adj_matrix[i][i] = 1\n",
    "            if i % self.iso_amount != 1:\n",
    "                adj_matrix[i][i+1] = 1\n",
    "            for j in range(self.split_amount):\n",
    "                if self.iso_amount * j + i < vert_amount:\n",
    "                    adj_matrix[i][self.iso_amount * j + i] = 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "\n",
    "        source_nodes = []\n",
    "        target_nodes = []\n",
    "        edge_list = []\n",
    "        for iy, ix in np.ndindex(adj_matrix.shape):\n",
    "            if adj_matrix[iy, ix] == 1:\n",
    "                source_nodes.append(ix)\n",
    "                target_nodes.append(iy)\n",
    "\n",
    "                # unweighted solution\n",
    "                edge_list.append(1)\n",
    "\n",
    "        return source_nodes, target_nodes, edge_list\n",
    "\n",
    "    def process(self):\n",
    "        idx = 0\n",
    "        source_vert, target_vert, edge_list = self._create_cco_matrix()\n",
    "        edge_idx = torch.tensor([source_vert, target_vert])\n",
    "        # DEBUG ROW\n",
    "        _, small_data = train_test_split(self.data, test_size=0.1, random_state=42)\n",
    "        train_data, test_data = train_test_split(small_data, test_size=0.3, random_state=42)\n",
    "        data = train_data if self.is_train else test_data\n",
    "\n",
    "        for file in data:\n",
    "            # Read data from `raw_path`.\n",
    "            amp, phase, target = np.load(file, allow_pickle=True)\n",
    "            amp = [torch.tensor(item).float() for item in amp]\n",
    "\n",
    "            #lst = [torch.from_numpy(item).float() for item in lst]\n",
    "\n",
    "            # temporary only amp|\n",
    "            data = Data(x=amp,\n",
    "                        # edge_index=torch.tensor(edge_idx).clone().detach().float().requires_grad_(True),\n",
    "                        edge_index=edge_idx,\n",
    "                        edge_attrs=edge_list,\n",
    "                        y=torch.tensor([target]))\n",
    "\n",
    "            torch.save(data, osp.join(self.processed_dir, f'data_{idx}_is_train_{self.is_train}_loops={self.allow_loops}.pt'))\n",
    "            idx += 1\n",
    "        self.gl_count = idx\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}_is_train_{self.is_train}_loops={self.allow_loops}.pt'))\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "932cb11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "25978cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_root = '/raid/data/cats_dogs_dataset/'\n",
    "files = glob.glob('/raid/data/cats_dogs_dataset/preprocessed/*/*.npy', recursive=True)\n",
    "train_dataset = MyOwnDataset(save_root, files, is_train = True)\n",
    "test_dataset = MyOwnDataset(save_root, files, is_train = False)\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "save_root = '/raid/data/cats_dogs_dataset/'\n",
    "files = glob.glob('/raid/data/cats_dogs_dataset/preprocessed/*/*.npy', recursive=True)\n",
    "train_dataset = MyOwnDataset(save_root, files, is_train = True)\n",
    "test_dataset = MyOwnDataset(save_root, files, is_train = False)\n",
    "\n",
    "model = GCN(num_classes=2, hidden_channels = 10).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001, weight_decay = 0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "batch_size = 20\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3a9152ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/49 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6948073506355286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▊                                        | 1/49 [01:50<1:28:32, 110.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.5217, Test Acc: 0.4920\n",
      "0.6699774265289307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|█▋                                       | 2/49 [03:41<1:26:52, 110.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Train Acc: 0.5229, Test Acc: 0.4920\n",
      "0.6886707544326782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|██▌                                      | 3/49 [05:32<1:25:08, 111.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Train Acc: 0.5223, Test Acc: 0.4920\n",
      "0.6941331028938293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▎                                     | 4/49 [07:24<1:23:17, 111.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Train Acc: 0.5229, Test Acc: 0.4920\n",
      "0.6746267080307007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▏                                    | 5/49 [09:13<1:21:07, 110.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Train Acc: 0.5394, Test Acc: 0.5187\n",
      "0.6726589798927307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████                                    | 6/49 [11:04<1:19:18, 110.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Train Acc: 0.5737, Test Acc: 0.5347\n",
      "0.722974419593811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█████▊                                   | 7/49 [12:55<1:17:35, 110.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Train Acc: 0.5829, Test Acc: 0.5493\n",
      "0.7246370315551758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████▋                                  | 8/49 [14:47<1:15:56, 111.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Train Acc: 0.5971, Test Acc: 0.5533\n",
      "0.7205384969711304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████▌                                 | 9/49 [16:38<1:14:06, 111.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Train Acc: 0.6069, Test Acc: 0.5627\n",
      "0.7775242328643799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▏                               | 10/49 [18:29<1:12:13, 111.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Train Acc: 0.6057, Test Acc: 0.5640\n",
      "0.5820300579071045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|████████▉                               | 11/49 [20:20<1:10:22, 111.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 011, Train Acc: 0.6023, Test Acc: 0.5707\n",
      "0.6598140001296997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|█████████▊                              | 12/49 [22:11<1:08:30, 111.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 012, Train Acc: 0.6080, Test Acc: 0.5707\n",
      "0.6984009146690369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██████████▌                             | 13/49 [24:01<1:06:22, 110.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 013, Train Acc: 0.6103, Test Acc: 0.5747\n",
      "0.5728474855422974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|███████████▍                            | 14/49 [25:51<1:04:21, 110.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014, Train Acc: 0.6320, Test Acc: 0.5667\n",
      "0.6139127016067505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|████████████▏                           | 15/49 [27:40<1:02:20, 110.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 015, Train Acc: 0.6377, Test Acc: 0.5640\n",
      "0.6764341592788696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|█████████████                           | 16/49 [29:29<1:00:25, 109.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 016, Train Acc: 0.6429, Test Acc: 0.5853\n",
      "0.5836280584335327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|██████████████▌                           | 17/49 [31:19<58:30, 109.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 017, Train Acc: 0.6383, Test Acc: 0.5707\n",
      "0.6432305574417114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███████████████▍                          | 18/49 [33:08<56:32, 109.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 018, Train Acc: 0.6486, Test Acc: 0.5653\n",
      "0.6612120866775513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|████████████████▎                         | 19/49 [34:57<54:38, 109.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 019, Train Acc: 0.6383, Test Acc: 0.5747\n",
      "0.5992501974105835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|█████████████████▏                        | 20/49 [36:46<52:47, 109.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Train Acc: 0.6611, Test Acc: 0.5720\n",
      "0.6421306729316711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|██████████████████                        | 21/49 [38:35<50:57, 109.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 021, Train Acc: 0.6640, Test Acc: 0.5760\n",
      "0.701310932636261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|██████████████████▊                       | 22/49 [40:24<49:07, 109.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 022, Train Acc: 0.6509, Test Acc: 0.5760\n",
      "0.5510022044181824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|███████████████████▋                      | 23/49 [42:12<47:14, 109.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 023, Train Acc: 0.6691, Test Acc: 0.5773\n",
      "0.7113203406333923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████████████████████▌                     | 24/49 [44:02<45:25, 109.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 024, Train Acc: 0.6789, Test Acc: 0.5613\n",
      "0.544886589050293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████████████████████▍                    | 25/49 [45:51<43:36, 109.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 025, Train Acc: 0.7126, Test Acc: 0.5840\n",
      "0.6499166488647461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|██████████████████████▎                   | 26/49 [47:40<41:47, 109.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 026, Train Acc: 0.7149, Test Acc: 0.5813\n",
      "0.5749301314353943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|███████████████████████▏                  | 27/49 [49:29<39:59, 109.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 027, Train Acc: 0.6823, Test Acc: 0.5680\n",
      "0.698563277721405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|████████████████████████                  | 28/49 [51:18<38:09, 109.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 028, Train Acc: 0.7429, Test Acc: 0.5853\n",
      "0.5486679077148438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|████████████████████████▊                 | 29/49 [53:07<36:20, 109.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 029, Train Acc: 0.7434, Test Acc: 0.5720\n",
      "0.5777146220207214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|█████████████████████████▋                | 30/49 [54:56<34:32, 109.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 030, Train Acc: 0.7663, Test Acc: 0.5720\n",
      "0.6211085319519043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████████████████████████▌               | 31/49 [56:45<32:43, 109.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 031, Train Acc: 0.8029, Test Acc: 0.5787\n",
      "0.5515869855880737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|███████████████████████████▍              | 32/49 [58:34<30:55, 109.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 032, Train Acc: 0.7891, Test Acc: 0.5693\n",
      "0.4350293278694153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████████████████████████▉             | 33/49 [1:00:23<29:05, 109.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 033, Train Acc: 0.8240, Test Acc: 0.5800\n",
      "0.4819869101047516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|███████████████████████████▊            | 34/49 [1:02:12<27:15, 109.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 034, Train Acc: 0.8080, Test Acc: 0.5600\n",
      "0.6432255506515503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|████████████████████████████▌           | 35/49 [1:04:01<25:25, 108.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 035, Train Acc: 0.8451, Test Acc: 0.5467\n",
      "0.33555835485458374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|█████████████████████████████▍          | 36/49 [1:05:50<23:36, 108.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 036, Train Acc: 0.8606, Test Acc: 0.5720\n",
      "0.33968883752822876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|██████████████████████████████▏         | 37/49 [1:07:39<21:47, 108.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 037, Train Acc: 0.8863, Test Acc: 0.5600\n",
      "0.3199881911277771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████████████████████████████         | 38/49 [1:09:28<19:58, 108.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 038, Train Acc: 0.8926, Test Acc: 0.5547\n",
      "0.5280546545982361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████▊        | 39/49 [1:11:17<18:09, 108.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 039, Train Acc: 0.9040, Test Acc: 0.5600\n",
      "0.3717808723449707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████████████████████████████▋       | 40/49 [1:13:06<16:20, 108.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 040, Train Acc: 0.8686, Test Acc: 0.5600\n",
      "0.3677230179309845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|█████████████████████████████████▍      | 41/49 [1:14:54<14:30, 108.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 041, Train Acc: 0.8949, Test Acc: 0.5613\n",
      "0.3142542839050293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|██████████████████████████████████▎     | 42/49 [1:16:43<12:41, 108.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 042, Train Acc: 0.9097, Test Acc: 0.5667\n",
      "0.375369131565094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|██████████████████████████████████▎     | 42/49 [1:17:01<12:50, 110.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3775/3747805677.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mbest_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3775/3747805677.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Perform a single forward pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute the loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mitt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3775/1519917035.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, batch, batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miso_amount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# current linear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m#Maybe add this in get_embedding?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m336\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3775/431836968.py\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(data, layers, iso_amount, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mlayer_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miso_amount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mnode_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3775/121599462.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# weights_stem = 'GNN_Linear_No_loops_bsize=1'\n",
    "# wandb.run.name = weights_stem\n",
    "\n",
    "\n",
    "#                 experiment.log_metric(\"train_dice_loss\", batch_loss.item(),\n",
    "#                                       epoch=epoch_idx, step=step_counter[action])\n",
    "def train():\n",
    "    model.train()\n",
    "    itt = 0\n",
    "    for itt, data in enumerate(train_loader):  # Iterate in batches over the training dataset.\n",
    "#         out = model(data.x, data.edge_index, data.batch) \n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "        data = data.to(device)\n",
    "#          print(data.x.shape)\n",
    "        \n",
    "        #\n",
    "        out = model(data.x, data.edge_index, data.batch, batch_size)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        if itt % 100 == 0:\n",
    "            print(loss.item())\n",
    "#             wandb.log({\"train_cross_entropy_loss\": loss.item()})\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for itt, data in enumerate(loader):  # %Iterate in batches over the training/test dataset.\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch, batch_size) # Perform a single forward pass.\n",
    "        data = data.to(device)\n",
    "        pred = out.argmax(dim=1)# Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "#         correct += int((pred == data.y>.squeeze().unsqueeze(0).float()).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "best_train, cur_epoch, best_val = -1, -1, -1 \n",
    "for epoch in tqdm(range(1, 50)):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    best_train, cur_epoch, best_val = (train_acc, epoch, test_acc) if test_acc > best_val \\\n",
    "                                    else (best_train, cur_epoch, best_val)\n",
    "    #wandb.log({'best train accuracy': best_train})\n",
    "    #wandb.log({'best test accuracy': best_val})\n",
    "    \n",
    "    #wandb.log({'current train accuracy': train_acc})\n",
    "    #wandb.log({'current test accuracy': test_acc})\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "print(f'Best test accuracy - {best_val} on epoch {cur_epoch} with train accuracy -ч> {best_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "658919f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_test(data, layers, iso_amount=16, batch_size=2):\n",
    "    result, node_embeddings = [], []\n",
    "    min_value = 0\n",
    "    \n",
    "    it_val = 0\n",
    "    for it_val, value in enumerate(data):\n",
    "        print(value.shape)\n",
    "        value = value.reshape(batch_size, iso_amount, -1)\n",
    "        if it_val == 0:\n",
    "            layer_id = 0\n",
    "        elif 1 <= it_val <= 4:\n",
    "            layer_id = 1\n",
    "        elif 5 <= it_val <= 21:\n",
    "            layer_id = 2\n",
    "        for i in range(iso_amount):\n",
    "            print(value.shape)\n",
    "            node_embeddings.append(layers[layer_id](value[:, i, :]))\n",
    "        \n",
    "    return torch.stack(node_embeddings, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "90266d30",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_embedding() got an unexpected keyword argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3775/2431402711.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Iterate in batches over the training dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miso_amount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_embedding() got an unexpected keyword argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "for itt, data in enumerate(train_loader):  # Iterate in batches over the training dataset.\n",
    "    z = get_embedding(data.x, model.layers, iso_amount=16, batch_size=2)\n",
    "    print(z.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943e2f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d2a38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
